{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "bce-aae-oea-dev-syn"
		},
		"LS_Azure_SQL_DB_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LS_Azure_SQL_DB'"
		},
		"LS_SQL_Serverless_OEA_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LS_SQL_Serverless_OEA'"
		},
		"bce-aae-oea-dev-syn-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'bce-aae-oea-dev-syn-WorkspaceDefaultSqlServer'"
		},
		"LS_ADLS_OEA_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bceaaeoeadevlrs.dfs.core.windows.net"
		},
		"LS_HTTP_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{linkedService().baseURL}"
		},
		"LS_KeyVault_OEA_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://bce-aae-oea-dev-kv.vault.azure.net/"
		},
		"bce-aae-oea-dev-syn-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bceaaeoeadevlrs.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_all_from_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get list of tables",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "select schema_name(t.schema_id) as schema_name, t.name as table_name\nfrom sys.tables t",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Azure_SQL_DB",
								"type": "DatasetReference",
								"parameters": {
									"dbServer": {
										"value": "@pipeline().parameters.dbServer",
										"type": "Expression"
									},
									"dbName": {
										"value": "@pipeline().parameters.dbName",
										"type": "Expression"
									},
									"userName": {
										"value": "@pipeline().parameters.userName",
										"type": "Expression"
									},
									"keyVaultSecretName": {
										"value": "@pipeline().parameters.keyVaultSecretName",
										"type": "Expression"
									}
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Set currentDateTime",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "currentDateTime",
							"value": {
								"value": "@{formatDateTime(convertTimeZone(utcnow(), 'UTC', pipeline().parameters.timezone), 'yyyy-MM-ddTHHmm_ss')}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get list of tables",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set currentDateTime",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get list of tables').output.value",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Copy_from_Azure_SQL_DB",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "Copy_from_Azure_SQL_DB",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"dbServer": {
												"value": "@pipeline().parameters.dbServer",
												"type": "Expression"
											},
											"dbName": {
												"value": "@pipeline().parameters.dbName",
												"type": "Expression"
											},
											"userName": {
												"value": "@pipeline().parameters.userName",
												"type": "Expression"
											},
											"keyVaultSecretName": {
												"value": "@pipeline().parameters.keyVaultSecretName",
												"type": "Expression"
											},
											"query": {
												"value": "select * from @{item().schema_name}.@{item().table_name}",
												"type": "Expression"
											},
											"sinkFilesystem": {
												"value": "@pipeline().parameters.sinkFilesystem",
												"type": "Expression"
											},
											"sinkDirectory": {
												"value": "@{pipeline().parameters.sinkDirectory}/@{variables('currentDateTime')}/@{item().table_name}",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "contosoisd3a.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqladmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqladmin-password"
					},
					"sinkFilesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"sinkDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis_db"
					},
					"timezone": {
						"type": "string",
						"defaultValue": "Eastern Standard Time"
					}
				},
				"variables": {
					"currentDateTime": {
						"type": "String"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2022-05-18T00:42:20Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_Azure_SQL_DB')]",
				"[concat(variables('workspaceId'), '/pipelines/Copy_from_Azure_SQL_DB')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_from_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy from Azure SQL DB",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": {
									"value": "@pipeline().parameters.query",
									"type": "Expression"
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "DS_Azure_SQL_DB",
								"type": "DatasetReference",
								"parameters": {
									"dbServer": {
										"value": "@pipeline().parameters.dbServer",
										"type": "Expression"
									},
									"dbName": {
										"value": "@pipeline().parameters.dbName",
										"type": "Expression"
									},
									"userName": {
										"value": "@pipeline().parameters.userName",
										"type": "Expression"
									},
									"keyVaultSecretName": {
										"value": "@pipeline().parameters.keyVaultSecretName",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ADLS_parquet",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.sinkFilesystem",
										"type": "Expression"
									},
									"directory": {
										"value": "@pipeline().parameters.sinkDirectory",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "contosoisd3a.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqladmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqladmin-password"
					},
					"query": {
						"type": "string",
						"defaultValue": "select * from person"
					},
					"sinkFilesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"sinkDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis_db/person"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2022-05-18T00:42:08Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_Azure_SQL_DB')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_parquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_from_URL')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Copies data from the specified URL and lands it in the specified location in the data lake.",
				"activities": [
					{
						"name": "copy from URL",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "URL",
								"value": "@pipeline().parameters.URL"
							},
							{
								"name": "sinkFilesystem",
								"value": "@pipeline().parameters.sinkFilesystem"
							},
							{
								"name": "sinkFilename",
								"value": "@pipeline().parameters.sinkFilename"
							}
						],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "DS_HTTP_binary",
								"type": "DatasetReference",
								"parameters": {
									"URL": {
										"value": "@pipeline().parameters.URL",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ADLS_binary_file",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.sinkFilesystem",
										"type": "Expression"
									},
									"filename": {
										"value": "@pipeline().parameters.sinkFilename",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"URL": {
						"type": "string",
						"defaultValue": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentattendance.csv"
					},
					"sinkFilesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"sinkFilename": {
						"type": "string",
						"defaultValue": "contoso_sis/example1/studentattendance.csv"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2022-05-18T00:41:40Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_HTTP_binary')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_file')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_from_each_URL')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Retrieves data from multiple HTTP endpoints as specified in the 'endpoints' parameter.\nThe data is landed in the data lake within a folder named with the current datetime (in the timezone specified).\n\nFor a list of timezones, see: https://docs.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#convertfromutc",
				"activities": [
					{
						"name": "get data for each endpoint",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Set currentDateTime",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.endpoints",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 3,
							"activities": [
								{
									"name": "Copy_from_URL",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "Copy_from_URL",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"URL": {
												"value": "@item().URL",
												"type": "Expression"
											},
											"sinkFilesystem": {
												"value": "@pipeline().parameters.sinkFilesystem",
												"type": "Expression"
											},
											"sinkFilename": {
												"value": "@{item().sinkDirectory}/@{variables('currentDateTime')}/@{item().sinkFilename}",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "Set currentDateTime",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "currentDateTime",
							"value": {
								"value": "@{formatDateTime(convertTimeZone(utcnow(), 'UTC', pipeline().parameters.timezone), 'yyyy-MM-ddTHHmm_ss')}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"endpoints": {
						"type": "array",
						"defaultValue": [
							{
								"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentattendance.csv",
								"sinkDirectory": "contoso_sis",
								"sinkFilename": "studentattendance/studentattendance.csv"
							},
							{
								"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentdemographics.csv",
								"sinkDirectory": "contoso_sis",
								"sinkFilename": "studentdemographics/studentdemographics.csv"
							},
							{
								"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentsectionmark.csv",
								"sinkDirectory": "contoso_sis",
								"sinkFilename": "studentsectionmark/studentsectionmark.csv"
							}
						]
					},
					"sinkFilesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"timezone": {
						"type": "string",
						"defaultValue": "Eastern Standard Time"
					}
				},
				"variables": {
					"currentDateTime": {
						"type": "String"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2022-05-18T00:41:56Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Copy_from_URL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Insights_main_pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Insights ingest into stage 2p and 2np",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Dimensions ingest",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "NEW_Insights_module_setup",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "spark3p1sm",
								"type": "BigDataPoolReference"
							}
						}
					},
					{
						"name": "If create_sql_db",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Insights ingest into stage 2p and 2np",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@pipeline().parameters.create_sql_db",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "create_sql_db",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "create_sql_db1",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"storageAccount": {
												"value": "@pipeline().parameters.storageAccount",
												"type": "Expression"
											},
											"sourceDirectory": {
												"value": "@pipeline().parameters.sourceDirectory",
												"type": "Expression"
											},
											"stage": "2"
										}
									}
								}
							]
						}
					},
					{
						"name": "If create_lake_db",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Insights ingest into stage 2p and 2np",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@pipeline().parameters.create_lake_db",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "create_lake_db",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "create_lake_db1",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"stageNum": "2",
											"sourceDirectory": {
												"value": "@pipeline().parameters.sourceDirectory",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "Dimensions ingest",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "SchoolPerformanceTable",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "spark3p1sm",
								"type": "BigDataPoolReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"storageAccount": {
						"type": "string",
						"defaultValue": "bceaaeoeadevlrs"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "SDS"
					},
					"create_sql_db": {
						"type": "bool",
						"defaultValue": false
					},
					"create_lake_db": {
						"type": "bool",
						"defaultValue": false
					}
				},
				"folder": {
					"name": "Insights Module"
				},
				"annotations": [],
				"lastPublishTime": "2021-11-23T20:25:17Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/NEW_Insights_module_setup')]",
				"[concat(variables('workspaceId'), '/bigDataPools/spark3p1sm')]",
				"[concat(variables('workspaceId'), '/notebooks/SchoolPerformanceTable')]",
				"[concat(variables('workspaceId'), '/pipelines/create_sql_db1')]",
				"[concat(variables('workspaceId'), '/pipelines/create_lake_db1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create_lake_db1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "create_lake_db",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "kwargs",
								"value": "{'stage_num':@{pipeline().parameters.stageNum},'source_dir':@{pipeline().parameters.sourceDirectory}"
							}
						],
						"typeProperties": {
							"notebook": {
								"referenceName": "OEA_connector1",
								"type": "NotebookReference"
							},
							"parameters": {
								"object_name": {
									"value": "oea",
									"type": "string"
								},
								"method_name": {
									"value": "create_lake_db",
									"type": "string"
								},
								"kwargs": {
									"value": {
										"value": "{'stage_num':@{pipeline().parameters.stageNum},'source_dir':'@{pipeline().parameters.sourceDirectory}'}",
										"type": "Expression"
									},
									"type": "string"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"stageNum": {
						"type": "string",
						"defaultValue": "2"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis"
					}
				},
				"folder": {
					"name": "OEA_Framework/Ingest"
				},
				"annotations": [],
				"lastPublishTime": "2021-11-19T17:35:55Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/OEA_connector1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create_sql_db1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "set sqlDBName",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "sqlDBName",
							"value": {
								"value": "sqls@{pipeline().parameters.stage}_@{pipeline().parameters.sourceDirectory}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Stored procedure1",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "set sqlDBName",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[dbo].[sp_executesql]",
							"storedProcedureParameters": {
								"command": {
									"value": {
										"value": "IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '@{variables('sqlDBName')}') \nBEGIN\n  CREATE DATABASE @{variables('sqlDBName')}; \nEND;",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "LS_SQL_Serverless_OEA",
							"type": "LinkedServiceReference",
							"parameters": {
								"dbName": "master"
							}
						}
					},
					{
						"name": "get folders in stageXp",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "Stored procedure1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder4",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "stage@{pipeline().parameters.stage}p",
										"type": "Expression"
									},
									"directory": {
										"value": "@pipeline().parameters.sourceDirectory",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "get folders in stageXp",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('get folders in stageXp').output.childItems",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "create or alter view for pseduonymized tables",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [
										{
											"name": "command",
											"value": "CREATE OR ALTER VIEW @{item().name} AS SELECT * FROM OPENROWSET( BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage@{pipeline().parameters.stage}p/@{pipeline().parameters.sourceDirectory}/@{item().name}', FORMAT='DELTA' ) AS [r]"
										}
									],
									"typeProperties": {
										"storedProcedureName": "[dbo].[sp_executesql]",
										"storedProcedureParameters": {
											"command": {
												"value": {
													"value": "CREATE OR ALTER VIEW @{item().name} AS\nSELECT * FROM OPENROWSET(\nBULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage@{pipeline().parameters.stage}p/@{pipeline().parameters.sourceDirectory}/@{item().name}',\nFORMAT='DELTA'\n) AS [r]",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_SQL_Serverless_OEA",
										"type": "LinkedServiceReference",
										"parameters": {
											"dbName": {
												"value": "@variables('sqlDBName')",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "get folders in stageXnp",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "Stored procedure1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder4",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "stage@{pipeline().parameters.stage}np",
										"type": "Expression"
									},
									"directory": {
										"value": "@pipeline().parameters.sourceDirectory",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "ForEach2",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "get folders in stageXnp",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('get folders in stageXnp').output.childItems",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "create or alter view for lookup tables",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [
										{
											"name": "command",
											"value": "CREATE OR ALTER VIEW @{item().name} AS SELECT * FROM OPENROWSET( BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage@{pipeline().parameters.stage}p/@{pipeline().parameters.sourceDirectory}/@{item().name}', FORMAT='DELTA' ) AS [r]"
										}
									],
									"typeProperties": {
										"storedProcedureName": "[dbo].[sp_executesql]",
										"storedProcedureParameters": {
											"command": {
												"value": {
													"value": "CREATE OR ALTER VIEW @{item().name} AS\nSELECT * FROM OPENROWSET(\nBULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage@{pipeline().parameters.stage}np/@{pipeline().parameters.sourceDirectory}/@{item().name}',\nFORMAT='DELTA'\n) AS [r]",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_SQL_Serverless_OEA",
										"type": "LinkedServiceReference",
										"parameters": {
											"dbName": {
												"value": "@variables('sqlDBName')",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"storageAccount": {
						"type": "string",
						"defaultValue": "bceaaeoeadevlrs"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "/"
					},
					"stage": {
						"type": "string",
						"defaultValue": "2"
					}
				},
				"variables": {
					"sqlDBName": {
						"type": "String",
						"defaultValue": "sqls2_mydb"
					}
				},
				"folder": {
					"name": "OEA_Framework/Ingest"
				},
				"annotations": [],
				"lastPublishTime": "2021-11-19T17:36:08Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_SQL_Serverless_OEA')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_folder4')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/example_main_pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Example pipeline demonstrating typical orchestration of data extraction, landing, ingestion, and creation of lake and sql db's.",
				"activities": [
					{
						"name": "Extract from source - land in stage1np",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Copy_from_each_URL",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"endpoints": {
									"value": "@json('[{\"URL\": \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Student_and_School_Data_Systems/test_data/batch1/studentattendance.csv\",\"sinkDirectory\": \"contoso_sis/studentattendance\",\"sinkFilename\": \"part1.csv\"}, {\"URL\": \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Student_and_School_Data_Systems/test_data/batch1/studentdemographics.csv\",\"sinkDirectory\": \"contoso_sis/studentdemographics\",\"sinkFilename\": \"part1.csv\"}, {\"URL\": \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Student_and_School_Data_Systems/test_data/batch1/studentsectionmark.csv\",\"sinkDirectory\": \"contoso_sis/studentsectionmark\",\"sinkFilename\": \"part1.csv\"}]')",
									"type": "Expression"
								},
								"sinkFilesystem": "stage1np",
								"timezone": "Eastern Standard Time"
							}
						}
					},
					{
						"name": "ingest into stage2p and 2np",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Extract from source - land in stage1np",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "OEA_connector",
								"type": "NotebookReference"
							},
							"parameters": {
								"object_name": {
									"value": "contoso_sis",
									"type": "string"
								},
								"method_name": {
									"value": "ingest",
									"type": "string"
								}
							},
							"sparkPool": {
								"referenceName": "spark3p1sm",
								"type": "BigDataPoolReference"
							}
						}
					},
					{
						"name": "If create_sql_db",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "ingest into stage2p and 2np",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@pipeline().parameters.create_sql_db",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "create_sql_db",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "create_sql_db1",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {}
									}
								}
							]
						}
					},
					{
						"name": "If create_lake_db",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "ingest into stage2p and 2np",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@pipeline().parameters.create_lake_db",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "create_lake_db",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "create_lake_db1",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"storageAccount": {
						"type": "string",
						"defaultValue": "bceaaeoeadevlrs"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "BCE/BI/SRS"
					},
					"create_sql_db": {
						"type": "bool",
						"defaultValue": true
					},
					"create_lake_db": {
						"type": "bool",
						"defaultValue": true
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-11T13:10:06Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Copy_from_each_URL')]",
				"[concat(variables('workspaceId'), '/notebooks/OEA_connector')]",
				"[concat(variables('workspaceId'), '/bigDataPools/spark3p1sm')]",
				"[concat(variables('workspaceId'), '/pipelines/create_sql_db1')]",
				"[concat(variables('workspaceId'), '/pipelines/create_lake_db1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reset_all_for_source')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Deletes the ingestion for the specified system and table - in order to start over when testing during implementation.",
				"activities": [
					{
						"name": "delete source system dir from stage1np",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"directory": {
										"value": "@pipeline().parameters.sourceSystem",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete from stage2np",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2np",
									"directory": {
										"value": "@pipeline().parameters.sourceSystem",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete from stage2p",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2p",
									"directory": {
										"value": "@pipeline().parameters.sourceSystem",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"sourceSystem": {
						"type": "string",
						"defaultValue": "contoso_sis"
					}
				},
				"folder": {
					"name": "OEA_Framework/Reset"
				},
				"annotations": [],
				"lastPublishTime": "2022-05-18T00:43:19Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_folder')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reset_ingestion_of_table')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Deletes the ingestion for the specified system and table - in order to start over when testing during implementation.",
				"activities": [
					{
						"name": "delete _checkpoints_p",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"directory": {
										"value": "@{pipeline().parameters.sourceSystem}/@{pipeline().parameters.tablename}/_checkpoints_p",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete _checkpoints_np",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"directory": {
										"value": "@{pipeline().parameters.sourceSystem}/@{pipeline().parameters.tablename}/_checkpoints_np",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete from stage2np",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2np",
									"directory": {
										"value": "@{pipeline().parameters.sourceSystem}/@{pipeline().parameters.tablename}",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete from stage2p",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2p",
									"directory": {
										"value": "@{pipeline().parameters.sourceSystem}/@{pipeline().parameters.tablename}",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"sourceSystem": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"tablename": {
						"type": "string",
						"defaultValue": "studentattendance"
					}
				},
				"folder": {
					"name": "OEA_Framework/Reset"
				},
				"annotations": [],
				"lastPublishTime": "2022-05-18T00:43:33Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_folder')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"directory": {
						"type": "string"
					},
					"file": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().file",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_file')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used for landing data in the data lake.\nDefaults to landing data in stage1np.\nNote that you can specify a full path in the filename param (eg, to land a file in a specific folder filename param can be 'contoso_sis/students/students.csv').\n",
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"filename": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_file1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used for landing data in the data lake.\nDefaults to landing data in stage1np.\nNote that you can specify a full path in the filename param (eg, to land a file in a specific folder filename param can be 'contoso_sis/students/students.csv').\n",
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"filename": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_folder')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_folder1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "bce-aae-oea-dev-syn-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/bce-aae-oea-dev-syn-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_folder2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_folder3')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_folder4')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used for landing data in the data lake as in parquet format.\nDefaults to landing data in stage1np.\nNote that you cannot specify a filename because with parquet the filename should be auto-generated.\n",
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_Azure_SQL_DB",
					"type": "LinkedServiceReference",
					"parameters": {
						"dbServer": {
							"value": "@dataset().dbServer",
							"type": "Expression"
						},
						"dbName": {
							"value": "@dataset().dbName",
							"type": "Expression"
						},
						"userName": {
							"value": "@dataset().userName",
							"type": "Expression"
						},
						"keyVaultSecretName": {
							"value": "@dataset().keyVaultSecretName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "myserver.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "testdb"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqlAdmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqlAdminPwd"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": []
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_Azure_SQL_DB')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_HTTP_binary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Retrieves data from an http endpoint.\nThe data can be in any format - the binary dataset allows us to pull any payload without affecting it.",
				"linkedServiceName": {
					"referenceName": "LS_HTTP",
					"type": "LinkedServiceReference",
					"parameters": {
						"baseURL": {
							"value": "@dataset().URL",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"URL": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_HTTP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_HTTP_binary1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Retrieves data from an http endpoint.\nThe data can be in any format - the binary dataset allows us to pull any payload without affecting it.",
				"linkedServiceName": {
					"referenceName": "LS_HTTP",
					"type": "LinkedServiceReference",
					"parameters": {
						"baseURL": {
							"value": "@dataset().URL",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"URL": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_HTTP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_HTTP_binary2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Retrieves data from an http endpoint.\nThe data can be in any format - the binary dataset allows us to pull any payload without affecting it.",
				"linkedServiceName": {
					"referenceName": "LS_HTTP",
					"type": "LinkedServiceReference",
					"parameters": {
						"baseURL": {
							"value": "@dataset().URL",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"URL": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_HTTP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_ADLS_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Connection to the OEA data lake",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('LS_ADLS_OEA_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Allows for connecting to an Azure SQL database using SQL authentication and retrieving the user password from the key vault.",
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "myserver.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "testdb"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqlAdmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqlAdminPwd"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('LS_Azure_SQL_DB_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "LS_KeyVault_OEA",
							"type": "LinkedServiceReference"
						},
						"secretName": {
							"value": "@linkedService().keyVaultSecretName",
							"type": "Expression"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_KeyVault_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_HTTP')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Connection to an HTTP endpoint.\nThe baseURL parameter must be passed in from the dataset that utilizes this linked service.",
				"parameters": {
					"baseURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('LS_HTTP_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_KeyVault_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('LS_KeyVault_OEA_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQL_Serverless_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"dbName": {
						"type": "string",
						"defaultValue": "master"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('LS_SQL_Serverless_OEA_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bce-aae-oea-dev-syn-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('bce-aae-oea-dev-syn-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bce-aae-oea-dev-syn-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bce-aae-oea-dev-syn-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1_read_me')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"![OEA](https://openeducationanalytics.org/assets/imgs/img_oea_logo.png)\n",
							"# OEA and the OEA Framework\n",
							"\n",
							"[OEA](https://openeducationanalytics.org/) is the overarching community and ecosystem centered around the effective and responsible use of data and analytics in education.\n",
							"\n",
							"The [OEA framework](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework) is an open source python library and synapse pipeline assets - built in collaboration with the OEA community - that simplifies the process of working with the data in your data lake in a way that follows a standardized data lake architecture and data processing best practices through use of [Apache Spark](https://spark.apache.org/) and [delta lake](https://delta.io/) technologies.\n",
							"\n",
							"Listed below are 3 included examples that demonstrate the usage of the OEA framework."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Example #1: End to end (collect, prep, view)\n",
							"The OEA framework comes with a set of Synapse pipelines that demonstrate how to extract data from data sources with common interfaces.\n",
							"\n",
							"By clicking on \"Integrate\" in the left nav bar and opening \"example_main_pipeline\" you can run an example pipeline that does the following:\n",
							"- 1. Retrieves data from an http endpoint\n",
							"- 2. Lands the data in the stage1np directory\n",
							"- 3. Ingests the data by first running a pseudonymization process, then writing pseudonymized data to delta tables in stage2p and writing non-pseudonymized data to delta tables in stage2np\n",
							"- 4. Creates a spark db that points to the delta tables in stage2p and stage2np\n",
							"- 5. Creates a sql serverless db with views pointing to the delta tables in stage2p and stage2np\n",
							"\n",
							"You can then run the pipeline in the Reset folder called \"reset_all_for_source\" to reset everything in the data lake that was done in the \"example_main_pipeline\"."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example #2: batch data processing\n",
							"The notebook **_2_batch_processing_demo_** provides a self-contained demonstration of landing and ingesting 3 different types of batch data sets:\n",
							"\n",
							"1. [Incremental data](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework#1-incremental-data)\n",
							"2. [Delta data](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework#2-delta-data-change-data)\n",
							"3. [Snapshot data](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework#3-snapshot-data)\n",
							"\n",
							"Open that notebook and walk through each cell for the details."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example #3: data generation demo\n",
							"When learning to work synapse studio and the OEA framework, and when developing the data exploration and data prep scripts you need, it's especially helpful to have test data sets to work with.\n",
							"\n",
							"The notebook **_3_data_generation_demo_** data generation demo shows how to generate test data sets across multiple fictional schools for testing purposes."
						],
						"attachments": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2_batch_processing_demo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# OEA Demo\r\n",
							"This notebook demonstrates the batch processing features of the OEA framework."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Incremental batches"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reset this example (deletes data in stage1np/example, stage2np/example, and stage2p/example)\r\n",
							"oea.delete_data_source('example')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Land the first batch of test data\r\n",
							"df1 = spark.createDataFrame([(1,'Joe','English','2021'), (2,'Helen','English','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('example', 'student', df1)\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ingest the first batch of test data into stage2\r\n",
							"example_schema = [['id', 'string', 'hash'], ['name', 'string', 'mask'], ['language', 'string', 'no-op'], ['school_year', 'string', 'partition-by']]\r\n",
							"oea.ingest_incremental_data('example', 'student', example_schema, 'school_year', 'id')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/example/student_pseudo')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# land the second batch of test data\r\n",
							"df2 = spark.createDataFrame([(3,'Elisa','Spanish','2021'), (4,'Lily','English','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('example', 'student', df2)\r\n",
							"# show the comprehensive set of data landed in stage1\r\n",
							"df = oea.load_csv('stage1np/example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ingest the second batch of test data into stage2\r\n",
							"oea.ingest_incremental_data('example', 'student', example_schema, 'school_year', 'id')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/example/student_pseudo')\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta batches"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reset this example (deletes data in stage1np/example, stage2np/example, and stage2p/example)\r\n",
							"oea.delete_data_source('delta_example')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Land the first batch of test data\r\n",
							"df1 = spark.createDataFrame([(1,'Joseph','English','2021'), (2,'Helen','English','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('delta_example', 'student', df1)\r\n",
							"\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/delta_example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ingest the first batch of test data into stage2\r\n",
							"example_schema = [['id', 'string', 'hash'], ['name', 'string', 'mask'], ['language', 'string', 'no-op'], ['school_year', 'string', 'partition-by']]\r\n",
							"oea.ingest_delta_data('delta_example', 'student', example_schema, 'school_year')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/delta_example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/delta_example/student_pseudo')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Land the second batch of test data\r\n",
							"df2 = spark.createDataFrame([(1,'Joseph','Spanish','2021'), (3,'Elisa','Spanish','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('delta_example', 'student', df2)\r\n",
							"\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/delta_example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ingest the second batch of test data into stage2\r\n",
							"oea.ingest_delta_data('delta_example', 'student', example_schema, 'school_year')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/delta_example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/delta_example/student_pseudo')\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Snapshot batches"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# reset this example (deletes data in stage1np/example, stage2np/example, and stage2p/example)\r\n",
							"oea.delete_data_source('snapshot_example')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# land data in stage1\r\n",
							"df1 = spark.createDataFrame([(1,'Joseph','English','2021'), (2,'Helen','English','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('snapshot_example', 'student', df1)\r\n",
							"\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/snapshot_example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# process data from stage1 into stage2\r\n",
							"example_schema = [['id', 'string', 'hash'], ['name', 'string', 'mask'], ['language', 'string', 'no-op'], ['school_year', 'string', 'partition-by']]\r\n",
							"oea.ingest_snapshot_data('snapshot_example', 'student', example_schema, 'school_year')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/snapshot_example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/snapshot_example/student_pseudo')\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# land the second test data batch in stage1\r\n",
							"df2 = spark.createDataFrame([(1,'Joseph','Spanish','2021'), (3,'Elisa','Spanish','2021')], ['id', 'name', 'language', 'school_year'])\r\n",
							"oea.land('snapshot_example', 'student', df2)\r\n",
							"\r\n",
							"# show what's landed in stage1\r\n",
							"df = oea.load_csv('stage1np/snapshot_example/student')\r\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# process data from stage1 into stage2\r\n",
							"example_schema = [['id', 'string', 'hash'], ['name', 'string', 'mask'], ['language', 'string', 'no-op'], ['school_year', 'string', 'partition-by']]\r\n",
							"oea.ingest_snapshot_data('snapshot_example', 'student', example_schema, 'school_year')\r\n",
							"\r\n",
							"# show what's in stage2\r\n",
							"df = oea.load_delta('stage2np/snapshot_example/student_lookup')\r\n",
							"df.show()\r\n",
							"df = oea.load_delta('stage2p/snapshot_example/student_pseudo')\r\n",
							"df.show()\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/3_data_generation_demo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data Generation Example\r\n",
							"This notebook demonstrates how to use the EdFiDataGenerator to generate test student data in the Ed-Fi format for as many schools as specified.\r\n",
							"\r\n",
							"To generate test Ed-Fi data, simple run this notebook.\r\n",
							"The test data will be generated in json format and written to stage1np/test_data in your data lake."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run OEA_py"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run DataGen_py"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dg = EdFiDataGenerator()\r\n",
							"writer = DataLakeWriter(oea.stage1np + '/test_data')\r\n",
							"dg.generate_data(2, writer)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ContosoISD_example')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Insights Module"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ae4a5a35-e27f-442d-bcd6-1536d9fd9925"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ContosoISD Example\r\n",
							"This example demonstrates how to use the OEA framework and modules to process incoming data, perform data prep, and view the data in an example Power BI dashboard.\r\n",
							"\r\n",
							"# Running the example\r\n",
							"1) Select your spark pool in the \"Attach to\" dropdown list above.\r\n",
							"\r\n",
							"2) Click on \"Publish\" in the top nav bar (and wait a few seconds for the notification that says \"Publishing completed\").\r\n",
							"\r\n",
							"3) Click on \"Run all\" at the top of this tab (and wait for the processing to complete - which can take around 5 to 10 minutes).\r\n",
							"\r\n",
							"4) Open the dashboard in Power BI desktop and point it to your newly setup data lake (you can download the pbix from here: [techInequityDashboardContoso v2.pbix](https://github.com/microsoft/OpenEduAnalytics/blob/main/packages/ContosoISD/power_bi/techInequityDashboardContoso%20v2.pbix) )\r\n",
							"\r\n",
							"# More info\r\n",
							"See [OEA Solution Guide](https://github.com/microsoft/OpenEduAnalytics/blob/main/docs/OpenEduAnalyticsSolutionGuide.pdf) for more details on this example."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /example_modules_py"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 0) Initialize the OEA framework and modules needed.\r\n",
							"oea = OEA()\r\n",
							"m365 = M365(oea)\r\n",
							"contoso_sis = ContosoSIS(oea, 'contoso_sis', False)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 1) Land data into stage1 of your data lake, from multiple source systems (this example copies in test data sets that came with the OEA installation).\r\n",
							"contoso_sis.copy_test_data_to_stage1()\r\n",
							"m365.copy_test_data_to_stage1()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 2) Process the raw data (csv format) from stage1 into stage2 (adds schema details and writes out in parquet format).\r\n",
							"#    [Note: we're not performing pseudonymization in this example, so everything is written to container stage2np.]\r\n",
							"m365.process_roster_data_from_stage1()\r\n",
							"contoso_sis.process_data_from_stage1()\r\n",
							"m365.process_activity_data_from_stage1()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 3) Run additional prep on the data to create a unified dataset that can be used in a Power BI report\r\n",
							"\r\n",
							"# Process sectionmark data. Convert id values to use the Person.Id and Section.Id values set in the m365 data.\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentsectionmark'), 'SectionMark')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Person'), 'Person')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Section'), 'Section')\r\n",
							"df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\r\n",
							"sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\r\n",
							"sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\r\n",
							"from SectionMark sm, Person p, Section s \\\r\n",
							"where sm.student_id = p.ExternalId \\\r\n",
							"and sm.section_id = s.ExternalId\")\r\n",
							"#df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/SectionMark')\r\n",
							"\r\n",
							"# Repeat the above process, this time for student attendance\r\n",
							"# Convert id values to use the Person.Id, Org.Id and Section.Id values\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentattendance'), 'Attendance')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Org'), 'Org')\r\n",
							"df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, att.attendance_date AttendanceDate, \\\r\n",
							"att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\r\n",
							"att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\r\n",
							"from Attendance att, Org o, Person p, Section s \\\r\n",
							"where att.student_id = p.ExternalId \\\r\n",
							"and att.school_id = o.ExternalId \\\r\n",
							"and att.section_id = s.ExternalId\")\r\n",
							"#df.write.format('parquet').mode('overwrite').save(oea.stage2np +'/ContosoISD/Attendance')\r\n",
							"\r\n",
							"# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Course'), 'Course')\r\n",
							"df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\r\n",
							"#df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/Course')"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 4) Create spark db's that point to the data in the data lake to allow for connecting via Power BI through use of the Serverless SQL endpoint.\r\n",
							"contoso_sis.create_stage2_db('PARQUET')\r\n",
							"m365.create_stage2_db('PARQUET')\r\n",
							"\r\n",
							"#spark.sql('CREATE DATABASE IF NOT EXISTS s2_ContosoISD')\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.Activity using PARQUET location '\" + oea.stage2np + \"/m365/TechActivity'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.Calendar using PARQUET location '\" + oea.stage2np + \"/m365/Calendar'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.Org using PARQUET location '\" + oea.stage2np + \"/m365/Org'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.Person using PARQUET location '\" + oea.stage2np + \"/m365/Person'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.PersonIdentifier using PARQUET location '\" + oea.stage2np + \"/m365/PersonIdentifier'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.RefDefinition using PARQUET location '\" + oea.stage2np + \"/m365/RefDefinition'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.Section using PARQUET location '\" + oea.stage2np + \"/m365/Section'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.Session using PARQUET location '\" + oea.stage2np + \"/m365/Session'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.StaffOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StaffOrgAffiliation'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.StaffSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StaffSectionMembership'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.StudentOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StudentOrgAffiliation'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StudentSectionMembership'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.Course using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Course'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.Attendance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Attendance'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.SectionMark using PARQUET location '\" + oea.stage2np + \"/ContosoISD/SectionMark'\")\r\n",
							"#spark.sql(\"create table if not exists s2_ContosoISD.Students using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Students'\")\r\n",
							"\r\n",
							"print(f\"Created spark db's.\\nYou can now open the 'techInequityDashboardContoso v2.pbix' dashboard and change the datasource to point to: {oea.serverless_sql_endpoint}\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything\r\n",
							"You can uncomment line 11 in the cell below and run the cell to reset everything and walk through the process again from the top.\r\n",
							"\r\n",
							"Note: remember to comment out line 11 again to prevent accidental resetting of the example"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def reset_all_processing():\r\n",
							"    contoso_sis.delete_all_stages()\r\n",
							"    m365.delete_all_stages()\r\n",
							"    oea.rm_if_exists(oea.stage2np + '/ContosoISD')\r\n",
							"\r\n",
							"    oea.drop_db('s2_contoso_sis')\r\n",
							"    #oea.drop_db('s2_contosoisd')\r\n",
							"    oea.drop_db('s2_m365')\r\n",
							"\r\n",
							"# Uncomment the following line and run this cell to reset everything if you want to walk through the process again.\r\n",
							"#reset_all_processing()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ContosoSIS_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "examples"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1bb0437c-6a56-42cf-b550-ca293b1e22ab"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class ContosoSIS(BaseOEAModule):\n",
							"    def __init__(self, source_folder='contoso_sis', pseudonymize = True):\n",
							"        BaseOEAModule.__init__(self, source_folder, pseudonymize)\n",
							"        self.schemas['studentattendance'] = [['id', 'string', 'no-op'],\n",
							"                                            ['student_id', 'string', 'hash'],\n",
							"                                            ['school_year', 'integer', 'partition-by'],\n",
							"                                            ['school_id', 'string', 'no-op'],\n",
							"                                            ['attendance_date', 'timestamp', 'no-op'],\n",
							"                                            ['all_day', 'string', 'no-op'],\n",
							"                                            ['Period', 'short', 'no-op'],\n",
							"                                            ['section_id', 'string', 'no-op'],\n",
							"                                            ['AttendanceCode', 'string', 'no-op'],\n",
							"                                            ['PresenceFlag', 'boolean', 'no-op'],\n",
							"                                            ['attendance_status', 'string', 'no-op'],\n",
							"                                            ['attendance_type', 'string', 'no-op'],\n",
							"                                            ['attendance_sequence', 'short', 'no-op']]\n",
							"\n",
							"        self.schemas['studentsectionmark'] = [['id', 'string', 'no-op'],\n",
							"                                            ['student_id', 'string', 'hash'],\n",
							"                                            ['section_id', 'string', 'no-op'],\n",
							"                                            ['school_year', 'string', 'partition-by'],\n",
							"                                            ['term_id', 'string', 'no-op'],\n",
							"                                            ['numeric_grade_earned', 'short', 'no-op'],\n",
							"                                            ['alpha_grade_earned', 'string', 'no-op'],\n",
							"                                            ['is_final_grade', 'string', 'no-op'],\n",
							"                                            ['credits_attempted', 'short', 'no-op'],\n",
							"                                            ['credits_earned', 'short', 'no-op'],\n",
							"                                            ['grad_credit_type', 'string', 'no-op']]\n",
							"\n",
							"        self.schemas['studentdemographics'] = [['SIS ID', 'string', 'hash'],\n",
							"                                            ['FederalRaceCategory', 'string', 'no-op'],\n",
							"                                            ['PrimaryLanguage', 'string', 'no-op'],\n",
							"                                            ['ELLStatus', 'string', 'no-op'],\n",
							"                                            ['SpecialEducation', 'string', 'no-op'],\n",
							"                                            ['LowIncome', 'boolean', 'no-op']]                                            \n",
							"\n",
							"    def ingest(self):\n",
							"        oea.ingest_incremental_data(self.source_folder, 'studentattendance', self.schemas['studentattendance'], 'school_year', 'id')\n",
							"        oea.ingest_snapshot_data(self.source_folder, 'studentsectionmark', self.schemas['studentsectionmark'], 'school_year', 'id')\n",
							"        oea.ingest_delta_data(self.source_folder, 'studentdemographics', self.schemas['studentdemographics'], 'school_year', 'id')\n",
							"\n",
							"contoso_sis = ContosoSIS()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataGen_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {}
				},
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import random\r\n",
							"import json\r\n",
							"from faker import Faker\r\n",
							"\r\n",
							"\"\"\" From DataGenUtil.py \"\"\"\r\n",
							"def list_of_dict_to_csv(list_of_dict, includeHeaders = True):\r\n",
							"    csv_str = ''\r\n",
							"    if includeHeaders == True:\r\n",
							"        header = []\r\n",
							"        for column_name in list_of_dict[0].keys(): \r\n",
							"            if not column_name.startswith('_'): header.append(column_name)\r\n",
							"        csv_str += \",\".join(header) + \"\\n\"\r\n",
							"\r\n",
							"    for row in list_of_dict:\r\n",
							"        csv_str += obj_to_csv(row) + \"\\n\"\r\n",
							"\r\n",
							"    return csv_str\r\n",
							"\r\n",
							"def obj_to_csv(obj):\r\n",
							"    csv = ''\r\n",
							"    for key in obj:\r\n",
							"        if not (key.startswith('_')): csv += str(obj[key]) + ','\r\n",
							"    return csv[:-1]\r\n",
							"\r\n",
							"def list_of_dict_to_json(list_of_dict):\r\n",
							"    json_str = '['\r\n",
							"    for row in list_of_dict:\r\n",
							"        json_str += obj_to_json(row) + \",\\n\"\r\n",
							"    return json_str[:-2] + ']'\r\n",
							"\r\n",
							"def obj_to_json(obj):\r\n",
							"    json_dict = {}\r\n",
							"    for key in obj:\r\n",
							"        if not (key.startswith('_')): json_dict[key] = obj[key]\r\n",
							"    return json.dumps(json_dict)\r\n",
							"\r\n",
							"\"\"\" From EdFiDataGenerator.py \"\"\"\r\n",
							"GENDER = ['Male','Female']\r\n",
							"BOOLEAN = [True, False]\r\n",
							"OPERATIONAL_STATUS = ['Active','Inactive']\r\n",
							"CHARTER_STATUS = ['School Charter', 'Open Enrollment Charter', 'Not a Charter School']\r\n",
							"GRADE_LEVEL = ['First Grade','Second Grade','Third Grade','Fourth Grade','Fifth Grade','Sixth Grade','Seventh Grade','Eighth Grade','Ninth Grade','Tenth Grade','Eleventh Grade','Twelfth Grade']\r\n",
							"SCHOOL_TYPES = ['High School', 'Middle School', 'Elementary School']\r\n",
							"SUBJECT_NAMES = [('Math','Algebra'), ('Math','Geometry'), ('Language','English'), ('History','World History'),('Science','Biology'), ('Science','Health'), ('Technology',' Programming'), ('Physical Education','Sports'), ('Arts','Music')]\r\n",
							"LEVELS_OF_EDUCATION = ['Some College No Degree', 'Doctorate', 'Bachelor\\'s','Master\\'s']\r\n",
							"PERSONAL_INFORMATION_VERIFICATION_DESCRIPTIONS = ['Entry in family Bible', 'Other official document', 'State-issued ID', 'Hospital certificate', 'Passport', 'Parents affidavit', 'Immigration document/visa', 'Drivers license']\r\n",
							"RACES = ['Asian' , 'Native Hawaiian - Pacific Islander', 'American Indian - Alaska Native', 'White']\r\n",
							"\r\n",
							"class EdFiDataGenerator:\r\n",
							"    def __init__(self,number_students_per_school=100, include_optional_fields=True, school_year='2021', credit_conversion_factor = 2.0, number_of_grades_per_school = 5, is_current_school_year = True, graduation_plans_per_school = 10, unique_id_length = 5, number_staffs_per_school = 50, number_sections_per_school = 10):\r\n",
							"        # Set a seed value in Faker so it generates same values every run.\r\n",
							"        self.faker = Faker('en_US')\r\n",
							"        Faker.seed(1)\r\n",
							"\r\n",
							"        self.include_optional_fields = include_optional_fields\r\n",
							"        self.graduation_plans_per_school = graduation_plans_per_school\r\n",
							"        self.school_year = school_year\r\n",
							"        self.country = 'United States of America'\r\n",
							"        self.number_students_per_school = number_students_per_school\r\n",
							"        self.credit_conversion_factor = credit_conversion_factor\r\n",
							"        self.number_of_grades_per_school = number_of_grades_per_school\r\n",
							"        self.is_current_school_year = is_current_school_year\r\n",
							"        self.unique_id_length = unique_id_length\r\n",
							"        self.number_staffs_per_school = number_staffs_per_school\r\n",
							"        self.number_sections_per_school = number_sections_per_school\r\n",
							"\r\n",
							"    def get_descriptor_string(self, key, value):\r\n",
							"        return \"uri://ed-fi.org/{}#{}\".format(key,value)\r\n",
							"\r\n",
							"    def generate_data(self, num_of_schools, writer):\r\n",
							"        edfi_data = [self.create_school() for _ in range(num_of_schools)]\r\n",
							"        edfi_data_formatted = self.format_edfi_data(edfi_data)\r\n",
							"\r\n",
							"\r\n",
							"        writer.write(f'EdFi/School.json',list_of_dict_to_json(edfi_data_formatted['Schools']))\r\n",
							"        writer.write(f'EdFi/Student.json',list_of_dict_to_json(edfi_data_formatted['Students']))\r\n",
							"        writer.write(f'EdFi/StudentSchoolAssociation.json',list_of_dict_to_json(edfi_data_formatted['StudentSchoolAssociations']))\r\n",
							"        writer.write(f'EdFi/Course.json',list_of_dict_to_json(edfi_data_formatted['Courses']))\r\n",
							"        writer.write(f'EdFi/Calendar.json',list_of_dict_to_json(edfi_data_formatted['Calendars']))\r\n",
							"        writer.write(f'EdFi/Sessions.json',list_of_dict_to_json(edfi_data_formatted['Sessions']))\r\n",
							"        writer.write(f'EdFi/StaffSchoolAssociations.json',list_of_dict_to_json(edfi_data_formatted['StaffSchoolAssociations']))\r\n",
							"        writer.write(f'EdFi/Sections.json',list_of_dict_to_json(edfi_data_formatted['Sections']))\r\n",
							"        writer.write(f'EdFi/Staffs.json',list_of_dict_to_json(edfi_data_formatted['Staffs']))\r\n",
							"        writer.write(f'EdFi/StudentSectionAssociations.json',list_of_dict_to_json(edfi_data_formatted['StudentSectionAssociations']))\r\n",
							"        writer.write(f'EdFi/StaffSectionAssociations.json',list_of_dict_to_json(edfi_data_formatted['StaffSectionAssociations']))\r\n",
							"\r\n",
							"\r\n",
							"    def create_school(self):\r\n",
							"        school_type = random.choice(SCHOOL_TYPES)\r\n",
							"        school_name = self.faker.city() + ' ' + school_type\r\n",
							"        school = {\r\n",
							"            'Id': self.faker.uuid4().replace('-',''),\r\n",
							"            'SchoolId': self.faker.random_number(digits = self.unique_id_length),\r\n",
							"            'NameOfInstitution': school_name,\r\n",
							"            'OperationalStatusDescriptor': self.get_descriptor_string('OperationalStatusDescriptor',random.choice(OPERATIONAL_STATUS)),\r\n",
							"            'ShortNameOfInstitution': ''.join([word[0] for word in school_name.split()]),\r\n",
							"            'Website':''.join(['www.',school_name.lower().replace(' ',''),'.com']),\r\n",
							"            'AdministrativeFundingControlDescriptor': self.get_descriptor_string('AdministrativeFundingControlDescriptor',random.choice(['public', 'private']) + ' School'),\r\n",
							"            'CharterStatusDescriptor': self.get_descriptor_string('CharterStatusDescriptor',random.choice(CHARTER_STATUS)),\r\n",
							"            'SchoolTypeDescriptor': self.get_descriptor_string('SchoolTypeDescriptor','Regular'),\r\n",
							"            'TitleIPartASchoolDesignationDescriptor': self.get_descriptor_string('TitleIPartASchoolDesignationDescriptor','Not A Title I School'),\r\n",
							"            'Addresses': self.create_address() if self.include_optional_fields else '',\r\n",
							"            'EducationOrganizationCategories':[{'EducationOrganizationCategoryDescriptor': self.get_descriptor_string('educationOrganizationCategoryDescriptor','School')}],\r\n",
							"            'IdentificationCodes': [\r\n",
							"                {\r\n",
							"                    'educationOrganizationIdentificationSystemDescriptor': self.get_descriptor_string('educationOrganizationIdentificationSystemDescriptor','SEA'),\r\n",
							"                    'identificationCode': self.faker.random_number(digits=10)\r\n",
							"                }\r\n",
							"            ],\r\n",
							"            'InstitutionTelephones': self.create_telephones(),\r\n",
							"            'InternationalAddresses': [],\r\n",
							"            'SchoolCategories': [\r\n",
							"                {\r\n",
							"                    'SchoolCategoryDescriptor': self.get_descriptor_string('SchoolCategoryDescriptor',school_type)\r\n",
							"                }\r\n",
							"            ],\r\n",
							"            'gradeLevels': [\r\n",
							"                {'gradeLevelDescriptor': self.get_descriptor_string('GradeLevelDescriptor',random.choice(GRADE_LEVEL))} for _ in range(4)\r\n",
							"            ]\r\n",
							"        }\r\n",
							"\r\n",
							"        school['_SchoolYears'] = self.create_school_years()\r\n",
							"        school['_Calendars'] = self.create_calendars(school)\r\n",
							"        school['_Students'] = self.create_students()\r\n",
							"        school['_Courses'] = self.create_courses(school['SchoolId'],school['Id'],school_name)\r\n",
							"        school['_GraduationPlans'] = self.create_graduation_plans(school)\r\n",
							"        school['_StudentAssociations'] = self.create_student_school_associations(school)\r\n",
							"        school['_Staffs'] = self.create_staffs()\r\n",
							"        school['_StaffSchoolAssociations'] = self.create_staff_school_associations(school)\r\n",
							"        school['_Sessions'] = self.create_sessions(school)\r\n",
							"        school['_Sections'] = self.create_sections(school)\r\n",
							"        school['_StaffSectionAssociations'] = self.create_staff_section_associations(school)\r\n",
							"        school['_StudentSectionAssociations'] = self.create_student_section_associations(school)\r\n",
							"        return school\r\n",
							"\r\n",
							"    def create_students(self):\r\n",
							"        students = []\r\n",
							"        for _ in range(self.number_students_per_school):\r\n",
							"            gender = random.choice(GENDER)\r\n",
							"            fname = self.faker.first_name_male() if gender == 'Male' else self.faker.first_name_female()\r\n",
							"            students.append({\r\n",
							"                'Id': self.faker.uuid4().replace('-',''),\r\n",
							"                'StudentUniqueId': self.faker.random_number(digits = self.unique_id_length),\r\n",
							"                \"BirthCity\": self.faker.city(),\r\n",
							"                \"BirthDate\": str(self.faker.date_between(start_date='-18y',end_date='-5y')),\r\n",
							"                \"BirthSexDescriptor\": self.get_descriptor_string('birthStateAbbreviationDescriptor', gender),\r\n",
							"                \"FirstName\": fname,\r\n",
							"                \"IdentificationDocuments\": [],\r\n",
							"                \"LastSurname\": self.faker.last_name(),\r\n",
							"                \"OtherNames\": [\r\n",
							"                    {\r\n",
							"                        \"OtherNameTypeDescriptor\": self.get_descriptor_string('otherNameTypeDescriptor','Nickname'),\r\n",
							"                        \"FirstName\": self.faker.first_name_male() if gender == 'Male' else self.faker.first_name_female(),\r\n",
							"                        \"PersonalTitlePrefix\": 'Mr' if gender == 'Male' else 'Ms'\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"PersonalIdentificationDocuments\": [],\r\n",
							"                \"PersonalTitlePrefix\": 'Mr' if gender == 'Male' else 'Ms',\r\n",
							"                \"Visas\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"        })\r\n",
							"        return students\r\n",
							"\r\n",
							"\r\n",
							"    def create_student_school_associations(self,school):\r\n",
							"        result = []\r\n",
							"        graduation_plan_ids = [gp['Id'] for gp in school['_GraduationPlans']]\r\n",
							"        for student in school['_Students']:\r\n",
							"            result.append({\r\n",
							"                'Id': self.faker.uuid4().replace('-',''),\r\n",
							"                \"GraduationPlanReference\": {\r\n",
							"                    \"EducationOrganizationId\": school['SchoolId'],\r\n",
							"                    \"GraduationPlanTypeDescriptor\": \"uri://ed-fi.org/GraduationPlanTypeDescriptor#Minimum\",\r\n",
							"                    \"GraduationSchoolYear\": self.school_year,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GraduationPlan\",\r\n",
							"                        \"href\": '/ed-fi/graduationPlans/{}'.format(random.choice(graduation_plan_ids))\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"SchoolReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": '/ed-fi/schools/{}'.format(school['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"StudentReference\": {\r\n",
							"                    \"StudentUniqueId\": student['StudentUniqueId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Student\",\r\n",
							"                        \"href\": \"/ed-fi/students/{}\".format(student['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"EntryDate\": str(self.faker.date_between(start_date='-5y',end_date='today')),\r\n",
							"                \"EntryGradeLevelDescriptor\": \"uri://ed-fi.org/GradeLevelDescriptor#{}\".format(random.choice(GRADE_LEVEL)),\r\n",
							"                \"AlternativeGraduationPlans\": [],\r\n",
							"                \"EducationPlans\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return result\r\n",
							"\r\n",
							"    def create_calendars(self,school):\r\n",
							"        return {\r\n",
							"            'Id': self.faker.uuid4().replace('-',''),\r\n",
							"            'CalendarCode':self.faker.random_number(digits = self.unique_id_length),\r\n",
							"            \"SchoolReference\": {\r\n",
							"                \"SchoolId\": school['SchoolId'],\r\n",
							"                \"Link\": {\r\n",
							"                    \"rel\": \"School\",\r\n",
							"                    \"href\": \"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SchoolYearTypeReference\": {\r\n",
							"                \"SchoolYear\": self.school_year,\r\n",
							"                \"Link\": {\r\n",
							"                    \"rel\": \"SchoolYearType\",\r\n",
							"                    \"href\": \"/ed-fi/schoolYearTypes/{}\".format(school['_SchoolYears']['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            'CalendarTypeDescriptor': self.get_descriptor_string('calendarTypeDescriptor','Student Specific'),\r\n",
							"            'GradeLevel': []\r\n",
							"        }\r\n",
							"\r\n",
							"    def create_address(self):\r\n",
							"        address = []\r\n",
							"        state = self.faker.state_abbr()\r\n",
							"        for n in ['Physical', 'Mailing']:\r\n",
							"            address.append({\r\n",
							"                'AddressType':n,\r\n",
							"                'City':self.faker.city(),\r\n",
							"                'PostalCode':self.faker.postcode(),\r\n",
							"                'StateAbbreviation':state,\r\n",
							"                'StreetNumberName':self.faker.street_name()\r\n",
							"            })\r\n",
							"        return address\r\n",
							"\r\n",
							"    def create_courses(self,school_id,id,school_name):\r\n",
							"        courses = []\r\n",
							"        for subject,course_name in SUBJECT_NAMES:\r\n",
							"            courseCode = '{}-{}'.format(course_name[0:3].upper(),random.choice(range(1,5)))\r\n",
							"            courses.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"EducationOrganizationReference\": {\r\n",
							"                    \"EducationOrganizationId\": school_id,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": \"/ed-fi/schools/{}\".format(id)\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"CourseCode\": courseCode,\r\n",
							"                \"AcademicSubjectDescriptor\": self.get_descriptor_string('academicSubjectDescriptor', subject),\r\n",
							"                \"CourseDefinedByDescriptor\": self.get_descriptor_string('CourseDefinedByDescriptor','SEA'),\r\n",
							"                \"CourseDescription\": 'Description about {}'.format(course_name),\r\n",
							"                \"CourseGPAApplicabilityDescriptor\": self.get_descriptor_string('CourseGPAApplicabilityDescriptor',random.choice(['Applicable','Not Applicable'])),\r\n",
							"                \"CourseTitle\": course_name,\r\n",
							"                \"HighSchoolCourseRequirement\": random.choice(BOOLEAN),\r\n",
							"                \"NumberOfParts\": 1,\r\n",
							"                \"CompetencyLevels\": [],\r\n",
							"                \"IdentificationCodes\": [\r\n",
							"                    {\r\n",
							"                        \"CourseIdentificationSystemDescriptor\": self.get_descriptor_string('CourseIdentificationSystemDescriptor','LEA course code'),\r\n",
							"                        \"CourseCatalogURL\": \"http://www.{}.edu/coursecatalog\".format(school_name.lower().replace(' ','')),\r\n",
							"                        \"IdentificationCode\": courseCode\r\n",
							"                    },\r\n",
							"                    {\r\n",
							"                        \"CourseIdentificationSystemDescriptor\": self.get_descriptor_string('CourseIdentificationSystemDescriptor','State course code'),\r\n",
							"                        \"IdentificationCode\": self.faker.random_number(digits = self.unique_id_length)\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"LearningObjectives\": [],\r\n",
							"                \"LearningStandards\": [\r\n",
							"                    {\r\n",
							"                        \"LearningStandardReference\": {\r\n",
							"                            \"LearningStandardId\": self.faker.random_number(digits = self.unique_id_length),\r\n",
							"                            \"Link\": {\r\n",
							"                                \"rel\": \"LearningStandard\",\r\n",
							"                                \"href\": \"/ed-fi/learningStandards/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                            }\r\n",
							"                        }\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"LevelCharacteristics\": [\r\n",
							"                    {\r\n",
							"                        \"CourseLevelCharacteristicDescriptor\": self.get_descriptor_string('CourseLevelCharacteristicDescriptor','Core Subject')\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"OfferedGradeLevels\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return courses\r\n",
							"\r\n",
							"\r\n",
							"    def create_graduation_plans(self, school):\r\n",
							"        graduation_plans = []\r\n",
							"        for _ in range(self.graduation_plans_per_school):\r\n",
							"            graduation_plans.append({\r\n",
							"                'Id': self.faker.uuid4().replace('-',''),\r\n",
							"                \"EducationOrganizationReference\": {\r\n",
							"                    \"EducationOrganizationId\": school['SchoolId'],\r\n",
							"                    \"link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": \"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"GraduationSchoolYearTypeReference\": {\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"SchoolYearType\",\r\n",
							"                        \"href\": \"/ed-fi/schoolYearTypes/{}\".format(school['_SchoolYears']['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"GraduationPlanTypeDescriptor\": self.get_descriptor_string('GraduationPlanTypeDescriptor', random.choice(['Minimum','Recommended'])),\r\n",
							"                \"TotalRequiredCredits\": random.choice(range(20,30)),\r\n",
							"                \"CreditsByCourses\": [],\r\n",
							"                \"CreditsByCreditCategories\": [\r\n",
							"                    {\r\n",
							"                        \"CreditCategoryDescriptor\": self.get_descriptor_string('CreditCategoryDescriptor','Honors'),\r\n",
							"                        \"Credits\": random.choice(range(5,15))\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"CreditsBySubjects\": [],\r\n",
							"                \"RequiredAssessments\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return graduation_plans\r\n",
							"\r\n",
							"    def create_school_years(self):\r\n",
							"        return {\r\n",
							"            'Id': self.faker.uuid4().replace('-',''),\r\n",
							"            'SchoolYear': self.school_year,\r\n",
							"            'CurrentSchoolYear': self.is_current_school_year,\r\n",
							"            'schoolYearDescription': 'Description about school year',\r\n",
							"            '_etag': self.faker.random_number(digits=10)\r\n",
							"        }\r\n",
							"\r\n",
							"    def create_telephones(self):\r\n",
							"        return [\r\n",
							"            {\r\n",
							"                'InstitutionTelephoneNumberTypeDescriptor': self.get_descriptor_string('InstitutionTelephoneNumberTypeDescriptor', _),\r\n",
							"                \"TelephoneNumber\": self.faker.phone_number()\r\n",
							"            }\r\n",
							"            for _ in ['Fax','Main']\r\n",
							"        ]\r\n",
							"\r\n",
							"    def create_staffs(self):\r\n",
							"        staffs = []\r\n",
							"        for _ in range(self.number_staffs_per_school):\r\n",
							"            gender = random.choice(GENDER)\r\n",
							"            fname = self.faker.first_name_male() if gender == 'Male' else self.faker.first_name_female()\r\n",
							"            lname = self.faker.last_name()\r\n",
							"            staffs.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"StaffUniqueId\": self.faker.random_number(digits = self.unique_id_length),\r\n",
							"                \"BirthDate\": str(self.faker.date_between(start_date='-60y',end_date='-30y')),\r\n",
							"                \"FirstName\": fname,\r\n",
							"                \"HighestCompletedLevelOfEducationDescriptor\": self.get_descriptor_string('LevelOfEducationDescriptor', value = random.choice(LEVELS_OF_EDUCATION)),\r\n",
							"                \"HispanicLatinoEthnicity\": random.choice(BOOLEAN),\r\n",
							"                \"LastSurname\": lname,\r\n",
							"                \"LoginId\": '{}{}'.format(fname[0],lname.lower()),\r\n",
							"                \"PersonalTitlePrefix\": 'Mr' if gender == 'Male' else 'Ms',\r\n",
							"                \"SexDescriptor\": self.get_descriptor_string('SexDescriptor', value = gender),\r\n",
							"                \"YearsOfPriorProfessionalExperience\": random.choice(range(50)),\r\n",
							"                \"Addresses\": self.create_address(),\r\n",
							"                \"AncestryEthnicOrigins\": [],\r\n",
							"                \"Credentials\": [\r\n",
							"                    {\r\n",
							"                        \"CredentialReference\": {\r\n",
							"                            \"CredentialIdentifier\": self.faker.random_number(digits = 10),\r\n",
							"                            \"StateOfIssueStateAbbreviationDescriptor\": self.get_descriptor_string('StateAbbreviationDescriptor', 'TX'),\r\n",
							"                            \"Link\": {\r\n",
							"                                \"rel\": \"Credential\",\r\n",
							"                                \"href\": \"/ed-fi/credentials/\" + self.faker.uuid4().replace('-','')\r\n",
							"                            }\r\n",
							"                        }\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"ElectronicMails\": [\r\n",
							"                    {\r\n",
							"                        \"ElectronicMailAddress\": \"{}{}@edfi.org\".format(fname,lname),\r\n",
							"                        \"ElectronicMailTypeDescriptor\": self.get_descriptor_string('ElectronicMailTypeDescriptor','Work')\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"IdentificationCodes\": [\r\n",
							"                    {\r\n",
							"                        \"StaffIdentificationSystemDescriptor\": self.get_descriptor_string('StaffIdentificationSystemDescriptor','State'),\r\n",
							"                        \"IdentificationCode\": self.faker.random_number(digits = self.unique_id_length)\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"IdentificationDocuments\": [],\r\n",
							"                \"InternationalAddresses\": self.create_address(),\r\n",
							"                \"Languages\": [],\r\n",
							"                \"OtherNames\": [self.faker.first_name_male() if gender == 'Male' else self.faker.first_name_female()],\r\n",
							"                \"PersonalIdentificationDocuments\": [\r\n",
							"                    {\r\n",
							"                        \"IdentificationDocumentUseDescriptor\": \"uri://ed-fi.org/IdentificationDocumentUseDescriptor#Personal Information Verification\",\r\n",
							"                        \"PersonalInformationVerificationDescriptor\": self.get_descriptor_string('PersonalInformationVerificationDescriptor', value = random.choice(PERSONAL_INFORMATION_VERIFICATION_DESCRIPTIONS))\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"Races\": [\r\n",
							"                    {\r\n",
							"                        \"RaceDescriptor\": self.get_descriptor_string('RaceDescriptor', value = random.choice(RACES))\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return staffs\r\n",
							"\r\n",
							"    def create_sessions(self, school):\r\n",
							"\r\n",
							"        return [{\r\n",
							"            \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"            \"SchoolReference\":{\r\n",
							"                \"SchoolId\":school['SchoolId'],\r\n",
							"                \"Link\":{\r\n",
							"                    \"rel\":\"School\",\r\n",
							"                    \"href\":\"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SchoolYearTypeReference\": {\r\n",
							"                \"SchoolYear\": self.school_year,\r\n",
							"                \"Link\": {\r\n",
							"                    \"rel\": \"SchoolYearType\",\r\n",
							"                    \"href\": \"/ed-fi/schoolYearTypes/{}\".format(school['_SchoolYears']['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SessionName\": \"{} - {} Fall Semester\".format(int(self.school_year) - 1, self.school_year ),\r\n",
							"            \"BeginDate\": \"{}-08-{}\".format(int(self.school_year) - 1, random.randint(1,30)),\r\n",
							"            \"EndDate\": \"{}-12-{}\".format(int(self.school_year) - 1, random.randint(1,30)),\r\n",
							"            \"TermDescriptor\": self.get_descriptor_string('TermDescriptor', 'Fall Semester'),\r\n",
							"            \"TotalInstructionalDays\": random.randint(60,130),\r\n",
							"            \"GradingPeriods\": [\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#First Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 1,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Second Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 2,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Third Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 3,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                }\r\n",
							"            ],\r\n",
							"            \"_etag\": self.faker.random_number(digits=10)\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"            \"SchoolReference\":{\r\n",
							"                \"SchoolId\":school['SchoolId'],\r\n",
							"                \"Link\":{\r\n",
							"                    \"rel\":\"School\",\r\n",
							"                    \"href\":\"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SchoolYearTypeReference\": {\r\n",
							"                \"SchoolYear\": self.school_year,\r\n",
							"                \"Link\": {\r\n",
							"                    \"rel\": \"SchoolYearType\",\r\n",
							"                    \"href\": \"/ed-fi/schoolYearTypes/{}\".format(school['_SchoolYears']['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SessionName\": \"{} - {} Spring Semester\".format(int(self.school_year) - 1, self.school_year),\r\n",
							"            \"BeginDate\": \"{}-01-{}\".format(self.school_year, random.randint(1,30)),\r\n",
							"            \"EndDate\": \"{}-05-{}\".format(self.school_year, random.randint(1,30)),\r\n",
							"            \"TermDescriptor\": self.get_descriptor_string('TermDescriptor', 'Spring Semester'),\r\n",
							"            \"TotalInstructionalDays\": random.randint(60,130),\r\n",
							"            \"GradingPeriods\": [\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Fourth Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 4,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Fifth Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 5,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Sixth Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 6,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                }\r\n",
							"            ],\r\n",
							"            \"_etag\": self.faker.random_number(digits=10)\r\n",
							"        }]\r\n",
							"\r\n",
							"    def create_sections(self, school):\r\n",
							"        sections = []\r\n",
							"        for _ in range(self.number_sections_per_school):\r\n",
							"            semesterType = random.choice(['Spring', 'Fall'])\r\n",
							"            subjectName = random.choice(SUBJECT_NAMES)[1]\r\n",
							"            subjectNumber = random.randint(1,5)\r\n",
							"            sections.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"CourseOfferingReference\": {\r\n",
							"                    \"LocalCourseCode\": \"{}-{}\".format(subjectName[0:3].upper(), subjectNumber),\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"SessionName\": \"{} - {} {} Semester\".format(int(self.school_year) - 1, semesterType, self.school_year),\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"CourseOffering\",\r\n",
							"                        \"href\": \"/ed-fi/courseOfferings/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"LocationReference\": {\r\n",
							"                    \"ClassroomIdentificationCode\": self.faker.random_number(digits = 3),\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Location\",\r\n",
							"                        \"href\": \"/ed-fi/locations/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"LocationSchoolReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": \"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"SectionIdentifier\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"AvailableCredits\": random.randint(1,4),\r\n",
							"                \"EducationalEnvironmentDescriptor\": self.get_descriptor_string('EducationalEnvironmentDescriptor','Classroom'),\r\n",
							"                \"SectionName\": \"{} {}\".format(subjectName, subjectNumber),\r\n",
							"                \"SequenceOfCourse\": random.randint(1,5),\r\n",
							"                \"Characteristics\": [],\r\n",
							"                \"ClassPeriods\": [\r\n",
							"                {\r\n",
							"                    \"ClassPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"ClassPeriodName\": \"{} - Traditional\".format(random.randint(1,5)),\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"ClassPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/classPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                }\r\n",
							"                ],\r\n",
							"                \"CourseLevelCharacteristics\": [],\r\n",
							"                \"OfferedGradeLevels\": [],\r\n",
							"                \"Programs\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return sections\r\n",
							"\r\n",
							"    def create_student_section_associations(self, school):\r\n",
							"        student_section_associations = []\r\n",
							"        session = random.choice(school['_Sessions'])\r\n",
							"        for student in school['_Students']:\r\n",
							"            course = random.choice(school['_Courses'])\r\n",
							"            section = random.choice(school['_Sections'])\r\n",
							"            student_section_associations.append({\r\n",
							"                    \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                    \"SectionReference\": {\r\n",
							"                        \"LocalCourseCode\": course['CourseCode'],\r\n",
							"                        \"SchoolId\": school['SchoolId'],\r\n",
							"                        \"SchoolYear\": self.school_year,\r\n",
							"                        \"SectionIdentifier\": section['SectionIdentifier'],\r\n",
							"                        \"SessionName\": session['SessionName'],\r\n",
							"                        \"Link\": {\r\n",
							"                            \"rel\": \"Section\",\r\n",
							"                            \"href\": \"/ed-fi/sections/{}\".format(section['Id'])\r\n",
							"                        }\r\n",
							"                    },\r\n",
							"                    \"StudentReference\": {\r\n",
							"                        \"StudentUniqueId\": student['StudentUniqueId'],\r\n",
							"                        \"Link\": {\r\n",
							"                            \"rel\": \"Student\",\r\n",
							"                            \"href\": \"/ed-fi/students/{}\".format(student['Id'])\r\n",
							"                        }\r\n",
							"                    },\r\n",
							"                    \"BeginDate\": session['BeginDate'],\r\n",
							"                    \"EndDate\": session['EndDate'],\r\n",
							"                    \"HomeroomIndicator\": random.choice(BOOLEAN),\r\n",
							"                    \"_etag\": self.faker.random_number(digits = 10)\r\n",
							"                })\r\n",
							"        return student_section_associations\r\n",
							"\r\n",
							"    def create_staff_section_associations(self,school):\r\n",
							"        staff_section_associations = []\r\n",
							"        for staff in school['_Staffs']:\r\n",
							"            session = random.choice(school['_Sessions'])\r\n",
							"            section = random.choice(school['_Sections'])\r\n",
							"            staff_section_associations.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"SectionReference\": {\r\n",
							"                    \"LocalCourseCode\": section['CourseOfferingReference']['LocalCourseCode'],\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"SectionIdentifier\": section['SectionIdentifier'],\r\n",
							"                    \"SessionName\": session['SessionName'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Section\",\r\n",
							"                        \"href\": \"/ed-fi/sections/{}\".format(section['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"StaffReference\": {\r\n",
							"                    \"StaffUniqueId\": staff['StaffUniqueId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Staff\",\r\n",
							"                        \"href\": \"/ed-fi/staffs/{}\".format(staff['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"BeginDate\": session['BeginDate'],\r\n",
							"                \"ClassroomPositionDescriptor\": \"uri://ed-fi.org/ClassroomPositionDescriptor#Teacher of Record\",\r\n",
							"                \"EndDate\": session['EndDate'],\r\n",
							"                \"_etag\": self.faker.uuid4().replace('-','')\r\n",
							"            })\r\n",
							"        return staff_section_associations\r\n",
							"\r\n",
							"\r\n",
							"    def create_staff_school_associations(self, school):\r\n",
							"        staff_school_associations = []\r\n",
							"        for staff in school['_Staffs']:\r\n",
							"            staff_school_associations.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"SchoolReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": \"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"StaffReference\": {\r\n",
							"                    \"StaffUniqueId\": staff['StaffUniqueId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Staff\",\r\n",
							"                        \"href\": \"/ed-fi/staffs/{}\".format(staff['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"ProgramAssignmentDescriptor\": self.get_descriptor_string('ProgramAssignmentDescriptor','Regular Education'),\r\n",
							"                \"AcademicSubjects\": [\r\n",
							"                    {\r\n",
							"                        \"AcademicSubjectDescriptor\": self.get_descriptor_string('AcademicSubjectDescriptor',random.choice(SUBJECT_NAMES)[0])\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"GradeLevels\": [\r\n",
							"                    {'GradeLevelDescriptor': self.get_descriptor_string('GradeLevelDescriptor',random.choice(GRADE_LEVEL))} for _ in range(4)\r\n",
							"            ],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return staff_school_associations\r\n",
							"\r\n",
							"    def format_edfi_data(self,data):\r\n",
							"        result = {\r\n",
							"            'Schools':[],\r\n",
							"            'Students':[],\r\n",
							"            'Calendars':[],\r\n",
							"            'Courses':[],\r\n",
							"            'StudentSchoolAssociations':[],\r\n",
							"            'Staffs':[],\r\n",
							"            'Sections': [],\r\n",
							"            'StaffSchoolAssociations':[],\r\n",
							"            'Sessions':[],\r\n",
							"            'StudentSectionAssociations':[],\r\n",
							"            'StaffSectionAssociations':[]\r\n",
							"\r\n",
							"        }\r\n",
							"        for school in data:\r\n",
							"            result['Schools'].append({key: school[key] for key in school if not (key.startswith('_')) })\r\n",
							"            result['Students'] += school['_Students']\r\n",
							"            result['Courses'] += school['_Courses']\r\n",
							"            result['StudentSchoolAssociations'] += school['_StudentAssociations']\r\n",
							"            result['Calendars'].append(school['_Calendars'])\r\n",
							"            result['Staffs'] += school['_Staffs']\r\n",
							"            result['Sections'] += school['_Sections']\r\n",
							"            result['StaffSchoolAssociations'] += school['_StaffSchoolAssociations']\r\n",
							"            result['Sessions'] += school['_Sessions']\r\n",
							"            result['StudentSectionAssociations'] += school['_StudentSectionAssociations']\r\n",
							"            result['StaffSectionAssociations'] += school['_StaffSectionAssociations']\r\n",
							"\r\n",
							"\r\n",
							"        return result\r\n"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Insights_module_ingestion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "examples"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a3c32cab-51c3-4e5b-bbef-712b55b7db3c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Microsoft Insights Module Example Notebook"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /NEW_Insights_py"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 0) Initialize the OEA framework and Insights module class notebook.\r\n",
							"oea = OEA()\r\n",
							"insights = Insights()"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"insights.ingest()"
						],
						"outputs": [],
						"execution_count": 23
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Insights_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8c06c46c-725e-4388-aa91-55b5d1cbfcb9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Microsoft Insights Class"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class Insights(BaseOEAModule):\r\n",
							"    \"\"\"    Provides data processing methods for MS Insights data.\r\n",
							"    Data is expected to be received via ADS into stage1np/ms_insights\r\n",
							"    The structure of the folders in stage1np will then be something like:\r\n",
							"        -> stage1np/ms_insights/activity/2021-06-02\r\n",
							"            -> stage1np/ms_insights/activity/2021-06-02/ApplicationUsage.csv\r\n",
							"        -> stage1np/ms_insights/roster/2021-06-02T06-05-11/\r\n",
							"            -> stage1np/ms_insights/roster/2021-06-02T06-05-11/AadUser\r\n",
							"            -> stage1np/ms_insights/roster/2021-06-02T06-05-11/Person\r\n",
							"            etc\r\n",
							"\r\n",
							"    In stage2, everything is written to stage2np/ms_insights and stage2p/ms_insights\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    def __init__(self, source_folder= 'SDS/M365'):\r\n",
							"        BaseOEAModule.__init__(self, source_folder)\r\n",
							"\r\n",
							"        self.stage1np_activity = self.stage1np + '/activity'\r\n",
							"        self.stage1np_roster = self.stage1np + '/roster'\r\n",
							"\r\n",
							"        self.schemas['TechActivity'] = [['SignalType', 'string', 'no-op'],\r\n",
							"                        ['StartTime', 'timestamp', 'no-op'],\r\n",
							"                        ['UserAgent', 'string', 'no-op'],\r\n",
							"                        ['SignalId', 'string', 'no-op'],\r\n",
							"                        ['SisClassId', 'string', 'no-op'],\r\n",
							"                        ['ClassId', 'string', 'no-op'],\r\n",
							"                        ['ChannelId', 'string', 'no-op'],\r\n",
							"                        ['AppName', 'string', 'no-op'],\r\n",
							"                        ['ActorId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['ActorRole', 'string', 'no-op'],\r\n",
							"                        ['SchemaVersion', 'string', 'no-op'],\r\n",
							"                        ['AssignmentId', 'string', 'no-op'],\r\n",
							"                        ['SubmissionId', 'string', 'no-op'],\r\n",
							"                        ['SubmissionCreatedTime','timestamp','no-opt'], \r\n",
							"                        ['Action', 'string', 'no-op'],\r\n",
							"                        ['DueDate', 'timestamp', 'no-op'],\r\n",
							"                        ['ClassCreationDate', 'timestamp', 'no-op'],\r\n",
							"                        ['Grade', 'string', 'no-op'],\r\n",
							"                        ['SourceFileExtension', 'string', 'no-op'],\r\n",
							"                        ['MeetingDuration', 'string', 'no-op'], \r\n",
							"                        ['MeetingSessionId', 'string', 'no-op'],\r\n",
							"                        ['MeetingType', 'string','no-op' ],\r\n",
							"                        ['ReadingSubmissionWordsPerMinute','integer'      , 'no-op'],      \r\n",
							"                        ['ReadingSubmissionAccuracyScore',          'integer' , 'no-op'],     \r\n",
							"                        ['ReadingSubmissionMispronunciationsCount','integer' , 'no-op' ],      \r\n",
							"                        ['ReadingSubmissionRepetitionsCount','integer'       , 'no-op'   ],      \r\n",
							"                        ['ReadingSubmissionInsertionsCount','integer'       , 'no-op'   ],    \r\n",
							"                        ['ReadingSubmissionOmissionsCount','integer'       , 'no-op'   ],     \r\n",
							"                        ['ReadingSubmissionAttemptNumber','integer'       , 'no-op'   ],       \r\n",
							"                        ['ReadingAssignmentWordCount','integer'       , 'no-op'        ],      \r\n",
							"                        ['ReadingAssignmentFleschKincaidGradeLevel','integer'          , 'no-op' ],  \r\n",
							"                        ['ReadingAssignmentLanguage',           'string'        , 'no-op'  ],\r\n",
							"                        ['year', 'integer', 'no-op'] ]\r\n",
							"\r\n",
							"        self.schemas['AadGroup'] = [['ObjectId', 'string', 'hash'],\r\n",
							"                        ['DisplayName', 'string', 'mask'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Mail', 'string', 'mask'],\r\n",
							"                        ['MailNickname', 'string', 'mask'],\r\n",
							"                        ['AnchorId', 'string', 'hash'],\r\n",
							"                        ['SectionId', 'string', 'no-op']]                           \r\n",
							"        self.schemas['AadGroupMembership'] = [['GroupObjectId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Role', 'string', 'no-op'],\r\n",
							"                        ['UserObjectId', 'string', 'hash-no-lookup']]  \r\n",
							"        self.schemas['AadUser'] = [['ObjectId', 'string', 'hash'],\r\n",
							"                        ['AnchorId', 'string', 'hash'],\r\n",
							"                        ['DisplayName', 'string', 'mask'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['GivenName', 'string', 'mask'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Mail', 'string', 'mask'],\r\n",
							"                        ['MailNickname', 'string', 'mask'],\r\n",
							"                        ['Role', 'string', 'no-op'],\r\n",
							"                        ['Surname', 'string', 'mask'],\r\n",
							"                        ['UserPrincipalName', 'string', 'hash'],\r\n",
							"                        ['StudentId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['TeacherId', 'string', 'hash-no-lookup']] \r\n",
							"        self.schemas['AadUserPersonMapping'] = [['ObjectId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup']] \r\n",
							"        self.schemas['Course'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['AcademicYearSessionId', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['IsActiveInSession', 'boolean', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op'],\r\n",
							"                        ['OrganizationId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['Code', 'string', 'no-op']] \r\n",
							"        self.schemas['CourseGradeLevel'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['CourseId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['RefGradeLevelId', 'string', 'no-op']] \r\n",
							"        self.schemas['CourseSubject'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['CourseId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['RefAcademicSubjectId', 'string', 'no-op']] \r\n",
							"        self.schemas['Enrollment'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['IsActiveInSession', 'boolean', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefSectionRoleId', 'string', 'no-op'],\r\n",
							"                        ['SectionId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['EntryDate', 'string', 'no-op'],\r\n",
							"                        ['ExitDate', 'string', 'no-op'],\r\n",
							"                        ['IsPrimaryStaffForSection', 'boolean', 'no-op']] \r\n",
							"        self.schemas['Organization'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op'],\r\n",
							"                        ['RefOrganizationTypeId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['Identifier', 'string', 'no-op'],\r\n",
							"                        ['ParentOrganizationId', 'string', 'no-op']] \r\n",
							"        self.schemas['Person'] = [['Id', 'string', 'hash'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['GivenName', 'string', 'mask'],\r\n",
							"                        ['MiddleName', 'string', 'mask'],\r\n",
							"                        ['PreferredGivenName', 'string', 'mask'],\r\n",
							"                        ['PreferredMiddleName', 'string', 'mask'],\r\n",
							"                        ['PreferredSurname', 'string', 'mask'],\r\n",
							"                        ['Surname', 'string', 'mask']] \r\n",
							"        self.schemas['PersonDemographic'] = [['PersonId', 'string', 'hash'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['BirthCity', 'string', 'mask'],\r\n",
							"                        ['BirthCountryCode', 'string', 'mask'],\r\n",
							"                        ['BirthDate', 'string', 'mask'],\r\n",
							"                        ['BirthState', 'string', 'mask'],\r\n",
							"                        ['RefSexId', 'string', 'mask']] \r\n",
							"        self.schemas['PersonDemographicEthnicity'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefEthnicityId', 'string', 'mask']] \r\n",
							"        self.schemas['PersonDemographicPersonFlag'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefPersonFlagId', 'string', 'mask']] \r\n",
							"        self.schemas['PersonDemographicRace'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefRaceId', 'string', 'mask']] \r\n",
							"        self.schemas['PersonEmailAddress'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['EmailAddress', 'string', 'mask'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['PriorityOrder', 'short', 'no-op'],\r\n",
							"                        ['RefEmailAddressTypeId', 'string', 'no-op']] \r\n",
							"        self.schemas['PersonIdentifier'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Identifier', 'string', 'mask'],\r\n",
							"                        ['IsPresentInSource', 'boolean', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefIdentifierTypeId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op']] \r\n",
							"        self.schemas['PersonOrganizationRole'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['IsActiveInSession', 'boolean', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['OrganizationId', 'string', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefRoleId', 'string', 'no-op'],\r\n",
							"                        ['SessionId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['IsPrimary', 'boolean', 'no-op'],\r\n",
							"                        ['RefGradeLevelId', 'string', 'no-op'],\r\n",
							"                        ['RoleEndDate', 'string', 'mask'],\r\n",
							"                        ['RoleStartDate', 'string', 'mask']] \r\n",
							"        self.schemas['PersonPhoneNumber'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['PhoneNumber', 'string', 'mask'],\r\n",
							"                        ['PriorityOrder', 'short', 'no-op'],\r\n",
							"                        ['RefPhoneNumberTypeId', 'string', 'no-op']] \r\n",
							"        self.schemas['PersonRelationship'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefPersonRelationshipId', 'string', 'no-op'],\r\n",
							"                        ['RelatedPersonId', 'string', 'hash-no-lookup']] \r\n",
							"        self.schemas['RefDefinition'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['Code', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Namespace', 'string', 'no-op'],\r\n",
							"                        ['RefType', 'string', 'no-op'],\r\n",
							"                        ['SortOrder', 'short', 'no-op']] \r\n",
							"        self.schemas['Section'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op'],\r\n",
							"                        ['OrganizationId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['Code', 'string', 'no-op'],\r\n",
							"                        ['CourseId', 'string', 'no-op'],\r\n",
							"                        ['Location', 'string', 'no-op']] \r\n",
							"        self.schemas['SectionGradeLevel'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['RefGradeLevelId', 'string', 'no-op'],\r\n",
							"                        ['SectionId', 'string', 'no-op']] \r\n",
							"        self.schemas['SectionSession'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['IsActiveInSession', 'boolean', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['SectionId', 'string', 'no-op'],\r\n",
							"                        ['SessionId', 'string', 'no-op']] \r\n",
							"        self.schemas['SectionSubject'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['RefAcademicSubjectId', 'string', 'no-op'],\r\n",
							"                        ['SectionId', 'string', 'no-op']] \r\n",
							"        self.schemas['Session'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['EndDate', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op'],\r\n",
							"                        ['RefAcademicYearId', 'string', 'no-op'],\r\n",
							"                        ['RefSessionTypeId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['StartDate', 'string', 'no-op'],\r\n",
							"                        ['ParentSessionId', 'string', 'no-op']] \r\n",
							"        self.schemas['SourceSystem'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op']] \r\n",
							"    \r\n",
							"    def ingest(self):\r\n",
							"        \"\"\"  Processes insights data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
							"        logger.info(\"Processing microsoft_insights data from: \" + self.stage1np)\r\n",
							"        \r\n",
							"        items = mssparkutils.fs.ls(self.stage1np)\r\n",
							"        for item in items:\r\n",
							"            if item.name == \"activity\":\r\n",
							"                self.process_insights_activity_stage1_data()\r\n",
							"            elif item.name == \"roster\":\r\n",
							"                self.process_roster()\r\n",
							"            elif item.name == \"schemas\":\r\n",
							"                logger.info(\"ignoring ingestion of the schemas folder\")\r\n",
							"            elif item.name == \"current.manifest.cdm.json\":\r\n",
							"                logger.info(\"ignoring ingestion of the manifest json\")\r\n",
							"            else:\r\n",
							"                logger.info(\"No defined function for processing this insights data\")\r\n",
							"        \r\n",
							"        logger.info(\"Finished ingesting insights data from stage 1 to stage 2\")\r\n",
							"\r\n",
							"    def process_insights_activity_stage1_data(self, date_folder_files=None):\r\n",
							"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights activity data from: \" + self.stage1np_activity)\r\n",
							"\r\n",
							"        # Currently not using the OEA ingest_incremental_data function due to pulling out the partition folders\r\n",
							"        activity_spark_schema = oea.to_spark_schema(self.schemas['TechActivity'])\r\n",
							"\r\n",
							"        if date_folder_files is None: \r\n",
							"            date_folder_files = '/*/*.csv' \r\n",
							"        else:\r\n",
							"            date_folder_files= date_folder_files+'/*.csv'\r\n",
							"        \r\n",
							"        df = spark.readStream.csv(self.stage1np_activity + date_folder_files , header='false', schema=activity_spark_schema)\r\n",
							"        df = df.dropDuplicates(['SignalId'])\r\n",
							"        df = df.withColumn('year', F.year(F.col('StartTime'))).withColumn('month', F.month(F.col('StartTime')))\r\n",
							"\r\n",
							"        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['TechActivity'])\r\n",
							"        #output complete only available with aggregations\r\n",
							"        if len(df_pseudo.columns) == 0:\r\n",
							"            logger.info('No data to be written to stage2p')\r\n",
							"        else:\r\n",
							"            query = df_pseudo.writeStream.format(\"delta\") \\\r\n",
							"                                         .outputMode(\"append\") \\\r\n",
							"                                         .option(\"checkpointLocation\", self.stage2p + '/_checkpoints/TechActivity_pseudo/') \\\r\n",
							"                                         .option(\"startingOffsets\", \"earliest\") \\\r\n",
							"                                         .option(\"truncate\", False) \\\r\n",
							"                                         .partitionBy('year')  \r\n",
							"            query = query.start(self.stage2p + '/TechActivity_pseudo')\r\n",
							"            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
							"            logger.info(query.lastProgress)\r\n",
							"        \r\n",
							"        if len(df_lookup.columns) == 0:\r\n",
							"            logger.info('No data to be written to stage2np')\r\n",
							"        else:\r\n",
							"            query2 = df_lookup.writeStream.format(\"delta\") \\\r\n",
							"                              .outputMode(\"append\") \\\r\n",
							"                              .option(\"checkpointLocation\", self.stage2p + '/_checkpoints/TechActivity_lookup/') \\\r\n",
							"                              .option(\"startingOffsets\", \"earliest\") \\\r\n",
							"                              .option(\"truncate\", False) \\\r\n",
							"                              .partitionBy('year')\r\n",
							"            query2 = query2.start(self.stage2np + '/TechActivity_lookup')\r\n",
							"            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
							"            logger.info(query2.lastProgress)  \r\n",
							"\r\n",
							"    def full_load_insights_activity_stage1_data(self, date_folder_files=None):\r\n",
							"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights activity data from: \" + self.stage1np_activity)\r\n",
							"\r\n",
							"        # Currently not using the OEA ingest_incremental_data function due to pulling out the partition folders\r\n",
							"        activity_spark_schema = oea.to_spark_schema(self.schemas['TechActivity'])\r\n",
							"\r\n",
							"     \r\n",
							"        if date_folder_files is None: \r\n",
							"            date_folder_files = '/*/*.csv' \r\n",
							"        else:\r\n",
							"            date_folder_files= date_folder_files+'/*.csv'\r\n",
							"        \r\n",
							"        df = spark.read.csv(self.stage1np_activity + date_folder_files , header='false', schema=activity_spark_schema)\r\n",
							"        df = df.dropDuplicates(['SignalId'])\r\n",
							"        df = df.withColumn('year', F.year(F.col('StartTime'))).withColumn('month', F.month(F.col('StartTime')))\r\n",
							"\r\n",
							"        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['TechActivity'])\r\n",
							"        #SignalId as Unique identifier for the Activities\r\n",
							"        mergeCondition = \"oldData.SignalId = newData.SignalId\"\r\n",
							"        Target= self.stage2np + '/TechActivity'\r\n",
							"        var_check = DeltaTable.isDeltaTable(spark,  Target)\r\n",
							"        try:\r\n",
							"            if (var_check):\r\n",
							"                print(\"Performing Merge... on Existing table\")\r\n",
							"\r\n",
							"                olddt = DeltaTable.forPath(spark, Target) \r\n",
							"\r\n",
							"                olddt.alias(\"oldData\").merge(\r\n",
							"                    df.alias(\"newData\"),\r\n",
							"                    mergeCondition) \\\r\n",
							"                .whenMatchedUpdateAll() \\\r\n",
							"                .whenNotMatchedInsertAll() \\\r\n",
							"                .execute()\r\n",
							"            else:\r\n",
							"                print(\"Creating new Delta Table.\")    \r\n",
							"                df.write.format(\"Delta\").save(Target)\r\n",
							"        except:\r\n",
							"            print(\"Table does not exist. Creating new Delta Table.\")    \r\n",
							"            df.write.format(\"Delta\").save(Target)\r\n",
							"\r\n",
							"\r\n",
							"    def returnDataFrameBeforeProcessing(self, tableName='TechActivity', sourcepath=''):\r\n",
							"        \"\"\"\" Returns a Spark DF from the requested source, based in a foder/path. For uses with a file see biDimensions \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights activity data from: \" + sourcepath)\r\n",
							"        if sourcepath == '': \r\n",
							"            sourcepath=self.stage1np_activity\r\n",
							"        # Currently not using the OEA ingest_incremental_data function due to pulling out the partition folders\r\n",
							"        spark_schema = oea.to_spark_schema(self.schemas[f'{tableName}'])\r\n",
							"        df = spark.readStream.csv(sourcepath + '/*/*.csv', header='false', schema=spark_schema)\r\n",
							"\r\n",
							"        return df\r\n",
							"\r\n",
							"    def _process_roster_entity(self, path, entity):\r\n",
							"        try:\r\n",
							"            p_destination_path = self.stage2p + '/' + entity + '_pseudo'\r\n",
							"            np_destination_path = self.stage2np + '/' + entity + '_lookup'\r\n",
							"            source_path = path + '/' + entity\r\n",
							"            spark_schema = oea.to_spark_schema(self.schemas[entity])\r\n",
							"            df = spark.read.load(source_path, format='csv', header='false', schema=spark_schema)\r\n",
							"            df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas[entity])\r\n",
							"\r\n",
							"            if len(df_pseudo.columns) == 0:\r\n",
							"                logger.info('No data to be written to stage2p')\r\n",
							"            else:\r\n",
							"                df_pseudo.write.save(p_destination_path, format='delta', mode='overwrite') \r\n",
							"\r\n",
							"            if len(df_lookup.columns) == 0:\r\n",
							"                logger.info('No data to be written to stage2np')\r\n",
							"            else:\r\n",
							"                df_lookup.write.save(np_destination_path, format='delta', mode='overwrite') \r\n",
							"\r\n",
							"        except (AnalysisException) as error:\r\n",
							"            logger.exception(str(error))\r\n",
							"\r\n",
							"\r\n",
							"    def _process_roster_date_folder(self, date_folder_path):\r\n",
							"        folders = oea.get_folders(date_folder_path)\r\n",
							"        for table_name in folders:\r\n",
							"            self._process_roster_entity(date_folder_path, table_name)\r\n",
							"            logger.info(\"Processing ms_insights roster snapshot data from: \" + date_folder_path +'/'+table_name)\r\n",
							"\r\n",
							"\r\n",
							"    def process_roster(self):\r\n",
							"        \"\"\" Processes all roster data in stage1 and writes out to stage2 and stage2p \"\"\"\r\n",
							"  \r\n",
							"        latest_batch = oea.get_latest_folder(self.stage1np_roster)\r\n",
							"        source_path = self.stage1np_roster + '/' + latest_batch\r\n",
							"\r\n",
							"        self._process_roster_date_folder(source_path)\r\n",
							"  \r\n",
							"    def process_activity(self):\r\n",
							"        \"\"\" Processes all roster data in stage1 and writes out to stage2 and stage2p \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights activity snapshot data from: \" + self.stage1np_activity)\r\n",
							"\r\n",
							"        latest_batch = oea.get_latest_folder(self.stage1np_activity)\r\n",
							"        source_path = self.stage1np_activity \r\n",
							"\r\n",
							"        folders = oea.get_folders(source_path)\r\n",
							"        for folder in folders:\r\n",
							"            self.process_insights_activity_stage1_data(folder)\r\n",
							"      "
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NEW_Insights_module_setup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Insights Module"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2c0946f8-9d4f-4480-a75f-e94d23c8e3af"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 59
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Microsoft Insights Module Example Notebook\r\n",
							"This notebook creates 26 tables into two new Spark databases called s2np_insights and s2p_insights.\r\n",
							"\r\n",
							"s2p_insights is utilized for the Graph Reports API PowerBI dashboard provided"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Provision storage accounts\r\n",
							"\r\n",
							"The storage account variable has to be changed to the name of the storage account associated with your Azure resource group."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# data lake and container information\r\n",
							"# meant to be parameters\r\n",
							"storage_account = 'bceaaeoeadevlrs'\r\n",
							"use_test_env = False"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from faker import Faker\r\n",
							"import datetime"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Load Raw Data from Lake\r\n",
							"The top cell sets the date of the data to be processed (this is currently set up to process the test data). \r\n",
							"\r\n",
							"Date can be used for watermark, but Insights class process everything in stage1 folder, \r\n",
							"for effectively control date needs to be used for roster and activity"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Set the date to be processed\r\n",
							"today = datetime.datetime.now()\r\n",
							"date1 = today.strftime('%Y-%m-%d')\r\n",
							"#date1.cast('string')\r\n",
							"date2 = date1\r\n",
							"#date1 = \"2022-02-02\"\r\n",
							"#date2 = \"2022-07-12\"\r\n",
							"#date1 = date2"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 1. Activity table\r\n",
							"Contains all activities (students and teachers) at a school-system level\r\n",
							"\r\n",
							"** Databases and tables used: **\r\n",
							"\r\n",
							" - None \r\n",
							" \r\n",
							"**CSV files used:**\r\n",
							"\r\n",
							"- M365/SDS/activity/(whatever the set date is)/*.csv\r\n",
							"\r\n",
							"**Database and table created:**\r\n",
							"\r\n",
							"1. Spark DB: s2_sds\r\n",
							"- Table: techactivity\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /Insights_py"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# no required per date1 as \r\n",
							"#dfActivityRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/activity/'+ date1 + '/*.csv')\r\n",
							"## load needed roster tables from data lake storage\r\n",
							"#dfAADGroupRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/AadGroup/*.csv')\r\n",
							"#dfAADGroupMembershipRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/AadGroupMembership/*.csv')\r\n",
							"#dfAADUserRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/AadUser/*.csv')\r\n",
							"#dfAADUserPersonMappingRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/AadUserPersonMapping/*.csv')\r\n",
							"#dfCourseRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/Course/*.csv')\r\n",
							"#dfCourseGradeLevelRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/CourseGradeLevel/*.csv')\r\n",
							"#dfCourseSubjectRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/CourseSubject/*.csv')\r\n",
							"#dfEnrollmentRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/Enrollment/*.csv')\r\n",
							"#dfOrganizationRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/Organization/*.csv')\r\n",
							"#dfPersonRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/Person/*.csv')\r\n",
							"#dfPersonDemographicRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/PersonDemographic/*.csv')\r\n",
							"#dfPersonDemographicEthnicityRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/PersonDemographicEthnicity/*.csv')\r\n",
							"#dfPersonDemographicPersonFlagRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/PersonDemographicPersonFlag/*.csv')\r\n",
							"#dfPersonDemographicRaceRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/PersonDemographicRace/*.csv')\r\n",
							"#dfPersonEmailAddressRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/PersonEmailAddress/*.csv')\r\n",
							"#dfPersonIdentifierRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/PersonIdentifier/*.csv')\r\n",
							"#dfPersonOrganizationRoleRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/PersonOrganizationRole/*.csv')\r\n",
							"#dfPersonPhoneNumberRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/PersonPhoneNumber/*.csv')\r\n",
							"#dfPersonRelationshipRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/PersonRelationship/*.csv')\r\n",
							"#dfRefDefinitionRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/RefDefinition/*.csv')\r\n",
							"#dfSectionRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/Section/*.csv')\r\n",
							"#dfSectionSessionRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/SectionSession/*.csv')\r\n",
							"#dfSectionSubjectRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/SectionSubject/*.csv')\r\n",
							"#dfSessionRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/Session/*.csv')\r\n",
							"#dfSourceSystemRaw = spark.read.format('csv').load(f'{stage1np}/SDS/M365/roster/' + date2 + '*/SourceSystem/*.csv')"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if use_test_env:\r\n",
							"    stage1np = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1np'\r\n",
							"    stage2np = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2np'\r\n",
							"    stage2p = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2p'\r\n",
							"else:\r\n",
							"    stage1np = oea.stage1np\r\n",
							"    stage2np = oea.stage2np\r\n",
							"    stage2p = oea.stage2p\r\n",
							"    # 'abfss://stage2p@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 0) Initialize the OEA framework and MSInsights module.\r\n",
							"#oea = OEA(storage_account)\r\n",
							"source_folder='SDS/M365'\r\n",
							"ms_insights = Insights(source_folder)"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"db_name= 'bceoea'\r\n",
							"pathp = ms_insights.stage2p+'/'+source_folder\r\n",
							"pathnp = ms_insights.stage2np\r\n",
							"source_format= 'DELTA'"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.insert_watermark( source_folder, \"Roster\", date2)\r\n",
							"ms_insights.process_roster()"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"folders= oea.get_folders( ms_insights.stage1np_roster )\r\n",
							"\r\n",
							"print ()\r\n",
							"\r\n",
							"#mssparkutils.fs.ls( path )"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pathnp =ms_insights.stage1np_roster + '/' + folders[-2]\r\n",
							"pathnp\r\n",
							"ms_insights._process_roster_date_folder(pathnp)"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for folder in folders\r\n",
							"    pathnp =ms_insights.stage1np_roster + '/' + folder\r\n",
							"    ms_insights._process_roster_date_folder(pathnp)"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1) Process the raw activity data (csv format) from stage1 into stage2 data lake \r\n",
							"# (adds schema details #and writes out in parquet format).\r\n",
							"# NOTE: Does not write out to delta lake. \r\n",
							"# Current \"NEW_MSInsights_py\" script only supports writing test data to stage2p.\r\n",
							"#ms_insights.process_activity()\r\n",
							"\r\n",
							"#ms_insights.ingest()\r\n",
							"\r\n",
							"oea.insert_watermark( source_folder, \"Activity\", date2)\r\n",
							"ms_insights.process_insights_activity_stage1_data()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print ( oea.get_latest_folder (ms_insights.stage1np_roster) )"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1b) Process the raw roster data (csv format) from stage1 into stage2 data lake\r\n",
							"# (adds schema details and writes out in parquet format).\r\n",
							"# NOTE: Does not write out to delta lake. \r\n",
							"# Current \"NEW_MSInsights_py\" script only supports writing test data to stage2p.\r\n",
							"#ms_insights._process_roster_date_folder(date_folder_path=date2)\r\n",
							"#oea.create_lake_views( db_name, path, db_name)\r\n",
							"oea.create_sql_views(pathp, source_format)"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.create_sql_views(pathnp, source_format)"
						],
						"outputs": [],
						"execution_count": 30
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_connector')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "examples"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "532a9c72-d161-423a-8e50-02a3beda9b6f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# OEA connector\n",
							"This notebook provides a way for invoking methods on the OEA framework or supporting modules from a pipeline.\n",
							"\n",
							"When setting up a new module, be sure to include a new cell below that imports that module, so that its methods can be invoked by pipelines."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# These values should be passed in from the pipeline that is using this notebook as an activity.\r\n",
							"# Note that kwargs allows you to pass in a dict of params, but the dict has to specified as a string when invoked from a pipeline.\r\n",
							"# Also note that you can refer to attributes of an object in the params, for example: {'path':oea.stage2np}\r\n",
							"object_name = 'oea'\r\n",
							"method_name = ''\r\n",
							"kwargs = '{}'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /ContosoSIS_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"obj = eval(object_name)\r\n",
							"kwargs = eval(kwargs)\r\n",
							"m = getattr(obj, method_name)\r\n",
							"result = m(**kwargs)\r\n",
							"mssparkutils.notebook.exit(result)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_connector1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "examples"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "30f9c5e4-7e9a-49e1-829a-46fe2fa1e698"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# OEA connector\n",
							"This notebook provides a way for invoking methods on the OEA framework or supporting modules from a pipeline.\n",
							"\n",
							"When setting up a new module, be sure to include a new cell below that imports that module, so that its methods can be invoked by pipelines."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# These values should be passed in from the pipeline that is using this notebook as an activity.\r\n",
							"# Note that kwargs allows you to pass in a dict of params, but the dict has to specified as a string when invoked from a pipeline.\r\n",
							"# Also note that you can refer to attributes of an object in the params, for example: {'path':oea.stage2np}\r\n",
							"object_name = 'oea'\r\n",
							"method_name = ''\r\n",
							"kwargs = '{}'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /ContosoSIS_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"obj = eval(object_name)\r\n",
							"kwargs = eval(kwargs)\r\n",
							"m = getattr(obj, method_name)\r\n",
							"m(**kwargs)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2c4d7f56-dc8d-4d38-b861-b4ad6a86c565"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": true
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"from delta.tables import DeltaTable\n",
							"from notebookutils import mssparkutils\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType, ShortType, DateType\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\n",
							"import pandas as pd\n",
							"import sys\n",
							"import re\n",
							"import json\n",
							"import datetime\n",
							"import pytz\n",
							"import random\n",
							"import io\n",
							"\n",
							"logger = logging.getLogger('OEA')\n",
							"\n",
							"class OEA:\n",
							"    def __init__(self, storage_account='', instrumentation_key=None, salt='', logging_level=logging.DEBUG):\n",
							"        if storage_account:\n",
							"            self.storage_account = storage_account\n",
							"        else:\n",
							"            workspace_str = mssparkutils.env.getWorkspaceName() # extracts the OEA id for this OEA instance from the synapse workspace name (based on OEA naming convention)\n",
							"            oea_id = workspace_str.split(\"-\")[0]\n",
							"            self.storage_account = oea_id + 'aaeoeadevlrs' # sets the name of the storage account based on OEA naming convention\n",
							"            self.keyvault = oea_id + '-aae-oea-dev-kv'\n",
							"        self.keyvault_linked_service = 'LS_KeyVault_OEA'\n",
							"        self.serverless_sql_endpoint = mssparkutils.env.getWorkspaceName() + '-ondemand.sql.azuresynapse.net'\n",
							"        self._initialize_logger(instrumentation_key, logging_level)\n",
							"        self.salt = salt\n",
							"        self.timezone = 'Australia/EST'\n",
							"        self.stage1np = 'abfss://stage1np@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.stage2np = 'abfss://stage2np@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.stage2p = 'abfss://stage2p@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.stage3np = 'abfss://stage3np@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.stage3p = 'abfss://stage3p@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.framework_path = 'abfss://oea-framework@' + self.storage_account + '.dfs.core.windows.net'\n",
							"\n",
							"        # Initialize framework db\n",
							"        spark.sql(f\"CREATE DATABASE IF NOT EXISTS oea\")\n",
							"        spark.sql(f\"CREATE TABLE IF NOT EXISTS oea.env (name string not null, value string not null, description string) USING DELTA LOCATION '{self.framework_path}/db/env'\")\n",
							"        df = spark.sql(\"select value from oea.env where name='storage_account'\")\n",
							"        if df.first(): spark.sql(f\"UPDATE oea.env set value='{self.storage_account}' where name='storage_account'\")\n",
							"        else: spark.sql(f\"INSERT INTO oea.env VALUES ('storage_account', '{self.storage_account}', 'The name of the data lake storage account for this OEA instance.')\")\n",
							"        spark.sql(f\"CREATE TABLE IF NOT EXISTS OEA.watermark (source string not null, entity string not null, watermark timestamp not null) USING DELTA LOCATION '{self.framework_path}/db/watermark'\")\n",
							"\n",
							"        logger.debug(\"OEA initialized.\")\n",
							"    #some require full path \n",
							"    def path(self, container_name, directory_path=None):\n",
							"        if directory_path:\n",
							"            return f'abfss://{container_name}@{self.storage_account}.dfs.core.windows.net/{directory_path}'\n",
							"        else:\n",
							"            return f'abfss://{container_name}@{self.storage_account}.dfs.core.windows.net'\n",
							"\n",
							"    def convert_path(self, path):\n",
							"        \"\"\" Converts the given path into a valid url.\n",
							"            eg, convert_path('stage1np/contoso_sis/student/*') # returns abfss://stage1np@storageaccount.dfs.core.windows.net/contoso_sis/student/*\n",
							"        \"\"\"\n",
							"        path_args = path.split('/')\n",
							"        stage = path_args.pop(0)\n",
							"        return self.path(stage, '/'.join(path_args))            \n",
							"\n",
							"    def _initialize_logger(self, instrumentation_key, logging_level):\n",
							"        logging.lastResort = None\n",
							"        # the logger will print an error like \"ValueError: I/O operation on closed file\" because we're trying to have log messages also print to stdout\n",
							"        # and apparently this causes issues on some of the spark executor nodes. The bottom line is that we don't want these logging errors to get printed in the notebook output.\n",
							"        logging.raiseExceptions = False\n",
							"        logger.setLevel(logging_level)\n",
							"\n",
							"        handler = logging.StreamHandler(sys.stdout)\n",
							"        handler.setLevel(logging_level)\n",
							"        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
							"        handler.setFormatter(formatter)\n",
							"        logger.addHandler(handler)\n",
							"\n",
							"        if instrumentation_key:\n",
							"            # Setup logging to go to app insights (more info here: https://github.com/balakreshnan/Samples2021/blob/main/Synapseworkspace/opencensuslog.md#azure-synapse-spark-logs-runtime-errors-to-application-insights)\n",
							"            logger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=' + instrumentation_key))\n",
							"\n",
							"    def get_value_from_db(self, query):\n",
							"        df = spark.sql(query)\n",
							"        if df.first(): return df.first()[0]\n",
							"        else: return None\n",
							"\n",
							"    def get_last_watermark(self, source, entity):\n",
							"        return self.get_value_from_db(f\"select w.watermark from oea.watermark w where w.source='{source}' and w.entity='{entity}' order by w.watermark desc\")\n",
							"    \n",
							"    #watermark for incremental tables\n",
							"    def insert_watermark(self, source, entity, watermark_datetime):\n",
							"        spark.sql(f\"insert into oea.watermark values ('{source}', '{entity}', '{watermark_datetime}')\")\n",
							"\n",
							"    def get_secret(self, secret_name):\n",
							"        \"\"\" Retrieves the specified secret from the keyvault.\n",
							"            This method assumes that the keyvault linked service has been setup and is accessible.\n",
							"        \"\"\"\n",
							"        sc = SparkSession.builder.getOrCreate()\n",
							"        token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"        value = token_library.getSecret(self.keyvault, secret_name, self.keyvault_linked_service)        \n",
							"        return value\n",
							"\n",
							"    def delete(self, path):\n",
							"        oea.rm_if_exists(self.convert_path(path))\n",
							"\n",
							"    def land(self, data_source, entity, df, partition_label='', format_str='csv', header=True, mode='overwrite'):\n",
							"        \"\"\" Lands data in stage1np. If partition label is not provided, the current datetime is used with the label of 'batchdate'.\n",
							"            eg, land('contoso_isd', 'student', data, 'school_year=2021')\n",
							"        \"\"\"\n",
							"        tz = pytz.timezone(self.timezone)\n",
							"        datetime_str = datetime.datetime.now(tz).replace(microsecond=0).isoformat()\n",
							"        datetime_str = datetime_str.replace(':', '') # Path names can't have a colon - https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/introduction.md#path-names\n",
							"        df.write.format(format_str).save(self.path('stage1np', f'{data_source}/{entity}/{partition_label}/batchdate={datetime_str}'), header=header, mode=mode)\n",
							"\n",
							"    def load(self, folder, table, stage=None, data_format='delta'):\n",
							"        \"\"\" Loads a dataframe based on the path specified in the given args \"\"\"\n",
							"        if stage is None: stage = self.stage2p\n",
							"        path = f\"{stage}/{folder}/{table}\"\n",
							"        try:\n",
							"            df = spark.read.load(f\"{stage}/{folder}/{table}\", format=data_format)\n",
							"            return df        \n",
							"        except AnalysisException as e:\n",
							"            raise ValueError(\"Failed to load. Are you sure you have the right path?\\nMore info below:\\n\" + str(e)) \n",
							"\n",
							"    def load_csv(self, path, header=True):\n",
							"        \"\"\" Loads a dataframe based on the path specified \n",
							"            eg, df = load_csv('stage1np/example/student/*')\n",
							"        \"\"\"\n",
							"        url_path = self.convert_path(path)\n",
							"        try:\n",
							"            df = spark.read.load(url_path, format='csv', header=header)\n",
							"            return df        \n",
							"        except AnalysisException as e:\n",
							"            raise ValueError(f\"Failed to load from: {url_path}. Are you sure you have the right path?\\nMore info below:\\n\" + str(e))\n",
							"\n",
							"    def load_delta(self, path):\n",
							"        \"\"\" Loads a dataframe based on the path specified \n",
							"            eg, df = load_delta('stage2np/example/student/*')\n",
							"        \"\"\"\n",
							"        url_path = self.convert_path(path)\n",
							"        try:\n",
							"            df = spark.read.load(url_path, format='delta')\n",
							"            return df        \n",
							"        except AnalysisException as e:\n",
							"            raise ValueError(f\"Failed to load from: {url_path}. Are you sure you have the right path?\\nMore info below:\\n\" + str(e))\n",
							"\n",
							"    def load_from_stage1(self, path_and_filename, data_format='csv', header=True):\n",
							"        \"\"\" Loads a dataframe with data from stage1, based on the path specified in the given args \"\"\"\n",
							"        path = f\"{self.stage1np}/{path_and_filename}\"\n",
							"        df = spark.read.load(path, format=data_format, header=header)\n",
							"        return df        \n",
							"\n",
							"    def load_sample_from_csv_file(self, path_and_filename, header=True, stage=None):\n",
							"        \"\"\" Loads a sample from the specified csv file and returns a pandas dataframe.\n",
							"            Ex: print(load_sample_from_csv_file('/student_data/students.csv'))\n",
							"        \"\"\"\n",
							"        if stage is None: stage = self.stage1np\n",
							"        csv_str = mssparkutils.fs.head(f\"{stage}/{path_and_filename}\") # https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#preview-file-content\n",
							"        complete_lines = re.match(r\".*\\n\", csv_str, re.DOTALL).group(0)\n",
							"        if header: header = 0 # for info on why this is needed: https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.read_csv.html\n",
							"        else: header = None\n",
							"        pdf = pd.read_csv(io.StringIO(complete_lines), sep=',', header=header)\n",
							"        return pdf\n",
							"\n",
							"    def print_stage(self, path):\n",
							"        \"\"\" Prints out the highlevel contents of the specified stage.\"\"\"\n",
							"        msg = path + \"\\n\"\n",
							"        folders = self.get_folders(path)\n",
							"        for folder_name in folders:\n",
							"            entities = self.get_folders(path + '/' + folder_name)\n",
							"            msg += f\"{folder_name}: {entities}\\n\"\n",
							"        print(msg)           \n",
							"\n",
							"    #fix column names by replacing unwanted characters with _ to satisfy the Parquet naming requirements\n",
							"    def fix_column_names(self, df):\n",
							"        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\n",
							"        df_with_valid_column_names = df.select([F.col(col).alias(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", col)) for col in df.columns])\n",
							"        return df_with_valid_column_names\n",
							"\n",
							"    def to_spark_schema(self, schema):#: list[list[str]]):\n",
							"        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \n",
							"            Example:\n",
							"            schemas['Person'] = [['Id','string','hash'],\n",
							"                                    ['CreateDate','timestamp','no-op'],\n",
							"                                    ['LastModifiedDate','timestamp','no-op']]\n",
							"            to_spark_schema(schemas['Person'])\n",
							"        \"\"\"\n",
							"        fields = []\n",
							"        for col_name, dtype, op in schema:\n",
							"            fields.append(StructField(col_name, globals()[dtype.lower().capitalize() + \"Type\"](), True))\n",
							"        spark_schema = StructType(fields)\n",
							"        return spark_schema\n",
							"\n",
							"    #drops duplicates and append new ones\n",
							"    def ingest_incremental_data(self, source_system, tablename, schema, partition_by, primary_key='id', data_format='csv', has_header=True):\n",
							"        \"\"\" Processes incremental batch data from stage1 into stage2 \"\"\"\n",
							"        source_path = f'{self.stage1np}/{source_system}/{tablename}'\n",
							"        p_destination_path = f'{self.stage2p}/{source_system}/{tablename}_pseudo'\n",
							"        np_destination_path = f'{self.stage2np}/{source_system}/{tablename}_lookup'\n",
							"        logger.info(f'Processing incremental data from: {source_path} and writing out to: {p_destination_path}')\n",
							"\n",
							"        if has_header: header_flag = 'true'\n",
							"        else: header_flag = 'false'\n",
							"        spark_schema = self.to_spark_schema(schema)\n",
							"        df = spark.readStream.load(source_path + '/*', format=data_format, header=header_flag, schema=spark_schema)\n",
							"        #df = spark.read.load(source_path + '/*', format=data_format, header=header_flag, schema=spark_schema)\n",
							"        #display(df)\n",
							"        #df = df.withColumn('batchdate', F.to_timestamp(df.batchdate, \"yyyy-MM-dd'T'HHmmssZ\"))\n",
							"        df = df.dropDuplicates([primary_key]) # drop duplicates across batches. More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\n",
							"        \n",
							"        df_pseudo, df_lookup = self.pseudonymize(df, schema)\n",
							"\n",
							"        if len(df_pseudo.columns) == 0:\n",
							"            logger.info('No data to be written to stage2p')\n",
							"        else:        \n",
							"            query = df_pseudo.writeStream.format(\"delta\") \\\n",
							"                        .outputMode(\"append\") \\\n",
							"                        .trigger(once=True) \\\n",
							"                        .option(\"checkpointLocation\", source_path + '/_checkpoints/incremental_p') \\\n",
							"                        .partitionBy(partition_by)\n",
							"                        \n",
							"            query = query.start(p_destination_path)\n",
							"            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
							"            logger.info(query.lastProgress)\n",
							"\n",
							"        if len(df_lookup.columns) == 0:\n",
							"            logger.info('No data to be written to stage2np')\n",
							"        else:\n",
							"            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints/incremental_np').partitionBy(partition_by)\n",
							"            query2 = query2.start(np_destination_path)\n",
							"            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
							"            logger.info(query2.lastProgress) \n",
							"\n",
							"    #updates or append\n",
							"    def _merge_into_table(self, df, destination_path, checkpoints_path, condition):\n",
							"        \"\"\" Merges data from the given dataframe into the delta table at the specified destination_path, based on the given condition.\n",
							"            If not delta table exists at the specified destination_path, a new delta table is created and the data from the given dataframe is inserted.\n",
							"            eg, merge_into_table(df_lookup, np_destination_path, source_path + '/_checkpoints/delta_np', \"current.id_pseudonym = updates.id_pseudonym\")\n",
							"        \"\"\"\n",
							"        if DeltaTable.isDeltaTable(spark, destination_path):      \n",
							"            dt = DeltaTable.forPath(spark, destination_path)\n",
							"            def upsert(batch_df, batchId):\n",
							"                dt.alias(\"current\").merge(batch_df.alias(\"updates\"), condition).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()                \n",
							"            query = df.writeStream.format(\"delta\").foreachBatch(upsert).outputMode(\"update\").trigger(once=True).option(\"checkpointLocation\", checkpoints_path)\n",
							"        else:\n",
							"            logger.info(f'Delta table does not yet exist at {destination_path} - creating one now and inserting initial data.')\n",
							"            query = df.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", checkpoints_path)\n",
							"        query = query.start(destination_path)\n",
							"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
							"        logger.info(query.lastProgress)    \n",
							"    \n",
							"    #calls merge into table \n",
							"    def ingest_delta_data(self, source_system, tablename, schema, partition_by, primary_key='id', data_format='csv', has_header=True):\n",
							"        \"\"\" Processes delta batch data from stage1 into stage2 \"\"\"\n",
							"        source_path = f'{self.stage1np}/{source_system}/{tablename}'\n",
							"        p_destination_path = f'{self.stage2p}/{source_system}/{tablename}_pseudo'\n",
							"        np_destination_path = f'{self.stage2np}/{source_system}/{tablename}_lookup'\n",
							"        logger.info(f'Processing delta data from: {source_path} and writing out to: {p_destination_path}')\n",
							"\n",
							"        if has_header: header_flag = 'true'\n",
							"        else: header_flag = 'false'\n",
							"        spark_schema = self.to_spark_schema(schema)\n",
							"        df = spark.readStream.load(source_path + '/*', format=data_format, header=header_flag, schema=spark_schema)\n",
							"        \n",
							"        df_pseudo, df_lookup = self.pseudonymize(df, schema)\n",
							"\n",
							"        if len(df_pseudo.columns) == 0:\n",
							"            logger.info('No data to be written to stage2p')\n",
							"        else:\n",
							"            self._merge_into_table(df_pseudo, p_destination_path, source_path + '/_checkpoints/delta_p', \"current.id_pseudonym = updates.id_pseudonym\")\n",
							"\n",
							"        if len(df_lookup.columns) == 0:\n",
							"            logger.info('No data to be written to stage2np')\n",
							"        else:\n",
							"            self._merge_into_table(df_lookup, np_destination_path, source_path + '/_checkpoints/delta_np', \"current.id_pseudonym = updates.id_pseudonym\")\n",
							"\n",
							"    #drops duplicated and writes with overwrite option  \n",
							"    def ingest_snapshot_data(self, source_system, tablename, schema, partition_by, primary_key='id', data_format='csv', has_header=True, has_snap=True):\n",
							"        \"\"\" Processes snapshot batch data from stage1 into stage2 \"\"\"\n",
							"        source_path = f'{self.stage1np}/{source_system}/{tablename}'\n",
							"        if has_snap:\n",
							"            latest_batch = self.get_latest_folder(source_path)\n",
							"            source_path = source_path + '/' + latest_batch\n",
							"        \n",
							"        p_destination_path = f'{self.stage2p}/{source_system}/{tablename}_pseudo'\n",
							"        np_destination_path = f'{self.stage2np}/{source_system}/{tablename}_lookup'\n",
							"        logger.info(f'Processing snapshot data from: {source_path} and writing out to: {p_destination_path}')\n",
							"\n",
							"        if has_header: header_flag = 'true'\n",
							"        else: header_flag = 'false'\n",
							"        spark_schema = self.to_spark_schema(schema)\n",
							"        df = spark.read.load(source_path, format=data_format, header=header_flag, schema=spark_schema)\n",
							"        df = df.dropDuplicates([primary_key]) # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\n",
							"        \n",
							"        df_pseudo, df_lookup = self.pseudonymize(df, schema)\n",
							"\n",
							"        if len(df_pseudo.columns) == 0:\n",
							"            logger.info('No data to be written to stage2p')\n",
							"        else:\n",
							"            df_pseudo.write.save(p_destination_path, format='delta', mode='overwrite', partitionBy=partition_by) \n",
							"\n",
							"        if len(df_lookup.columns) == 0:\n",
							"            logger.info('No data to be written to stage2np')\n",
							"        else:\n",
							"            df_lookup.write.save(np_destination_path, format='delta', mode='overwrite', partitionBy=partition_by) \n",
							"\n",
							"\n",
							"\n",
							"    def ingest_snapshot_data_frame(self, inpdataframe, source_path, schema,  primary_key='id'):\n",
							"        \"\"\" Processes snapshot batch data from stage1 into stage2 \"\"\"\n",
							"\n",
							"        p_destination_path = f'{self.stage2p}/{source_path}_pseudo'\n",
							"        np_destination_path = f'{self.stage2np}/{source_path}_lookup'\n",
							"        logger.info(f'Processing snapshot data from: {source_path} and writing out to: {p_destination_path}')\n",
							"\n",
							"        df = inpdataframe.dropDuplicates([primary_key]) # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\n",
							"        \n",
							"        df_pseudo, df_lookup = self.pseudonymize(df, schema)\n",
							"\n",
							"        if len(df_pseudo.columns) == 0:\n",
							"            logger.info('No data to be written to stage2p')\n",
							"        else:\n",
							"            df_pseudo.write.save(p_destination_path, format='delta', mode='overwrite', mergeSchema= True) \n",
							"\n",
							"        if len(df_lookup.columns) == 0:\n",
							"            logger.info('No data to be written to stage2np')\n",
							"        else:\n",
							"            df_lookup.write.save(np_destination_path, format='delta', mode='overwrite', mergeSchema= True) \n",
							"\n",
							"#important here the treatment is specified following the label giving in the schema definition for the columns \n",
							"    def pseudonymize(self, df, schema): #: list[list[str]]):\n",
							"        \"\"\" Performs pseudonymization of the given dataframe based on the provided schema.\n",
							"            For example, if the given df is for an entity called person, \n",
							"            2 dataframes will be returned, one called person that has hashed ids and masked fields, \n",
							"            and one called person_lookup that contains the original person_id, person_id_pseudo,\n",
							"            and the non-masked values for columns marked to be masked.\"\"\"\n",
							"        \n",
							"        df_pseudo = df_lookup = df\n",
							"\n",
							"        for col_name, dtype, op in schema:\n",
							"            if op == \"hash-no-lookup\" or op == \"hnl\":\n",
							"                # This means that the lookup can be performed against a different table so no lookup is needed.\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
							"                df_lookup = df_lookup.drop(col_name)           \n",
							"            elif op == \"hash\" or op == 'h':\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
							"                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256))\n",
							"            elif op == \"mask\" or op == 'm':\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\n",
							"            elif op == \"partition-by\":\n",
							"                pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\n",
							"            elif op == \"no-op\" or op == 'x':\n",
							"                df_lookup = df_lookup.drop(col_name)\n",
							"\n",
							"        df_pseudo = self.fix_column_names(df_pseudo)\n",
							"        df_lookup = self.fix_column_names(df_lookup)\n",
							"\n",
							"        return (df_pseudo, df_lookup)\n",
							"\n",
							"    # Returns true if the path exists\n",
							"    def path_exists(self, path):\n",
							"        tableExists = False\n",
							"        try:\n",
							"            items = mssparkutils.fs.ls(path)\n",
							"            tableExists = True\n",
							"        except Exception as e:\n",
							"            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\n",
							"            pass\n",
							"        return tableExists\n",
							"\n",
							"    def ls(self, path):\n",
							"        if not path.startswith(\"abfss:\"):\n",
							"            path = self.convert_path(path)\n",
							"        folders = []\n",
							"        files = []\n",
							"        try:\n",
							"            items = mssparkutils.fs.ls(path)\n",
							"            for item in items:\n",
							"                if item.isFile:\n",
							"                    files.append(item.name)\n",
							"                elif item.isDir:\n",
							"                    folders.append(item.name)\n",
							"        except Exception as e:\n",
							"            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
							"        return (folders, files)\n",
							"\n",
							"    def print_stage(self, path):\n",
							"        print(path)\n",
							"        folders = self.get_folders(path)\n",
							"        for folder_name in folders:\n",
							"            entities = self.get_folders(path + '/' + folder_name)\n",
							"            print(f\"{folder_name}: {entities}\")\n",
							"\n",
							"    # Return the list of folders found in the given path.\n",
							"    def get_folders(self, path):\n",
							"        dirs = []\n",
							"        try:\n",
							"            items = mssparkutils.fs.ls(path)\n",
							"            for item in items:\n",
							"                #print(item.name, item.isDir, item.isFile, item.path, item.size)\n",
							"                if item.isDir:\n",
							"                    dirs.append(item.name)\n",
							"        except Exception as e:\n",
							"            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
							"        return dirs\n",
							"\n",
							"    def get_latest_folder(self, path):\n",
							"        folders = self.get_folders(path)\n",
							"        if len(folders) > 0: return folders[-1]\n",
							"        else: return None\n",
							"\n",
							"    # Remove a folder if it exists (defaults to use of recursive removal).\n",
							"    def rm_if_exists(self, path, recursive_remove=True):\n",
							"        try:\n",
							"            mssparkutils.fs.rm(path, recursive_remove)\n",
							"            print(\"deleting files in\")\n",
							"        except Exception as e:\n",
							"            pass\n",
							"\n",
							"    def pop_from_path(self, path):\n",
							"        \"\"\" Pops the last arg in a path and returns the path and the last arg as a tuple.\n",
							"            pop_from_path('abfss://stage2@xyz.dfs.core.windows.net/ms_insights/test.csv') # returns ('abfss://stage2@xyz.dfs.core.windows.net/ms_insights', 'test.csv')\n",
							"        \"\"\"\n",
							"        m = re.match(r\"(.*)\\/([^/]+)\", path)\n",
							"        return (m.group(1), m.group(2))\n",
							"\n",
							"    def parse_source_path(self, path):\n",
							"        \"\"\" Parses a path that looks like this: abfss://stage2p@stoeacisd3ggimpl3.dfs.core.windows.net/ms_insights\n",
							"            and returns a dictionary like this: {'stage_num': '2', 'ss': 'ms_insights'}\n",
							"            Note that it will also return a 'stage_num' of 2 if the path is stage2p or stage2np - this is by design because the spark db with the s2 prefix will be used for data in stage2 and stage2p.\n",
							"        \"\"\"\n",
							"        m = re.match(r\".*:\\/\\/stage(?P<stage_num>\\d+)[n]?[p]?@[^/]+\\/(?P<ss>[^/]+)\", path)\n",
							"        return m.groupdict()\n",
							"    \n",
							"    def create_lake_db(self, stage_num, source_dir, source_format='DELTA'):\n",
							"        \"\"\" Creates a spark db that points to data in the given stage under the specified source directory (assumes that every folder in the source_dir is a table).\n",
							"            Example: create_lake_db(2, 'contoso_sis')\n",
							"            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
							"        \"\"\"\n",
							"        db_name = f's{stage_num}_{source_dir}'\n",
							"        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
							"        self.create_lake_views(db_name, self.path(f'stage{stage_num}p', source_dir), source_format)\n",
							"        self.create_lake_views(db_name, self.path(f'stage{stage_num}np', source_dir), source_format)\n",
							"        result = \"Database created: \" + db_name\n",
							"        logger.info(result)\n",
							"        return result        \n",
							"\n",
							"    def create_lake_views(self, db_name, source_path, source_format):\n",
							"        dirs = self.get_folders(source_path)\n",
							"        for table_name in dirs:\n",
							"            spark.sql(f\"create table if not exists {db_name}.{table_name} using {source_format} location '{source_path}/{table_name}'\")\n",
							"\n",
							"    def drop_lake_db(self, db_name):\n",
							"        spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
							"        result = \"Database dropped: \" + db_name\n",
							"        logger.info(result)\n",
							"        return result       \n",
							"\n",
							"    def create_sql_db(self, stage_num, source_dir, source_format='DELTA'):\n",
							"        \"\"\" Prints out the sql script needed for creating a sql serverless db and set of views. \"\"\"\n",
							"        db_name = f'sqls{stage_num}_{source_dir}'\n",
							"        cmd += '-- Create a new sql script then execute the following in it:'\n",
							"        cmd += f\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\\nBEGIN\\n  CREATE DATABASE {db_name};\\nEND;\\nGO\\n\"\n",
							"        cmd += f\"USE {db_name};\\nGO\\n\\n\"\n",
							"        cmd += self.create_sql_views(self.path(f'stage{stage_num}p', source_dir), source_format)\n",
							"        cmd += self.create_sql_views(self.path(f'stage{stage_num}np', source_dir), source_format)\n",
							"        print(cmd)\n",
							"    \n",
							"    #RETURNS STRING WITH SQL SETENCES COMBININT A DIRECTORY \n",
							"    def create_sql_views(self, source_path, source_format):\n",
							"        cmd = ''      \n",
							"        dirs = self.get_folders(source_path)\n",
							"        for table_name in dirs:\n",
							"            view_name = table_name.replace('.','_')\n",
							"            cmd += f\"CREATE OR ALTER VIEW {view_name} AS\\n  SELECT * FROM OPENROWSET(BULK '{source_path}/{table_name}', FORMAT='{source_format}') AS [r];\\nGO\\n\"\n",
							"        return cmd\n",
							"\n",
							"    def drop_sql_db(self, db_name):\n",
							"        print('Click on the menu next to the SQL db and select \"Delete\"')\n",
							"\n",
							"    # List installed packages\n",
							"    def list_packages(self):\n",
							"        import pkg_resources\n",
							"        for d in pkg_resources.working_set:\n",
							"            print(d)\n",
							"\n",
							"    def print_schema_starter(self, entity_name, df):\n",
							"        \"\"\" Prints a starter schema that can be modified as needed when developing the oea schema for a new module. \"\"\"\n",
							"        st = f\"self.schemas['{entity_name}'] = [\"\n",
							"        for col in df.schema:\n",
							"            st += f\"['{col.name}', '{str(col.dataType)[:-4].lower()}', 'no-op'],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\"\n",
							"        return st[:-11] + ']'\n",
							"\n",
							"    def write_rows_as_csv(data, folder, filename, container=None):\n",
							"        \"\"\" Writes a dictionary as a csv to the specified location. This is helpful when creating test data sets and landing them in stage1np.\n",
							"            data = [{'id':'1','fname':'John'}, {'id':'1','fname':'Jane'}]\n",
							"        \"\"\"\n",
							"        if container == None: container = self.stage1np\n",
							"        pdf = pd.DataFrame(data)\n",
							"        mssparkutils.fs.put(f\"{container}/{folder}/{filename}\", pdf.to_csv(index=False), True) # True indicates overwrite mode  \n",
							"\n",
							"    def write_rowset_as_csv(data, folder, container=None):\n",
							"        \"\"\" Writes out as csv rows the passed in data. The inbound data should be in a format like this:\n",
							"            data = { 'students':[{'id':'1','fname':'John'}], 'courses':[{'id':'31', 'name':'Math'}] }\n",
							"        \"\"\"\n",
							"        if container == None: container = self.stage1np\n",
							"        for entity_name, value in data.items():\n",
							"            pdf = pd.DataFrame(value)\n",
							"            mssparkutils.fs.put(f\"{container}/{folder}/{entity_name}.csv\", pdf.to_csv(index=False), True) # True indicates overwrite mode         \n",
							"\n",
							"    def create_empty_dataframe(self, schema):\n",
							"        \"\"\" Creates an empty dataframe based on the given schema which is specified as an array of column names and sql types.\n",
							"            eg, schema = [['data_source','string'], ['entity','string'], ['watermark','timestamp']]\n",
							"        \"\"\"\n",
							"        fields = []\n",
							"        for col_name, col_type in schema:\n",
							"            fields.append(StructField(col_name, globals()[col_type.lower().capitalize() + \"Type\"](), True))\n",
							"        spark_schema = StructType(fields)\n",
							"        df = spark.createDataFrame(spark.sparkContext.emptyRDD(), spark_schema)\n",
							"        return df\n",
							"\n",
							"    def delete_data_source(self, data_source):\n",
							"        self.rm_if_exists(self.convert_path(f'stage1np/{data_source}'))\n",
							"        self.rm_if_exists(self.convert_path(f'stage2np/{data_source}'))\n",
							"        self.rm_if_exists(self.convert_path(f'stage2p/{data_source}'))\n",
							"\n",
							"class BaseOEAModule:\n",
							"    \"\"\" Provides data processing methods for Contoso SIS data (the student information system for the fictional Contoso school district).  \"\"\"\n",
							"    def __init__(self, source_folder, pseudonymize = True):\n",
							"        self.source_folder = source_folder\n",
							"        #to be defined externarly on the Insights subclass\n",
							"        self.pseudonymize = pseudonymize\n",
							"        self.stage1np = f\"{oea.stage1np}/{source_folder}\"\n",
							"        self.stage2np = f\"{oea.stage2np}/{source_folder}\"\n",
							"        self.stage2p = f\"{oea.stage2p}/{source_folder}\"\n",
							"        self.stage3np = f\"{oea.stage3np}/{source_folder}\"\n",
							"        self.stage3p = f\"{oea.stage3p}/{source_folder}\"\n",
							"        self.module_path = f\"{oea.framework_path}/modules/{source_folder}\"\n",
							"        self.schemas = {}\n",
							"\n",
							"    def _process_entity_from_stage1(self, path, entity_name, format='csv', write_mode='overwrite', header='true'):\n",
							"        spark_schema = oea.to_spark_schema(self.schemas[entity_name])\n",
							"        df = spark.read.format(format).load(f\"{self.stage1np}/{path}/{entity_name}\", header=header, schema=spark_schema)\n",
							"\n",
							"        if self.pseudonymize:\n",
							"            df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas[entity_name])\n",
							"            df_pseudo.write.format('delta').mode(write_mode).save(f\"{self.stage2p}/{entity_name}\")\n",
							"            if len(df_lookup.columns) > 0:\n",
							"                df_lookup.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}_lookup\")\n",
							"        else:\n",
							"            df = oea.fix_column_names(df)   \n",
							"            df.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}\")\n",
							"\n",
							"    def delete_stage1(self):\n",
							"        oea.rm_if_exists(self.stage1np)\n",
							"\n",
							"    def delete_stage2(self):\n",
							"        oea.rm_if_exists(self.stage2np)\n",
							"        oea.rm_if_exists(self.stage2p)\n",
							"\n",
							"    def delete_stage3(self):\n",
							"        oea.rm_if_exists(self.stage3np)\n",
							"        oea.rm_if_exists(self.stage3p)                \n",
							"\n",
							"    def delete_all_stages(self):\n",
							"        self.delete_stage1()\n",
							"        self.delete_stage2()\n",
							"        self.delete_stage3()\n",
							"\n",
							"    def create_stage2_lake_db(self, format='DELTA'):\n",
							"        oea.create_lake_db(self.stage2p, format)\n",
							"        oea.create_lake_db(self.stage2np, format)\n",
							"\n",
							"    def create_stage3_lake_db(self, format='DELTA'):\n",
							"        oea.create_lake_db(self.stage3p, format)\n",
							"        oea.create_lake_db(self.stage3np, format)\n",
							"\n",
							"    def copy_test_data_to_stage1(self):\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data', self.stage1np, True)   \n",
							"\n",
							"class DataLakeWriter:\n",
							"    def __init__(self, root_destination):\n",
							"        self.root_destination = root_destination\n",
							"\n",
							"    def write(self, path_and_filename, data_str, format='csv'):\n",
							"        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist\n",
							"\n",
							"oea = OEA()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SIFLoadCodeSets')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SIF"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d63e9ded-b1d2-4d42-bcb1-b12fcaa81a29"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"StorageAccountName = \"adsdevdlsadsalfsadsl\"\r\n",
							"StorageAccountContainer = \"datalakeraw\"\r\n",
							"StorageAccountFolder = \"/samples/sif/\"\r\n",
							"SifDbName = \"sif\"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Read in Full Json Schema from SIF repo**"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"rdd =  spark.sparkContext.wholeTextFiles(\"abfss://\"+StorageAccountContainer+\"@\"+StorageAccountName+\".dfs.core.windows.net\"+StorageAccountFolder+\"SifOpenApi/jsonSchemaCreate_AU.json\")\r\n",
							"text = rdd.collect()[0][1]\r\n",
							"dict1 = json.loads(str(text))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Declare Function that will register codesets as managed tables**"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types  import *\r\n",
							"from pyspark.sql import Row\r\n",
							"\r\n",
							"def PersistRefData(name, dict1):\r\n",
							"    ref = dict1['definitions'][name]['oneOf']\r\n",
							"    schema = StructType([StructField('const', StringType()), StructField('title',StringType())])\r\n",
							"    refdf = spark.createDataFrame(data=ref, schema = schema)\r\n",
							"    tablename = \"ref_\" + name\r\n",
							"    spark.catalog.setCurrentDatabase(SifDbName)\r\n",
							"    refdf.write.mode(\"overwrite\").saveAsTable(tablename)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Call Function for Each Codeset Entity**"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"entities = [\"AbstractContentElement\" \\\n",
							",\"AUCodeSets0211ProgramAvailability\" \\\n",
							",\"AUCodeSets0792IdentificationProcedure\" \\\n",
							",\"AUCodeSetsAccompaniment\" \\\n",
							",\"AUCodeSetsACStrand\" \\\n",
							",\"AUCodeSetsActivityInvolvementCode\" \\\n",
							",\"AUCodeSetsActivityType\" \\\n",
							",\"AUCodeSetsAddressRole\" \\\n",
							",\"AUCodeSetsAddressType\" \\\n",
							",\"AUCodeSetsAGCollection\" \\\n",
							",\"AUCodeSetsAGContextQuestion\" \\\n",
							",\"AUCodeSetsAGSubmissionStatus\" \\\n",
							",\"AUCodeSetsAssessmentReportingMethod\" \\\n",
							",\"AUCodeSetsAssessmentType\" \\\n",
							",\"AUCodeSetsAttendanceCode\" \\\n",
							",\"AUCodeSetsAttendanceStatus\" \\\n",
							",\"AUCodeSetsAustralianCitizenshipStatus\" \\\n",
							",\"AUCodeSetsAustralianStandardClassificationOfCulturalAndEthnicGroupsASCCEG\" \\\n",
							",\"AUCodeSetsAustralianStandardClassificationOfLanguagesASCL\" \\\n",
							",\"AUCodeSetsAustralianStandardClassificationOfReligiousGroupsASCRG\" \\\n",
							",\"AUCodeSetsAustralianStandardGeographicalClassificationASGC\" \\\n",
							",\"AUCodeSetsAustralianTimeZone\" \\\n",
							",\"AUCodeSetsBirthdateVerification\" \\\n",
							",\"AUCodeSetsBoarding\" \\\n",
							",\"AUCodeSetsCalendarEvent\" \\\n",
							",\"AUCodeSetsContactMethod\" \\\n",
							",\"AUCodeSetsDayValueCode\" \\\n",
							",\"AUCodeSetsDetentionCategory\" \\\n",
							",\"AUCodeSetsDwellingArrangement\" \\\n",
							",\"AUCodeSetsEducationAgencyType\" \\\n",
							",\"AUCodeSetsEducationLevel\" \\\n",
							",\"AUCodeSetsElectronicIdType\" \\\n",
							",\"AUCodeSetsEmailType\" \\\n",
							",\"AUCodeSetsEmploymentType\" \\\n",
							",\"AUCodeSetsEnglishProficiency\" \\\n",
							",\"AUCodeSetsEnrollmentTimeFrame\" \\\n",
							",\"AUCodeSetsEntryType\" \\\n",
							",\"AUCodeSetsEquipmentType\" \\\n",
							",\"AUCodeSetsEventCategory\" \\\n",
							",\"AUCodeSetsEventSubCategory\" \\\n",
							",\"AUCodeSetsExitWithdrawalStatus\" \\\n",
							",\"AUCodeSetsExitWithdrawalType\" \\\n",
							",\"AUCodeSetsFederalElectorate\" \\\n",
							",\"AUCodeSetsFFPOSStatusCode\" \\\n",
							",\"AUCodeSetsFTPTStatusCode\" \\\n",
							",\"AUCodeSetsGroupCategoryCode\" \\\n",
							",\"AUCodeSetsImmunisationCertificateStatus\" \\\n",
							",\"AUCodeSetsIndigenousStatus\" \\\n",
							",\"AUCodeSetsLanguageType\" \\\n",
							",\"AUCodeSetsLearningStandardItemRelationshipTypes\" \\\n",
							",\"AUCodeSetsMaritalStatusAIHW\" \\\n",
							",\"AUCodeSetsMediumOfInstruction\" \\\n",
							",\"AUCodeSetsNameUsageType\" \\\n",
							",\"AUCodeSetsNAPJurisdiction\" \\\n",
							",\"AUCodeSetsNAPParticipationCode\" \\\n",
							",\"AUCodeSetsNAPResponseCorrectness\" \\\n",
							",\"AUCodeSetsNAPTestDomain\" \\\n",
							",\"AUCodeSetsNAPTestItemMarkingType\" \\\n",
							",\"AUCodeSetsNAPTestItemType\" \\\n",
							",\"AUCodeSetsNAPTestType\" \\\n",
							",\"AUCodeSetsNAPWritingGenre\" \\\n",
							",\"AUCodeSetsNonSchoolEducation\" \\\n",
							",\"AUCodeSetsOperationalStatus\" \\\n",
							",\"AUCodeSetsPermanentResidentStatus\" \\\n",
							",\"AUCodeSetsPermissionCategoryCode\" \\\n",
							",\"AUCodeSetsPersonalisedPlan\" \\\n",
							",\"AUCodeSetsPictureSource\" \\\n",
							",\"AUCodeSetsPNPCode\" \\\n",
							",\"AUCodeSetsPrePrimaryHours\" \\\n",
							",\"AUCodeSetsProgramFundingSourceCode\" \\\n",
							",\"AUCodeSetsProgressLevel\" \\\n",
							",\"AUCodeSetsPublicSchoolCatchmentStatus\" \\\n",
							",\"AUCodeSetsReceivingLocationOfInstruction\" \\\n",
							",\"AUCodeSetsRelationshipToStudent\" \\\n",
							",\"AUCodeSetsResourceUsageContentType\" \\\n",
							",\"AUCodeSetsScheduledActivityType\" \\\n",
							",\"AUCodeSetsSchoolCoEdStatus\" \\\n",
							",\"AUCodeSetsSchoolEducationLevelType\" \\\n",
							",\"AUCodeSetsSchoolEnrollmentType\" \\\n",
							",\"AUCodeSetsSchoolFocusCode\" \\\n",
							",\"AUCodeSetsSchoolLevel\" \\\n",
							",\"AUCodeSetsSchoolLocation\" \\\n",
							",\"AUCodeSetsSchoolSectorCode\" \\\n",
							",\"AUCodeSetsSchoolSystem\" \\\n",
							",\"AUCodeSetsSessionType\" \\\n",
							",\"AUCodeSetsSexCode\" \\\n",
							",\"AUCodeSetsSourceCodeType\" \\\n",
							",\"AUCodeSetsStaffActivity\" \\\n",
							",\"AUCodeSetsStaffStatus\" \\\n",
							",\"AUCodeSetsStandardAustralianClassificationOfCountriesSACC\" \\\n",
							",\"AUCodeSetsStateTerritoryCode\" \\\n",
							",\"AUCodeSetsStudentFamilyProgramType\" \\\n",
							",\"AUCodeSetsSuspensionCategory\"  \\\n",
							",\"AUCodeSetsSystemicStatus\" \\\n",
							",\"AUCodeSetsTeacherCoverCredit\" \\\n",
							",\"AUCodeSetsTeacherCoverSupervision\" \\\n",
							",\"AUCodeSetsTelephoneNumberType\" \\\n",
							",\"AUCodeSetsTravelMode\" \\\n",
							",\"AUCodeSetsVisaStudyEntitlement\" \\\n",
							",\"AUCodeSetsVisaSubClass\" \\\n",
							",\"AUCodeSetsWellbeingAlertCategory\" \\\n",
							",\"AUCodeSetsWellbeingAppealStatus\" \\\n",
							",\"AUCodeSetsWellbeingCharacteristicCategory\" \\\n",
							",\"AUCodeSetsWellbeingCharacteristicClassification\" \\\n",
							",\"AUCodeSetsWellbeingCharacteristicSubCategory\" \\\n",
							",\"AUCodeSetsWellbeingEventCategoryClass\" \\\n",
							",\"AUCodeSetsWellbeingEventLocation\" \\\n",
							",\"AUCodeSetsWellbeingEventTimePeriod\" \\\n",
							",\"AUCodeSetsWellbeingResponseCategory\" \\\n",
							",\"AUCodeSetsWellbeingStatus\" \\\n",
							",\"AUCodeSetsYearLevelCode\" \\\n",
							",\"AUCodeSetsYesOrNoCategory\" \\\n",
							",\"DefinedProtocols\" \\\n",
							",\"GenericYesNo\" \\\n",
							",\"ISO4217CurrencyNamesAndCodeElements\"]\n",
							"\n",
							"\n",
							"from multiprocessing.pool import ThreadPool\n",
							"pool = ThreadPool(5)\n",
							"pool.map(lambda e: PersistRefData(e,dict1),entities)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SIFLoadDimStaffPersonal')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SIF"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f1c1bbb8-c1ac-4d4f-b4cc-460f4270b9ca"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SifDbName = \"sif\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.catalog.setCurrentDatabase(SifDbName)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\n",
							"df_Raw = spark.sql(\"select * from raw_staffpersonal\")\n",
							"df_RawWJ = df_Raw.withColumn('json',to_json (struct (col ('*')))).select(\"RefId\", \"json\")\n",
							"\n",
							"df_Out = df_RawWJ \\\n",
							".withColumn('LocalId',get_json_object(col('json'), '$.LocalId')) \\\n",
							".withColumn('StateProvinceId',get_json_object(col('json'), '$.StateProvinceId')) \\\n",
							".withColumn('FirstName',get_json_object(col('json'), '$.FirstName')) \\\n",
							".withColumn('LastName',get_json_object(col('json'), '$.LastName')) \\\n",
							".withColumn('MiddleName',get_json_object(col('json'), '$.MiddleName')) \\\n",
							".withColumn('OtherNames',get_json_object(col('json'), '$.OtherNames')) \\\n",
							".withColumn('EmploymentStatus',get_json_object(col('json'), '$.EmploymentStatus')) \\\n",
							".withColumn('Title',get_json_object(col('json'), '$.Title')) \\\n",
							".withColumn('IndigenousStatus',get_json_object(col('json'), '$.IndigenousStatus')) \\\n",
							".withColumn('Sex',get_json_object(col('json'), '$.Sex')) \\\n",
							".withColumn('BirthDate',get_json_object(col('json'), '$.BirthDate')) \\\n",
							".withColumn('DateOfDeath',get_json_object(col('json'), '$.DateOfDeath')) \\\n",
							".withColumn('Deceased',get_json_object(col('json'), '$.Deceased')) \\\n",
							".withColumn('BirthDateVerification',get_json_object(col('json'), '$.BirthDateVerification')) \\\n",
							".withColumn('PlaceOfBirth',get_json_object(col('json'), '$.PlaceOfBirth')) \\\n",
							".withColumn('StateOfBirth',get_json_object(col('json'), '$.StateOfBirth')) \\\n",
							".withColumn('CountryOfBirth ',get_json_object(col('json'), '$.CountryOfBirth ')) \\\n",
							".withColumn('CountryOfCitizenship ',get_json_object(col('json'), '$.CountryOfCitizenship ')) \\\n",
							".withColumn('CountryOfResidency',get_json_object(col('json'), '$.CountryOfResidency')) \\\n",
							".withColumn('CountryArrivalDate',get_json_object(col('json'), '$.CountryArrivalDate')) \\\n",
							".withColumn('AustralianCitizenshipStatus',get_json_object(col('json'), '$.AustralianCitizenshipStatus')) \\\n",
							".withColumn('EnglishProficiency',get_json_object(col('json'), '$.EnglishProficiency')) \\\n",
							".withColumn('LanguageList',get_json_object(col('json'), '$.LanguageList')) \\\n",
							".withColumn('DwellingArrangement',get_json_object(col('json'), '$.DwellingArrangement')) \\\n",
							".withColumn('Religion',get_json_object(col('json'), '$.Religion')) \\\n",
							".withColumn('ReligiousEventList',get_json_object(col('json'), '$.ReligiousEventList')) \\\n",
							".withColumn('ReligiousRegion',get_json_object(col('json'), '$.ReligiousRegion')) \\\n",
							".withColumn('PermanentResident',get_json_object(col('json'), '$.PermanentResident')) \\\n",
							".withColumn('VisaSubClass',get_json_object(col('json'), '$.VisaSubClass')) \\\n",
							".withColumn('VisaStatisticalCode',get_json_object(col('json'), '$.VisaStatisticalCode')) \\\n",
							".withColumn('EmailList',get_json_object(col('json'), '$.EmailList')) \\\n",
							".withColumn('PhoneNumberList',get_json_object(col('json'), '$.PhoneNumberList')) \\\n",
							".withColumn('AddressList',get_json_object(col('json'), '$.AddressList')) \\\n",
							".select('RefId',\t'LocalId',\t'StateProvinceId',\t'FirstName',\t'LastName',\t'MiddleName',\t'OtherNames',\t'EmploymentStatus',\t'Title',\t'IndigenousStatus',\t'Sex',\t'BirthDate',\t'DateOfDeath',\t'Deceased',\t'BirthDateVerification',\t'PlaceOfBirth',\t'StateOfBirth',\t'CountryOfBirth ',\t'CountryOfCitizenship ',\t'CountryOfResidency',\t'CountryArrivalDate',\t'AustralianCitizenshipStatus',\t'EnglishProficiency',\t'LanguageList',\t'DwellingArrangement',\t'Religion',\t'ReligiousEventList',\t'ReligiousRegion',\t'PermanentResident',\t'VisaSubClass',\t'VisaStatisticalCode',\t'EmailList',\t'PhoneNumberList',\t'AddressList',\n",
							") \n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_Out.write.mode(\"overwrite\").saveAsTable(\"dm_staffpersonal\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SIFLoadDimStudentPersonal')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SIF"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "339c5203-5898-44ca-ae88-6e1ec1abebc0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SifDbName = \"sif\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.catalog.setCurrentDatabase(SifDbName)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"df_Raw = spark.sql(\"select * from raw_studentpersonal\")\r\n",
							"df_RawWJ = df_Raw.withColumn('json',to_json (struct (col ('*')))).select(\"RefId\", \"json\")\r\n",
							"\r\n",
							"df_Out = df_RawWJ \\\r\n",
							".withColumn('StateProvinceId',get_json_object(col('json'), '$.StateProvinceId')) \\\r\n",
							".withColumn('NationalUniqueStudentIdentifier',get_json_object(col('json'), '$.NationalUniqueStudentIdentifier')) \\\r\n",
							".withColumn('AlertMessages',get_json_object(col('json'), '$.AlertMessages')) \\\r\n",
							".withColumn('MedicalAlertMessages',get_json_object(col('json'), '$.MedicalAlertMessages')) \\\r\n",
							".withColumn('FirstName',get_json_object(col('json'), '$.PersonInfo.Name.GivenName')) \\\r\n",
							".withColumn('LastName',get_json_object(col('json'), '$.PersonInfo.Name.FamilyName')) \\\r\n",
							".withColumn('MiddleName',get_json_object(col('json'), '$.MiddleName')) \\\r\n",
							".withColumn('OtherNames',get_json_object(col('json'), '$.PersonInfo.OtherNames')) \\\r\n",
							".withColumn('ProjectedGraduationYear',get_json_object(col('json'), '$.ProjectedGraduationYear')) \\\r\n",
							".withColumn('OnTimeGraduationYear',get_json_object(col('json'), '$.OnTimeGraduationYear')) \\\r\n",
							".withColumn('GraduationDate',get_json_object(col('json'), '$.GraduationDate')) \\\r\n",
							".withColumn('MostRecent',get_json_object(col('json'), '$.MostRecent')) \\\r\n",
							".withColumn('AcceptableUsePolicy',get_json_object(col('json'), '$.AcceptableUsePolicy')) \\\r\n",
							".withColumn('GiftedTalented',get_json_object(col('json'), '$.GiftedTalented')) \\\r\n",
							".withColumn('EconomicDisadvantage',get_json_object(col('json'), '$.EconomicDisadvantage')) \\\r\n",
							".withColumn('ESL',get_json_object(col('json'), '$.ESL')) \\\r\n",
							".withColumn('ESLDateAssessed',get_json_object(col('json'), '$.ESLDateAssessed')) \\\r\n",
							".withColumn('YoungCarersRole',get_json_object(col('json'), '$.YoungCarersRole')) \\\r\n",
							".withColumn('Disability',get_json_object(col('json'), '$.Disability')) \\\r\n",
							".withColumn('IntegrationAide',get_json_object(col('json'), '$.IntegrationAide')) \\\r\n",
							".withColumn('EducationSupport',get_json_object(col('json'), '$.EducationSupport')) \\\r\n",
							".withColumn('HomeSchooledStudent',get_json_object(col('json'), '$.HomeSchooledStudent')) \\\r\n",
							".withColumn('IndependentStudent',get_json_object(col('json'), '$.IndependentStudent')) \\\r\n",
							".withColumn('Sensitive',get_json_object(col('json'), '$.Sensitive')) \\\r\n",
							".withColumn('OfflineDelivery',get_json_object(col('json'), '$.OfflineDelivery')) \\\r\n",
							".withColumn('ESLSupport',get_json_object(col('json'), '$.ESLSupport')) \\\r\n",
							".withColumn('PrePrimaryEducation',get_json_object(col('json'), '$.PrePrimaryEducation')) \\\r\n",
							".withColumn('PrePrimaryEducationHours',get_json_object(col('json'), '$.PrePrimaryEducationHours')) \\\r\n",
							".withColumn('FirstAUSchoolEnrollment',get_json_object(col('json'), '$.FirstAUSchoolEnrollment')) \\\r\n",
							".withColumn('EmailList',get_json_object(col('json'), '$.PersonInfo.EmailList')) \\\r\n",
							".withColumn('PhoneNumberList',get_json_object(col('json'), '$.PersonInfo.PhoneNumberList')) \\\r\n",
							".withColumn('AddressList',get_json_object(col('json'), '$.PersonInfo.AddressList')) \\\r\n",
							".withColumn('IndigenousStatus',get_json_object(col('json'), '$.PersonInfo.Demographics.IndigenousStatus')) \\\r\n",
							".withColumn('Sex',get_json_object(col('json'), '$.PersonInfo.Demographics.Sex')) \\\r\n",
							".withColumn('BirthDate',get_json_object(col('json'), '$.PersonInfo.Demographics.BirthDate')) \\\r\n",
							".withColumn('DateOfDeath',get_json_object(col('json'), '$.PersonInfo.Demographics.DateOfDeath')) \\\r\n",
							".withColumn('Deceased',get_json_object(col('json'), '$.PersonInfo.Demographics.Deceased')) \\\r\n",
							".withColumn('BirthDateVerification',get_json_object(col('json'), '$.PersonInfo.Demographics.BirthDateVerification')) \\\r\n",
							".withColumn('PlaceOfBirth',get_json_object(col('json'), '$.PersonInfo.Demographics.PlaceOfBirth')) \\\r\n",
							".withColumn('StateOfBirth',get_json_object(col('json'), '$.PersonInfo.Demographics.StateOfBirth')) \\\r\n",
							".withColumn('CountryOfBirth',get_json_object(col('json'), '$.PersonInfo.Demographics.CountryOfBirth')) \\\r\n",
							".withColumn('CountryOfCitizenship',get_json_object(col('json'), '$.PersonInfo.Demographics.CountriesOfCitizenship')) \\\r\n",
							".withColumn('CountryOfResidency',get_json_object(col('json'), '$.PersonInfo.Demographics.CountriesOfResidency')) \\\r\n",
							".withColumn('CountryArrivalDate',get_json_object(col('json'), '$.PersonInfo.Demographics.CountryArrivalDate')) \\\r\n",
							".withColumn('AustralianCitizenshipStatus',get_json_object(col('json'), '$.PersonInfo.Demographics.AustralianCitizenshipStatus')) \\\r\n",
							".withColumn('EnglishProficiency',get_json_object(col('json'), '$.PersonInfo.Demographics.EnglishProficiency.Code')) \\\r\n",
							".withColumn('MainLanguageSpokenAtHome',get_json_object(col('json'), '$.PersonInfo.Demographics.LanguageList.Language[0].Code')) \\\r\n",
							".withColumn('SecondLanguage',get_json_object(col('json'), '$.PersonInfo.Demographics.LanguageList.Language[1].Code')) \\\r\n",
							".withColumn('OtherLanguage',get_json_object(col('json'), '$.PersonInfo.Demographics.LanguageList.Language[2].Code')) \\\r\n",
							".withColumn('DwellingArrangement',get_json_object(col('json'), '$.PersonInfo.Demographics.DwellingArrangement.Code')) \\\r\n",
							".withColumn('Religion',get_json_object(col('json'), '$.PersonInfo.Demographics.Religion.Code')) \\\r\n",
							".withColumn('ReligiousEventList',get_json_object(col('json'), '$.PersonInfo.Demographics.ReligiousEventList')) \\\r\n",
							".withColumn('ReligiousRegion',get_json_object(col('json'), '$.PersonInfo.Demographics.ReligiousRegion')) \\\r\n",
							".withColumn('PermanentResident',get_json_object(col('json'), '$.PersonInfo.Demographics.PermanentResident')) \\\r\n",
							".withColumn('VisaSubClass',get_json_object(col('json'), '$.PersonInfo.Demographics.VisaSubClass')) \\\r\n",
							".withColumn('VisaStatisticalCode',get_json_object(col('json'), '$.PersonInfo.Demographics.VisaStatisticalCode')) \\\r\n",
							".withColumn('VisaSubClassList',get_json_object(col('json'), '$.PersonInfo.Demographics.VisaSubClassList')) \\\r\n",
							".withColumn('PassportNumber',get_json_object(col('json'), '$.PersonInfo.Demographics.Passport.Number')) \\\r\n",
							".withColumn('PassportExpiryDate',get_json_object(col('json'), '$.PersonInfo.Demographics.Passport.ExpiryDate')) \\\r\n",
							".withColumn('PassportCountry',get_json_object(col('json'), '$.PersonInfo.Demographics.Passport.Country')) \\\r\n",
							".withColumn('LBOTE',get_json_object(col('json'), '$.LBOTE')) \\\r\n",
							".withColumn('InterpreterRequired',get_json_object(col('json'), '$.InterpreterRequired')) \\\r\n",
							".withColumn('ImmunisationCertificateStatus',get_json_object(col('json'), '$.ImmunisationCertificateStatus')) \\\r\n",
							".withColumn('CulturalBackground',get_json_object(col('json'), '$.CulturalBackground')) \\\r\n",
							".withColumn('MaritalStatus',get_json_object(col('json'), '$.MaritalStatus')) \\\r\n",
							".withColumn('MedicareNumber',get_json_object(col('json'), '$.MedicareNumber')) \\\r\n",
							".withColumn('MedicarePositionNumber',get_json_object(col('json'), '$.MedicarePositionNumber')) \\\r\n",
							".withColumn('MedicareCardHolderName',get_json_object(col('json'), '$.MedicareCardHolderName')) \\\r\n",
							".withColumn('PrivateHealthInsurance',get_json_object(col('json'), '$.PrivateHealthInsurance')) \\\r\n",
							".select('RefId',\t'StateProvinceId',\t'NationalUniqueStudentIdentifier',\t'AlertMessages',\t'MedicalAlertMessages',\t'FirstName',\t'LastName',\t'MiddleName',\t'OtherNames',\t'ProjectedGraduationYear',\t'OnTimeGraduationYear',\t'GraduationDate',\t'MostRecent',\t'AcceptableUsePolicy',\t'GiftedTalented',\t'EconomicDisadvantage',\t'ESL',\t'ESLDateAssessed',\t'YoungCarersRole',\t'Disability',\t'IntegrationAide',\t'EducationSupport',\t'HomeSchooledStudent',\t'IndependentStudent',\t'Sensitive',\t'OfflineDelivery',\t'ESLSupport',\t'PrePrimaryEducation',\t'PrePrimaryEducationHours',\t'FirstAUSchoolEnrollment',\t'EmailList',\t'PhoneNumberList',\t'AddressList',\t'IndigenousStatus',\t'Sex',\t'BirthDate',\t'DateOfDeath',\t'Deceased',\t'BirthDateVerification',\t'PlaceOfBirth',\t'StateOfBirth',\t'CountryOfBirth',\t'CountryOfCitizenship',\t'CountryOfResidency',\t'CountryArrivalDate',\t'AustralianCitizenshipStatus',\t'EnglishProficiency',\t'MainLanguageSpokenAtHome', 'SecondLanguage', 'OtherLanguage',\t'DwellingArrangement',\t'Religion',\t'ReligiousEventList',\t'ReligiousRegion',\t'PermanentResident',\t'VisaSubClass',\t'VisaStatisticalCode',\t'VisaSubClassList',\t'PassportNumber', 'PassportExpiryDate', 'PassportCountry',\t'LBOTE',\t'InterpreterRequired',\t'ImmunisationCertificateStatus',\t'CulturalBackground',\t'MaritalStatus',\t'MedicareNumber',\t'MedicarePositionNumber',\t'MedicareCardHolderName',\t'PrivateHealthInsurance') \r\n",
							"\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_Out.write.mode(\"overwrite\").saveAsTable(\"dm_studentpersonal\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(spark.sql(\"select * from dm_studentpersonal\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SIFLoadMain')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SIF"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e191434e-cceb-4cc9-88a5-b4a2dda415fc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"StorageAccountName = \"adsdevdlsadsalfsadsl\"\r\n",
							"StorageAccountContainer = \"datalakeraw\"\r\n",
							"StorageAccountFolder = \"/samples/sif/\"\r\n",
							"SifDbName = \"sif\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Persist Codesets**"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"dict1 = json.loads('{\"StorageAccountName\": \"' + StorageAccountName + '\", \"StorageAccountContainer\":\"' + StorageAccountContainer + '\", \"StorageAccountFolder\": \"'+StorageAccountFolder+'SifOpenApi/\", \"SifDbName\":\"' + SifDbName + '\"}')\r\n",
							"dict2 = json.dumps(dict1)\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFLoadCodeSets\", 900,dict1)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 71
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Declare Function to Create TaskObject**"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def RunDimNoteBook(StorageAccountName, Entity):\r\n",
							"    OutputDict = {}\r\n",
							"    TaskObject = \"{\\\"TaskInstanceId\\\":53,\\\"TaskMasterId\\\":1,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"Execute Synapse Notebook\\\",\\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"dc872650-b992-4cae-9ae2-c714c95563ee\\\",\\\"NumberOfRetries\\\":2,\\\"DegreeOfCopyParallelism\\\":1, \\\"KeyVaultBaseUrl\\\":\\\"https://ads-dev-kv-ads-tad5.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\",\\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-4, \\\"SystemServer\\\":\\\"https://\"+StorageAccountName+\".dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakeraw\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"/samples/sif/\\\",  \\\"TargetRelativePath\\\":\\\"/samples/sif/\"+Entity+\"/\\\"},\\\"DataFileName\\\":\\\"\"+Entity+\".json\\\",\\\"RelativePath\\\":\\\"/samples/sif/\\\",\\\"SchemaFileName\\\":\\\"\\\",\\\"Type\\\":\\\"Notebook-Optional\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\"}, \\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-4,\\\"SystemServer\\\":\\\"https://\"+StorageAccountName+\".dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakeraw\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"/samples/sif/\\\",\\\"TargetRelativePath\\\":\\\"/samples/sif/\"+Entity+\"/\\\"},\\\"DataFileName\\\":\\\"\"+Entity+\".parquet\\\",\\\"RelativePath\\\":\\\"/samples/sif/\"+Entity+\"/\\\", \\\"SchemaFileName\\\":\\\"\\\",\\\"Type\\\":\\\"Notebook-Optional\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\"},\\\"TMOptionals\\\":{\\\"CustomDefinitions\\\":\\\"\\\",\\\"ExecuteNotebook\\\":\\\"SIFParameterizedJson\\\",\\\"Purview\\\":\\\"Disabled\\\", \\\"QualifiedIDAssociation\\\":\\\"TaskMasterId\\\",\\\"UseNotebookActivity\\\":\\\"Enabled\\\"}}\"\r\n",
							"    OutputDict['TaskObject'] = TaskObject\r\n",
							"    mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 87
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Generate Task Object and Execute Delta Lake Load for Each Raw SIF Entity**"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"entities = [\"CalendarDate\" \\\r\n",
							",\"GradingAssignment\" \\\r\n",
							",\"GradingAssignmentScore\" \\\r\n",
							",\"LearningStandardItem\" \\\r\n",
							",\"MarkValueInfo\" \\\r\n",
							",\"SchoolInfo2\" \\\r\n",
							",\"SectionInfo\" \\\r\n",
							",\"StaffAssignment\" \\\r\n",
							",\"StaffPersonal\" \\\r\n",
							",\"StudentContactPersonal\" \\\r\n",
							",\"StudentContactRelationship\" \\\r\n",
							",\"StudentDailyAttendance\" \\\r\n",
							",\"StudentGrade\" \\\r\n",
							",\"StudentPersonal\" \\\r\n",
							",\"StudentSchoolEnrollment\" \\\r\n",
							",\"StudentScoreJudgementAgainstStandard\" \\\r\n",
							",\"StudentSectionEnrollment\" \\\r\n",
							",\"TeachingGroup\" \\\r\n",
							",\"TermInfo\" \\\r\n",
							",\"TermInfo2\" \\\r\n",
							",\"TermInfo3\" \\\r\n",
							",\"schoolinfo\" \\\r\n",
							"]\r\n",
							"\r\n",
							"\r\n",
							"from multiprocessing.pool import ThreadPool\r\n",
							"pool = ThreadPool(5)\r\n",
							"pool.map(lambda e: RunDimNoteBook(StorageAccountName, e),entities)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"OutputDict = {}\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"CalendarDate\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"GradingAssignment\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"GradingAssignmentScore\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"LearningStandardItem\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"MarkValueInfo\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"SchoolInfo2\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"SectionInfo\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StaffAssignment\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StaffPersonal\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StudentContactPersonal\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StudentContactRelationship\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StudentDailyAttendance\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StudentGrade\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StudentPersonal\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StudentSchoolEnrollment\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StudentScoreJudgementAgainstStandard\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"StudentSectionEnrollment\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"TeachingGroup\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"TermInfo\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"TermInfo2\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"TermInfo3\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"OutputDict['TaskObject'] = GetTaskObject(StorageAccountName = StorageAccountName, Entity=\"schoolinfo\")\r\n",
							"mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFParameterizedJson\",900, OutputDict)\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 73
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Create SIF Dimensional Artefacts**"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import json\r\n",
							"from pyspark.sql.types import StructType,StructField, StringType,LongType\r\n",
							"from pyspark.sql.functions import concat,col\r\n",
							"\r\n",
							"\r\n",
							"#Schema = StructType([       \r\n",
							"#    StructField('Notebookname', StringType(), True)   \r\n",
							"#])\r\n",
							"\r\n",
							"dict1 = json.loads('{\"SifDbName\":\"' + SifDbName + '\"}')\r\n",
							"\r\n",
							"#Data = [[\"SIFLoadStaffPersonal\"],[\"SIFLoadStudentPersonal\"]]\r\n",
							"#df = spark.createDataFrame(data = Data, schema = [\"NotebookName\"])\r\n",
							"\r\n",
							"def fnc_calldimnotebook(NotebookName, dict1, returnType=StringType()):\r\n",
							"  NotebookName = concat('FrameworkNotebooks/sif/' + str(NotebookName))  \r\n",
							"  #mssparkutils.notebook.run(str(NotebookName), 900,dict1)\r\n",
							"  return NotebookName\r\n",
							"\r\n",
							"#spark.udf.register('fnc_calldimnotebook_udf', fnc_calldimnotebook, StringType())\r\n",
							"\r\n",
							"#df2 = df.withColumn(\"FncCall\", fnc_calldimnotebook(col('NotebookName'), dict1))\r\n",
							"#display(df2)\r\n",
							"\r\n",
							"from multiprocessing.pool import ThreadPool\r\n",
							"pool = ThreadPool(5)\r\n",
							"notebooks = ['SIFLoadDimStaffPersonal','SIFLoadDimStudentPersonal']\r\n",
							"pool.map(lambda path: mssparkutils.notebook.run(\"/FrameworkNotebooks/sif/\"+path, timeout_seconds= 600, arguments=dict1),notebooks)\r\n",
							"\r\n",
							"#mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFLoadStaffPersonal\", 900,dict1)\r\n",
							"#mssparkutils.notebook.run(\"FrameworkNotebooks/sif/SIFLoadStudentPersonal\", 900,dict1)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SIFParameterizedJson')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SIF"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fce14def-d2a6-4a53-9bf1-274f2ec193a5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"TaskObject = \"{\\\"TaskInstanceId\\\":53,\\\"TaskMasterId\\\":1,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"Execute Synapse Notebook\\\",\\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"dc872650-b992-4cae-9ae2-c714c95563ee\\\",\\\"NumberOfRetries\\\":2,\\\"DegreeOfCopyParallelism\\\":1,\\\"KeyVaultBaseUrl\\\":\\\"https://ads-dev-kv-ads-tad5.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\",\\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-4,\\\"SystemServer\\\":\\\"https://adsdevdlsadstad5adsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakeraw\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"/samples/sif/\\\",\\\"TargetRelativePath\\\":\\\"/samples/sif/StudentPersonal/\\\"},\\\"DataFileName\\\":\\\"StudentPersonal.json\\\",\\\"RelativePath\\\":\\\"/samples/sif/\\\",\\\"SchemaFileName\\\":\\\"\\\",\\\"Type\\\":\\\"Notebook-Optional\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\"},\\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-4,\\\"SystemServer\\\":\\\"https://adsdevdlsadstad5adsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakeraw\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"/samples/sif/\\\",\\\"TargetRelativePath\\\":\\\"/samples/sif/StudentPersonal/\\\"},\\\"DataFileName\\\":\\\"StudentPersonal.parquet\\\",\\\"RelativePath\\\":\\\"/samples/sif/StudentPersonal/\\\",\\\"SchemaFileName\\\":\\\"\\\",\\\"Type\\\":\\\"Notebook-Optional\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\"},\\\"TMOptionals\\\":{\\\"CustomDefinitions\\\":\\\"\\\",\\\"ExecuteNotebook\\\":\\\"SIFParameterizedJson\\\",\\\"Purview\\\":\\\"Disabled\\\",\\\"QualifiedIDAssociation\\\":\\\"TaskMasterId\\\",\\\"UseNotebookActivity\\\":\\\"Enabled\\\"}}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"TaskObject"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import random\r\n",
							"import json\r\n",
							"from pyspark.sql import Row\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"session_id = random.randint(0,1000000)\r\n",
							"#invalid source\r\n",
							"##TaskObject = \"{\\\"TaskInstanceId\\\":1,\\\"TaskMasterId\\\":2,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"TestTask Type Name\\\",\\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"8448eabb-9ba4-4779-865b-29e973431273\\\",\\\"NumberOfRetries\\\":0,\\\"DegreeOfCopyParallelism\\\":1,\\\"KeyVaultBaseUrl\\\":\\\"https://ark-stg-kv-ads-irud.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\",\\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"ExecutionEngine\\\":{\\\"EngineId\\\":-1,\\\"EngineName\\\":\\\"ark-stg-adf-ads-irud\\\",\\\"SystemType\\\":\\\"Datafactory\\\",\\\"ResourceGroup\\\":\\\"dlzdev04\\\",\\\"SubscriptionId\\\":\\\"ed1206e0-17c7-4bc2-ad4b-f8d4dab9284f\\\",\\\"ADFPipeline\\\":\\\"GPL_AzureSqlTable_NA_AzureBlobFS_Parquet_Azure\\\",\\\"EngineJson\\\":\\\"{}\\\",\\\"TaskDatafactoryIR\\\":\\\"Azure\\\",\\\"JsonProperties\\\":{}},\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://arkstgdlsadsirudadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakelanding\\\"},\\\"Instance\\\":{\\\"TargetRelativePath\\\":\\\"\\\"},\\\"DataFileName\\\":\\\"TestFile.parquet\\\",\\\"RelativePath\\\":\\\"\\\",\\\"SchemaFileName\\\":\\\"TestFile.json\\\"},\\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://arkstgdlsadsirudadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakelanding\\\"},\\\"Instance\\\":{\\\"TargetRelativePath\\\":\\\"\\\"},\\\"DataFileName\\\":\\\"TestFile.parquet\\\",\\\"RelativePath\\\":\\\"\\\",\\\"SchemaFileName\\\":\\\"TestFile.json\\\",\\\"Type\\\":\\\"Parquet\\\"}}\"\r\n",
							"#valid source\r\n",
							"#TaskObject = \"{\\\"TaskInstanceId\\\":1,\\\"TaskMasterId\\\":2,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"TestTask Type Name\\\", \\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"8448eabb-9ba4-4779-865b-29e973431273\\\",\\\"NumberOfRetries\\\":0,\\\"DegreeOfCopyParallelism\\\":1, \\\"KeyVaultBaseUrl\\\":\\\"https://ads-dev-kv-ads-ic038069.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\", \\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"ExecutionEngine\\\":{\\\"EngineId\\\":-1,\\\"EngineName\\\":\\\"ads-dev-kv-ads-ic038069\\\", \\\"SystemType\\\":\\\"Microsoft.Synapse/workspaces\\\",\\\"ResourceGroup\\\":\\\"sifgofast\\\",\\\"SubscriptionId\\\":\\\"cd486ba9-eef3-466d-b16c-7f1b2941ae9d\\\", \\\"ADFPipeline\\\":\\\"GPL_AzureSqlTable_NA_AzureBlobFS_Parquet_Azure\\\",\\\"EngineJson\\\":\\\"{}\\\",\\\"TaskDatafactoryIR\\\":\\\"Azure\\\", \\\"JsonProperties\\\":{}},\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://adsdevdlsadsic03adsl.blob.core.windows.net\\\", \\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"adsdevdlsadsic03\\\"},\\\"Instance\\\":\\\"\\\",{\\\"TargetRelativePath\\\":\\\"synapse/sif\\\"}, \\\"DataFileName\\\":\\\"StudentPersonal.parquet\\\",\\\"SourceRelativePath\\\":\\\"synapse/sif\\\",\\\"SchemaFileName\\\":\\\"StudentPersonal.json\\\",\\\"Type\\\":\\\"Parquet\\\"}, \\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://adsdevdlsadsic03adsl.blob.core.windows.net\\\", \\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ABS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"adsdevdlsadsic03\\\"}, \\\"Instance\\\":{\\\"TargetRelativePath\\\":\\\"\\\"},\\\"DataFileName\\\":\\\"StudentPersonal.parquet\\\",\\\"SourceRelativePath\\\":\\\"synapse\\/sif\\\", \\\"SchemaFileName\\\":\\\"StudentPersonal.json\\\",\\\"Type\\\":\\\"Parquet\\\"}}\"\r\n",
							"TaskDict = {}\r\n",
							"OutputDict = {}\r\n",
							"TaskObjectJson = json.loads(TaskObject)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"##we want to delete EngineJson as it causes issues when converting back to a json and it is not needed as its properties are within JsonProperties as children\r\n",
							"try:\r\n",
							"    del TaskObjectJson['ExecutionEngine']['EngineJson']\r\n",
							"except:\r\n",
							"    print(\"No EngineJson Found\")\r\n",
							"\r\n",
							"Source = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\").replace(\"blob\",\"dfs\") + \"/\"\r\n",
							"Source = Source.replace('///', '/')\r\n",
							"Source = Source.replace('//', '/')\r\n",
							"print(Source)\r\n",
							"Source = Source + TaskObjectJson['Source'][\"Instance\"]['SourceRelativePath']\r\n",
							"print(Source)\r\n",
							"Source = Source  +\"/\" + TaskObjectJson['Source']['DataFileName'] \r\n",
							"print(Source)\r\n",
							"Source = Source.replace('///', '/')\r\n",
							"Source = Source.replace('//', '/')\r\n",
							"print(Source)\r\n",
							"Source = \"abfss://\" + Source \r\n",
							"print(Source)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Target = TaskObjectJson['Target']['System']['Container'] + \"@\" + TaskObjectJson['Target']['System']['SystemServer'].replace(\"https://\",\"\").replace(\"blob\",\"dfs\") + \"/\"\r\n",
							"Target = Target.replace('///', '/')\r\n",
							"Target = Target.replace('//', '/')\r\n",
							"print(Target)\r\n",
							"Target = Target + TaskObjectJson['Target'][\"Instance\"]['TargetRelativePath']\r\n",
							"Target = Target  +\"/\" + TaskObjectJson['Target']['DataFileName'] \r\n",
							"print(Target)\r\n",
							"Target = Target.replace('///', '/')\r\n",
							"Target = Target.replace('//', '/')\r\n",
							"print(Target)\r\n",
							"Target = \"abfss://\" + Target \r\n",
							"print(Target)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import col, explode_outer, from_json, lit, concat\r\n",
							"from pyspark.sql.types import StructType, ArrayType\r\n",
							"print(Source)\r\n",
							"input_df = spark.read.option(\"multiline\",\"true\").json(Source)\r\n",
							"#display(input_df)\r\n",
							"\r\n",
							"\r\n",
							"#output_df = execute_autoflatten_with_PK(input_df,None)\r\n",
							"#display(output_df)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\r\n",
							"\r\n",
							"df = input_df\r\n",
							"\r\n",
							"#SIF model uses RefId as Unique identifier in all the structures\r\n",
							"mergeCondition = \"oldData.RefId = newData.RefId\"\r\n",
							"\r\n",
							"\r\n",
							"print(\"SourceDT = \" + TaskObjectJson['Source']['Type']  + \", TargetDT = \" + TaskObjectJson['Target']['Type'] )\r\n",
							"#df = spark.read.load(Source, format=SourceDT)\r\n",
							"\r\n",
							"var_check = DeltaTable.isDeltaTable(spark, Target )\r\n",
							"try:\r\n",
							"    if (var_check):\r\n",
							"        print(\"Performing Merge... on Existing table\")\r\n",
							"\r\n",
							"        olddt = DeltaTable.forPath(spark, Target) \r\n",
							"\r\n",
							"        olddt.alias(\"oldData\").merge(\r\n",
							"            df.alias(\"newData\"),\r\n",
							"            mergeCondition) \\\r\n",
							"        .whenMatchedUpdateAll() \\\r\n",
							"        .whenNotMatchedInsertAll() \\\r\n",
							"        .execute()\r\n",
							"    else:\r\n",
							"        print(\"Creating new Delta Table.\")    \r\n",
							"        df.write.format(\"Delta\").save(Target)\r\n",
							"except:\r\n",
							"    print(\"Table does not exist. Creating new Delta Table.\")    \r\n",
							"    df.write.format(\"Delta\").save(Target)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"Creating Spark Table\")\r\n",
							"df = spark.read.load(Target, format='delta')\r\n",
							"targetDB = \"sif\"\r\n",
							"targetTable = \"raw_\" + TaskObjectJson['Target']['DataFileName'].split(\".\")[0] \r\n",
							"#if the target datatype is parquet then we do not need to create a copy of the data - we can use the recently saved sink target\r\n",
							"if (TaskObjectJson['Target']['Type'] == 'Parquet'):\r\n",
							"    SnapshotTarget = Target\r\n",
							"else:\r\n",
							"    SnapshotTarget = Target + '/Snapshot/' + targetTable\r\n",
							"    #we need to update the parquet file - this is not very efficient but there isnt a current better way as delta tables are not supported for persistent tables\r\n",
							"    df.write.format(\"parquet\").mode(\"overwrite\").save(SnapshotTarget)\r\n",
							"\r\n",
							"\r\n",
							"#we need to make the DB and table lowercase as synapse persistent tables dont identify them as different identities\r\n",
							"targetDB = targetDB.lower()\r\n",
							"targetTable = targetTable.lower()\r\n",
							"\r\n",
							"#check if the specified DB / table exists - if so only do required actions.\r\n",
							"dbList = spark.catalog.listDatabases()\r\n",
							"dbExists = False\r\n",
							"for db in dbList:\r\n",
							"    if (db.name == targetDB):\r\n",
							"        dbExists = True\r\n",
							"        break\r\n",
							"if (dbExists):\r\n",
							"    print(\"DB Exists\")\r\n",
							"    tableExists = False\r\n",
							"    spark.catalog.setCurrentDatabase(targetDB)\r\n",
							"    tableList = spark.catalog.listTables()\r\n",
							"    for table in tableList:\r\n",
							"        if (table.name == targetTable):\r\n",
							"            tableExists = True\r\n",
							"            break\r\n",
							"    if (tableExists):\r\n",
							"        print(\"Table exists - nothing needed to be done\")\r\n",
							"        spark.catalog.refreshTable(targetTable)\r\n",
							"    else:\r\n",
							"        print(\"Table doesnt exist - creating\")\r\n",
							"        spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')\r\n",
							"else:\r\n",
							"    print(\"DB Doesnt exist - creating DB and table\")\r\n",
							"    createDBString = \"CREATE DATABASE \" + targetDB \r\n",
							"    spark.sql(createDBString)\r\n",
							"    spark.catalog.setCurrentDatabase(targetDB)\r\n",
							"    spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SchoolPerformance')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "School Performance data including attendance",
				"folder": {
					"name": "Insights Module"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark8pmd",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7505978a-352d-4aa4-870c-d2f075d0edf3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark8pmd",
						"name": "spark8pmd",
						"type": "Spark",
						"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark8pmd",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 59
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# School Performance Insights Module Example Notebook\r\n",
							"This notebook creates a table into  Spark database called bceoea.\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Provision storage accounts\r\n",
							"\r\n",
							"The storage account variable has to be changed to the name of the storage account associated with your Azure resource group."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set('spark.sql.execution.arrow.enabled', 'false')"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# data lake and container information\r\n",
							"# meant to be parameters\r\n",
							"storage_account = 'bceaaeoeadevlrs'\r\n",
							"use_test_env = False\r\n",
							"# NOTE NOT / AT THE BEGINNING\r\n",
							"source_folder='BCE/BI/Dimensions'"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.window import Window\r\n",
							"import pandas as pd\r\n",
							"from faker import Faker\r\n",
							"import datetime\r\n",
							""
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Load Raw Data from Lake\r\n",
							"The top cell sets the date of the data to be processed (this is currently set up to process the test data). \r\n",
							"\r\n",
							"Date can be used for watermark, but Insights class process everything in stage1 folder, \r\n",
							"for effectively control date needs to be used for roster and activity"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if use_test_env:\r\n",
							"    stage1np = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1np'\r\n",
							"    stage2np = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2np'\r\n",
							"    stage2p = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2p'\r\n",
							"else:\r\n",
							"    stage1np = oea.stage1np\r\n",
							"    stage2np = oea.stage2np\r\n",
							"    stage2p = oea.stage3np"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 1. School Performance table\r\n",
							"Contains all School Performance for maths and English\r\n",
							"\r\n",
							"** Databases and tables used: **\r\n",
							"\r\n",
							" - None \r\n",
							" \r\n",
							"**CSV files used:**\r\n",
							"\r\n",
							"- BCE/BI/SchoolPerformance (whatever the set date is)/*.csv\r\n",
							"\r\n",
							"**Database and table created:**\r\n",
							"\r\n",
							"1. Spark DB: \r\n",
							"- Table: SchoolPerformance\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /biDimensions_py"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 0) Initialize the OEA framework and MSInsights module.\r\n",
							"#oea = OEA(storage_account)\r\n",
							"\r\n",
							"classBiDim = biDimensions(source_folder)"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(source_folder)\r\n",
							"print(classBiDim.stage1np)\r\n",
							"print(classBiDim.source)"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"   "
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"classBiDim.returnDataFrameBeforeProcessing(\"SRS_SubjectNames.csv\")"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#table_name='Locations'\r\n",
							"\r\n",
							"classBiDim.ingest()"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Set the date to be processed\r\n",
							"today = datetime.datetime.now()\r\n",
							"date1 = today.strftime('%Y-%m-%d')\r\n",
							"#date1.cast('string')\r\n",
							"date2 = date1"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.insert_watermark( source_folder, \"Locations\", date2)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## School Classification\r\n",
							"\r\n",
							"reassign some variables and init related classes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# load needed activity table(s) from data lake storage\r\n",
							"# NOTE NOT / AT THE BEGINNING\r\n",
							"source_folder='BCE/BI/SchoolClassification/'\r\n",
							"print (f'{stage1np}/{source_folder}')\r\n",
							"\r\n",
							"table_name='SchoolClassification'\r\n",
							"\r\n",
							"dfSchoolPerformanceRaw = pd.read_excel(f'{stage1np}/{source_folder}{table_name}' + '.xlsx')\r\n",
							"\r\n",
							"#spark.read.format(\"com.elastacloud.spark.excel\").load(f'{stage1np}{source_folder}' + 'SchoolClassification.xlsx')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#ideally column names need curation otherwise \r\n",
							"dfSchoolPerformanceRaw['pk'] = dfSchoolPerformanceRaw['School'] + dfSchoolPerformanceRaw['Year'].astype(str)\r\n",
							"\r\n",
							"#dfSchoolPerformanceClear= oea.fix_column_names(df=dfSchoolPerformanceRaw)\r\n",
							"#only used for now only for the schema definition\r\n",
							"classSchool= SchoolPerformance()"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dfSchoolPNaN=dfSchoolPerformanceRaw.fillna(0)"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#for item in classSchool.schemas['SchoolClassification']:\r\n",
							"#    print(item[0] + item[1])\r\n",
							"#    if item[1] == 'Integer' or item[1] == 'float64':\r\n",
							"#        dfSchoolPerformanceRaw.fillna(item[0], 0)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#For %debug\r\n",
							"#NaN\r\n",
							"dfSchoolPerformanceRaw.info\r\n",
							"dfSchoolPNaN.info"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set('spark.sql.execution.arrow.enabled', 'false')\r\n",
							"dfschool=spark.createDataFrame(dfSchoolPNaN)"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display( dfschool.limit(10))"
						],
						"outputs": [],
						"execution_count": 114
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.ingest_snapshot_data_frame(inpdataframe=dfschool, source_path= f'{source_folder}{table_name}', primary_key='pk', schema=classSchool.schemas['SchoolClassification'])"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.insert_watermark( source_folder, \"SchoolClassification\", date2)"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Below this line was meant for testing purposes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"db_name= 'bceoea'\r\n",
							"#pathp = classSchool.stage2p\r\n",
							"#pathnp = classSchool.stage2np\r\n",
							"pathp  = classBiDim.stage2p #.replace('//','/')\r\n",
							"pathnp = classBiDim.stage2np#.replace('//','/')\r\n",
							"source_format= 'DELTA'\r\n",
							"oea.create_sql_views(pathp, source_format)\r\n",
							"#oea.create_sql_views(pathnp, source_format)"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls( pathp )"
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.get_folders( pathp )\r\n",
							""
						],
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": 130
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SchoolPerformanceTable')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Table creation",
				"folder": {
					"name": "SIF"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "874961c2-8f56-4cee-ba74-b40c0341c473"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SifDbName = \"oea\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.catalog.setCurrentDatabase(SifDbName)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\n",
							"df_Raw = spark.sql(\"select * from raw_staffpersonal\")\n",
							"df_RawWJ = df_Raw.withColumn('json',to_json (struct (col ('*')))).select(\"RefId\", \"json\")\n",
							"\n",
							"df_Out = df_RawWJ \\\n",
							".withColumn('LocalId',get_json_object(col('json'), '$.LocalId')) \\\n",
							".withColumn('StateProvinceId',get_json_object(col('json'), '$.StateProvinceId')) \\\n",
							".withColumn('FirstName',get_json_object(col('json'), '$.FirstName')) \\\n",
							".withColumn('LastName',get_json_object(col('json'), '$.LastName')) \\\n",
							".withColumn('MiddleName',get_json_object(col('json'), '$.MiddleName')) \\\n",
							".withColumn('OtherNames',get_json_object(col('json'), '$.OtherNames')) \\\n",
							".withColumn('EmploymentStatus',get_json_object(col('json'), '$.EmploymentStatus')) \\\n",
							".withColumn('Title',get_json_object(col('json'), '$.Title')) \\\n",
							".withColumn('IndigenousStatus',get_json_object(col('json'), '$.IndigenousStatus')) \\\n",
							".withColumn('Sex',get_json_object(col('json'), '$.Sex')) \\\n",
							".withColumn('BirthDate',get_json_object(col('json'), '$.BirthDate')) \\\n",
							".withColumn('DateOfDeath',get_json_object(col('json'), '$.DateOfDeath')) \\\n",
							".withColumn('Deceased',get_json_object(col('json'), '$.Deceased')) \\\n",
							".withColumn('BirthDateVerification',get_json_object(col('json'), '$.BirthDateVerification')) \\\n",
							".withColumn('PlaceOfBirth',get_json_object(col('json'), '$.PlaceOfBirth')) \\\n",
							".withColumn('StateOfBirth',get_json_object(col('json'), '$.StateOfBirth')) \\\n",
							".withColumn('CountryOfBirth ',get_json_object(col('json'), '$.CountryOfBirth ')) \\\n",
							".withColumn('CountryOfCitizenship ',get_json_object(col('json'), '$.CountryOfCitizenship ')) \\\n",
							".withColumn('CountryOfResidency',get_json_object(col('json'), '$.CountryOfResidency')) \\\n",
							".withColumn('CountryArrivalDate',get_json_object(col('json'), '$.CountryArrivalDate')) \\\n",
							".withColumn('AustralianCitizenshipStatus',get_json_object(col('json'), '$.AustralianCitizenshipStatus')) \\\n",
							".withColumn('EnglishProficiency',get_json_object(col('json'), '$.EnglishProficiency')) \\\n",
							".withColumn('LanguageList',get_json_object(col('json'), '$.LanguageList')) \\\n",
							".withColumn('DwellingArrangement',get_json_object(col('json'), '$.DwellingArrangement')) \\\n",
							".withColumn('Religion',get_json_object(col('json'), '$.Religion')) \\\n",
							".withColumn('ReligiousEventList',get_json_object(col('json'), '$.ReligiousEventList')) \\\n",
							".withColumn('ReligiousRegion',get_json_object(col('json'), '$.ReligiousRegion')) \\\n",
							".withColumn('PermanentResident',get_json_object(col('json'), '$.PermanentResident')) \\\n",
							".withColumn('VisaSubClass',get_json_object(col('json'), '$.VisaSubClass')) \\\n",
							".withColumn('VisaStatisticalCode',get_json_object(col('json'), '$.VisaStatisticalCode')) \\\n",
							".withColumn('EmailList',get_json_object(col('json'), '$.EmailList')) \\\n",
							".withColumn('PhoneNumberList',get_json_object(col('json'), '$.PhoneNumberList')) \\\n",
							".withColumn('AddressList',get_json_object(col('json'), '$.AddressList')) \\\n",
							".select('RefId',\t'LocalId',\t'StateProvinceId',\t'FirstName',\t'LastName',\t'MiddleName',\t'OtherNames',\t'EmploymentStatus',\t'Title',\t'IndigenousStatus',\t'Sex',\t'BirthDate',\t'DateOfDeath',\t'Deceased',\t'BirthDateVerification',\t'PlaceOfBirth',\t'StateOfBirth',\t'CountryOfBirth ',\t'CountryOfCitizenship ',\t'CountryOfResidency',\t'CountryArrivalDate',\t'AustralianCitizenshipStatus',\t'EnglishProficiency',\t'LanguageList',\t'DwellingArrangement',\t'Religion',\t'ReligiousEventList',\t'ReligiousRegion',\t'PermanentResident',\t'VisaSubClass',\t'VisaStatisticalCode',\t'EmailList',\t'PhoneNumberList',\t'AddressList',\n",
							") \n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_Out.write.mode(\"overwrite\").saveAsTable(\"dm_staffpersonal\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/biDimensions_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "WIP meant to contain the schemas for bi dimensions",
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0ccbf9f7-0f96-487e-8afe-0db56195b3b0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": true
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# BCE BI Dimensions Class"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class biDimensions(BaseOEAModule):\r\n",
							"    \"\"\"\r\n",
							"    Provides data processing methods for biDimensions data.\r\n",
							"    Data is expected to be received via ADS into stage1np/BCE/BI\r\n",
							"    The structure of the folders in stage1np will then be something like:\r\n",
							"    In stage2, everything is written to stage2np/BCE/BI and stage2p/ms_insights\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    def __init__(self, source_folder= 'BCE/BI/Dimensions'):\r\n",
							"        BaseOEAModule.__init__(self, source_folder)\r\n",
							"      \r\n",
							"        self.schemas['Locations'] = [\r\n",
							"                        ['ADMIN_LOCATION','string','no-op'],\r\n",
							"                        ['SUB_LOCATION','string','no-op'],\r\n",
							"                        ['ADMIN_DESC','string','no-op'],\r\n",
							"                        ['Directorate','string','no-op'],\r\n",
							"                        ['ServiceCentre','string','no-op'],\r\n",
							"                        ['Department','string','no-op'],\r\n",
							"                        ['CampusID','Integer','no-op'],\r\n",
							"                        ['SchoolType','string','no-op'],\r\n",
							"                        ['IntegrumOU','string','no-op'],\r\n",
							"                        ['OU4','string','no-op'],\r\n",
							"                        ['Directorate_old','string','no-op'],\r\n",
							"                        ['LocationType','string','no-op'],\r\n",
							"                        ['ADMIN_LOCATION_PBI','string','no-op'],\r\n",
							"                        ['Clusters','string','no-op'],\r\n",
							"                        ['Suburb','string','no-op'],\r\n",
							"                        ['Directorate_Sort','string','no-op'],\r\n",
							"                        ['School Name','string','no-op'],\r\n",
							"                        ['School_Code','Integer','no-op'],\r\n",
							"                        ['Campus_Code','Integer','no-op'],\r\n",
							"                        ['Campus Name','string','no-op'],\r\n",
							"                        ['latitude','string','no-op'],\r\n",
							"                        ['longitude','string','no-op'],\r\n",
							"                        ['LatLong','string','no-op'],\r\n",
							"                        ['OfficeName','string','no-op'],\r\n",
							"                        ['Suburb - School','string','no-op'],\r\n",
							"                        ['GeoType','string','no-op'],\r\n",
							"                        ['LeuvenCode','string','no-op']\r\n",
							"                        ]\r\n",
							"        self.schemas['SRS_ScaleScoreConversion'] = [['Score', 'integer', 'no-op'],\r\n",
							"                                    ['Mark', 'string', 'no-op']]\r\n",
							" \r\n",
							"        self.schemas['SRS_SubjectNames'] = [['SubjectName', 'string', 'no-op'],\r\n",
							"                                            ['SubjectGroupName', 'string', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['Dates_WithSchoolTerm']=[  ['YearCalendar','integer','no-op'],\r\n",
							"                ['MonthNumberOfYear','integer','no-op'],\r\n",
							"                ['Month Dates','string','no-op'],\r\n",
							"                ['WeekNumberOfYear','integer','no-op'],\r\n",
							"                ['Week','string','no-op'],\r\n",
							"                ['DateKey','integer','no-op'],\r\n",
							"                ['FullDate','timestamp','no-op'],\r\n",
							"                ['ShortDate','timestamp','no-op'],\r\n",
							"                ['DayNumberOfWeek','integer','no-op'],\r\n",
							"                ['DayNameOfWeek','string','no-op'],\r\n",
							"                ['WeekDayType','string','no-op'],\r\n",
							"                ['DayNumberOfMonth','integer','no-op'],\r\n",
							"                ['DayNumberOfYear','integer','no-op'],\r\n",
							"                ['MonthNameOfYear','string','no-op'],\r\n",
							"                ['QuarterNumberCalendar','integer','no-op'],\r\n",
							"                ['QuarterNameCalendar','string','no-op'],\r\n",
							"                ['SemesterNumberCalendar','integer','no-op'],\r\n",
							"                ['SemesterNameCalendar','string','no-op'],\r\n",
							"                ['Term','integer','no-op'],\r\n",
							"                ['TermName','string','no-op'],\r\n",
							"                ['FutureDate','string','no-op'],\r\n",
							"                ['IsToday','string','no-op'],\r\n",
							"                ['Today Date','timestamp','no-op'],\r\n",
							"                ['Year - Week','string','no-op'],\r\n",
							"                ['Year - Month','string','no-op'],\r\n",
							"                ['School Week','integer','no-op'],\r\n",
							"                ['School Day','integer','no-op'],\r\n",
							"                ['School Week Name','string','no-op'],\r\n",
							"                ['Day of Term','integer','no-op'],\r\n",
							"                ['Day of Term Reverse','integer','no-op'],\r\n",
							"                ['Work Days','integer','no-op']]\r\n",
							"\r\n",
							"        self.source = self.source_folder\r\n",
							"        self.table_name = ''\r\n",
							"                    \r\n",
							"    \r\n",
							"    def ingest(self):\r\n",
							"        \"\"\"  Processes insights data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
							"        logger.info(\"Processing microsoft_insights data from: \" + self.stage1np)\r\n",
							"        \r\n",
							"        items = mssparkutils.fs.ls(self.stage1np)\r\n",
							"        for item in items:\r\n",
							"            df= self.returnDataFrameBeforeProcessing(filename=item.name)\r\n",
							"            if item.name == \"Locations.csv\":\r\n",
							"                oea.ingest_snapshot_data_frame( inpdataframe=df,source_path=f'{self.source_folder}/{item.name}', schema=self.schemas[f'{self.table_name}'], primary_key='CampusID')\r\n",
							"            if item.name == \"SRS_SubjectNames.csv\":\r\n",
							"                oea.ingest_snapshot_data_frame( inpdataframe=df,source_path=f'{self.source_folder}/{item.name}', schema=self.schemas[f'{self.table_name}'], primary_key='SubjectName')\r\n",
							"            if item.name == \"SRS_ScaleScoreConversion.csv\":\r\n",
							"                oea.ingest_snapshot_data_frame( inpdataframe=df,source_path=f'{self.source_folder}/{item.name}', schema=self.schemas[f'{self.table_name}'], primary_key='Score')\r\n",
							"            if item.name == \"Dates_WithSchoolTerm.csv\":\r\n",
							"                oea.ingest_snapshot_data_frame( inpdataframe=df,source_path=f'{self.source_folder}/{item.name}', schema=self.schemas[f'{self.table_name}'], primary_key='DateKey')\r\n",
							"\r\n",
							"            else:\r\n",
							"                logger.info(\"No defined function for processing this insights data\"+item.name)\r\n",
							"        \r\n",
							"        logger.info(\"Finished ingesting insights data from stage 1 to stage 2\")\r\n",
							"    \r\n",
							"    def returnDataFrameBeforeProcessing(self, filename='', sourcepath=''):\r\n",
							"        \"\"\" Returns a Spark DF from the requested source, based on file or foder/path. \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights activity data from: \" + sourcepath)\r\n",
							"        if sourcepath == '': \r\n",
							"            self.source=self.stage1np  + '/'+ filename\r\n",
							"            self.table_name = filename.replace('.csv','')\r\n",
							"        else:\r\n",
							"            self.source=self.stage1np  \r\n",
							"        \r\n",
							"        # Currently not using the OEA ingest_incremental_data function due to pulling out the partition folders\r\n",
							"        spark_schema = oea.to_spark_schema(self.schemas[f'{self.table_name}'])\r\n",
							"        df = spark.read.csv(self.source, header='True', schema=spark_schema)\r\n",
							"\r\n",
							"        return df\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"class SchoolPerformance(BaseOEAModule):\r\n",
							"    \"\"\"\r\n",
							"    Provides data processing methods for biDimensions data.\r\n",
							"    Data is expected to be received via ADS into stage1np/BCE/BI\r\n",
							"    The structure of the folders in stage1np will then be something like:\r\n",
							"    In stage2, everything is written to stage2np/BCE/BI and stage2p/ms_insights\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    def __init__(self, source_folder= 'BCE/BI/SchoolClassification'):\r\n",
							"        BaseOEAModule.__init__(self, source_folder)\r\n",
							"      \r\n",
							"        self.schemas['SchoolClassification'] = [\r\n",
							"                    ['Year' ,   'Integer', 'no-op'],\r\n",
							"                    ['School' ,   'string', 'no-op'],\r\n",
							"                    ['Attendance_Level_%' ,   'float64', 'no-op'],\r\n",
							"                    ['5Y_Enrolment_%' ,   'float64', 'no-op'],\r\n",
							"                    ['Similar_Student_Difference' ,   'float64', 'no-op'],\r\n",
							"                    ['SRS_English_A-C_%_Semester_1' ,   'float64', 'no-op'],\r\n",
							"                    ['SRS_English_A-C_%_Semester_2' ,   'float64', 'no-op'],\r\n",
							"                    ['SRS_Maths_A-C_%_Semester_1' ,   'float64', 'no-op'],\r\n",
							"                    ['SRS_Maths_A-C_%_Semester_2' ,   'float64', 'no-op'],\r\n",
							"                    ['Overall' ,   'Integer  ', 'no-op']                      ,\r\n",
							"                    ['Change_from_PY' ,   'Integer', 'no-op']\r\n",
							"        ]\r\n",
							"\r\n",
							"    def ingest(self, inpdataframe, primary_key):\r\n",
							"\r\n",
							"        \r\n",
							"        \"\"\"  Processes insights data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
							"        logger.info(\"Processing microsoft_insights data from: \" + self.stage1np)\r\n",
							"        \r\n",
							"        items = mssparkutils.fs.ls(self.stage1np)\r\n",
							"        for item in items:\r\n",
							"            if item.name == \"SchoolClassification\":\r\n",
							"                oea.ingest_snapshot_data_frame(inpdataframe, schema=self.schemas['SchoolClassification'], primary_key=primary_key, source_folder= self.source_folder)\r\n",
							"            else:\r\n",
							"                logger.info(\"No defined function for processing this insights data\")\r\n",
							"        \r\n",
							"        logger.info(\"Finished ingesting insights data from stage 1 to stage 2\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/example_modules_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Insights Module"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "31659899-3137-4e68-a40b-a758bc86e7ec"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class ContosoSIS(BaseOEAModule):\r\n",
							"    def __init__(self, oea, source_folder='contoso_sis', pseudonymize = True):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder, pseudonymize)\r\n",
							"        self.schemas['studentattendance'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['school_year', 'integer', 'no-op'],\r\n",
							"                                            ['school_id', 'string', 'no-op'],\r\n",
							"                                            ['attendance_date', 'string', 'no-op'],\r\n",
							"                                            ['all_day', 'string', 'no-op'],\r\n",
							"                                            ['Period', 'short', 'no-op'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['AttendanceCode', 'string', 'no-op'],\r\n",
							"                                            ['PresenceFlag', 'integer', 'no-op'],\r\n",
							"                                            ['attendance_status', 'string', 'no-op'],\r\n",
							"                                            ['attendance_type', 'string', 'no-op'],\r\n",
							"                                            ['attendance_sequence', 'short', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['studentsectionmark'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['school_year', 'string', 'no-op'],\r\n",
							"                                            ['term_id', 'string', 'no-op'],\r\n",
							"                                            ['numeric_grade_earned', 'short', 'no-op'],\r\n",
							"                                            ['alpha_grade_earned', 'string', 'no-op'],\r\n",
							"                                            ['is_final_grade', 'string', 'no-op'],\r\n",
							"                                            ['credits_attempted', 'short', 'no-op'],\r\n",
							"                                            ['credits_earned', 'short', 'no-op'],\r\n",
							"                                            ['grad_credit_type', 'string', 'no-op']]\r\n",
							"                                            \r\n",
							"        self.schemas['students'] = [['SIS ID', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['School SIS ID', 'string', 'no-op'],\r\n",
							"                                            ['Username', 'string', 'no-op'],\r\n",
							"                                            ['Password', 'string', 'no-op'],\r\n",
							"                                            ['First Name', 'string', 'no-op'],\r\n",
							"                                            ['Last Name', 'string', 'no-op'],\r\n",
							"                                            ['Middle Name', 'string', 'no-op'],\r\n",
							"                                            ['Secondary Email', 'string', 'no-op'],\r\n",
							"                                            ['Student Number', 'string', 'no-op'],\r\n",
							"                                            ['Grade', 'string', 'no-op'],\r\n",
							"                                            ['State Id', 'string', 'no-op'],\r\n",
							"                                            ['Status', 'string', 'no-op'],\r\n",
							"                                            ['Birthdate', 'string', 'no-op'],\r\n",
							"                                            ['Graduation Year', 'string', 'no-op'],\r\n",
							"                                            ['Gender', 'string', 'no-op'],\r\n",
							"                                            ['FederalRaceCategory', 'string', 'no-op'],\r\n",
							"                                            ['PrimaryLanguage', 'string', 'no-op'],\r\n",
							"                                            ['ELLStatus', 'string', 'no-op'],\r\n",
							"                                            ['SpecialEducation', 'string', 'no-op'],\r\n",
							"                                            ['LowIncome', 'string', 'no-op'],\r\n",
							"                                            ['CumulativeGPA', 'string', 'no-op']]\r\n",
							"\r\n",
							"    def process_data_from_stage1(self):\r\n",
							"        self._process_entity_from_stage1('studentattendance', 'csv', 'overwrite', 'true')\r\n",
							"        self._process_entity_from_stage1('studentsectionmark', 'csv', 'overwrite', 'true')\r\n",
							"        self._process_entity_from_stage1('students', 'csv', 'overwrite', 'true')\r\n",
							"\r\n",
							"    def copy_test_data_to_stage1(self):\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentattendance.csv', self.stage1np + '/studentattendance/studentattendance.csv', True)\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentsectionmark.csv', self.stage1np + '/studentsectionmark/studentsectionmark.csv', True)\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/students.csv', self.stage1np + '/students/students.csv', True)\r\n",
							"\r\n",
							"class M365(BaseOEAModule):\r\n",
							"    \"\"\"\r\n",
							"    Provides data processing methods for MS Insights data v0.2 format.\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    def __init__(self, oea, source_folder='m365'):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder)\r\n",
							"\r\n",
							"        self.stage1np_activity = self.stage1np + '/DIPData/Activity/ApplicationUsage'\r\n",
							"        self.stage1np_roster = self.stage1np + '/DIPData/Roster'\r\n",
							"\r\n",
							"        self.schemas['Activity0p2'] = [['SignalType', 'string', 'no-op'],\r\n",
							"                                            ['StartTime', 'timestamp', 'no-op'],\r\n",
							"                                            ['UserAgent', 'string', 'no-op'],\r\n",
							"                                            ['SignalId', 'string', 'no-op'],\r\n",
							"                                            ['SISClassId', 'string', 'no-op'],\r\n",
							"                                            ['OfficeClassId', 'string', 'no-op'],\r\n",
							"                                            ['ChannelId', 'string', 'no-op'],\r\n",
							"                                            ['AppName', 'string', 'no-op'],\r\n",
							"                                            ['ActorId', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['ActorRole', 'string', 'no-op'],\r\n",
							"                                            ['SchemaVersion', 'string', 'no-op'],\r\n",
							"                                            ['AssignmentId', 'string', 'no-op'],\r\n",
							"                                            ['SubmissionId', 'string', 'no-op'],\r\n",
							"                                            ['Action', 'string', 'no-op'],\r\n",
							"                                            ['AssginmentDueDate', 'string', 'no-op'],\r\n",
							"                                            ['ClassCreationDate', 'string', 'no-op'],\r\n",
							"                                            ['Grade', 'string', 'no-op'],\r\n",
							"                                            ['SourceFileExtension', 'string', 'no-op'],\r\n",
							"                                            ['MeetingDuration', 'string', 'no-op']]\r\n",
							"        self.schemas['Calendar'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['SchoolYear', 'integer', 'no-op'],\r\n",
							"                                            ['IsCurrent', 'boolean', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op']]\r\n",
							"        self.schemas['Course'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Code', 'string', 'no-op'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['CalendarId', 'string', 'no-op']]\r\n",
							"        self.schemas['Org'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Identifier', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['ParentOrgId', 'string', 'no-op'],\r\n",
							"                                            ['RefOrgTypeId', 'string', 'no-op'],\r\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\r\n",
							"        self.schemas['Person'] = [['Id', 'string', 'hash'],\r\n",
							"                                            ['FirstName', 'string', 'mask'],\r\n",
							"                                            ['MiddleName', 'string', 'mask'],\r\n",
							"                                            ['LastName', 'string', 'mask'],\r\n",
							"                                            ['GenerationCode', 'string', 'no-op'],\r\n",
							"                                            ['Prefix', 'string', 'no-op'],\r\n",
							"                                            ['EnabledUser', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'hash'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\r\n",
							"        self.schemas['PersonIdentifier'] = [['Id', 'string', 'hash'],\r\n",
							"                                            ['Identifier', 'string', 'hash'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['RefIdentifierTypeId', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'hash'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\r\n",
							"        self.schemas['RefDefinition'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['RefType', 'string', 'no-op'],\r\n",
							"                                            ['Namespace', 'string', 'no-op'],\r\n",
							"                                            ['Code', 'string', 'no-op'],\r\n",
							"                                            ['SortOrder', 'integer', 'no-op'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op']]\r\n",
							"        self.schemas['Section'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Code', 'string', 'no-op'],\r\n",
							"                                            ['Location', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['CourseId', 'string', 'no-op'],\r\n",
							"                                            ['RefSectionTypeId', 'string', 'no-op'],\r\n",
							"                                            ['SessionId', 'string', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op']]\r\n",
							"        self.schemas['Session'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['BeginDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['EndDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['CalendarId', 'string', 'no-op'],\r\n",
							"                                            ['ParentSessionId', 'string', 'no-op'],\r\n",
							"                                            ['RefSessionTypeId', 'string', 'no-op']]\r\n",
							"        self.schemas['StaffOrgAffiliation'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['IsPrimary', 'boolean', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefStaffOrgRoleId', 'string', 'no-op']]\r\n",
							"        self.schemas['StaffSectionMembership'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['IsPrimaryStaffForSection', 'boolean', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefStaffSectionRoleId', 'string', 'no-op'],\r\n",
							"                                            ['SectionId', 'string', 'no-op']]\r\n",
							"        self.schemas['StudentOrgAffiliation'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['IsPrimary', 'boolean', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefGradeLevelId', 'string', 'no-op'],\r\n",
							"                                            ['RefStudentOrgRoleId', 'string', 'no-op'],\r\n",
							"                                            ['RefEnrollmentStatusId', 'string', 'no-op']]\r\n",
							"        self.schemas['StudentSectionMembership'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefGradeLevelWhenCourseTakenId', 'string', 'no-op'],\r\n",
							"                                            ['RefStudentSectionRoleId', 'string', 'no-op'],\r\n",
							"                                            ['SectionId', 'string', 'no-op']]\r\n",
							"    \r\n",
							"    def process_activity_data_from_stage1(self):\r\n",
							"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \r\n",
							"            https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\r\n",
							"        \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights activity data from: \" + self.stage1np_activity)\r\n",
							"\r\n",
							"        spark_schema = self.oea.to_spark_schema(self.schemas['Activity0p2'])\r\n",
							"        df = spark.read.csv(self.stage1np_activity + '/*.csv', header='false', schema=spark_schema) \r\n",
							"        sqlContext.registerDataFrameAsTable(df, 'Activity')\r\n",
							"        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(self.oea.stage2np + '/m365/PersonIdentifier'), 'PersonIdentifier')\r\n",
							"        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(self.oea.stage2np + '/m365/RefDefinition'), 'RefDefinition')\r\n",
							"\r\n",
							"        df = spark.sql( \r\n",
							"            \"select act.SignalType, act.StartTime, act.UserAgent, act.SignalId, act.SISClassId, act.OfficeClassId, act.ChannelId, \\\r\n",
							"            act.AppName, act.ActorId, act.ActorRole, act.SchemaVersion, act.AssignmentId, act.SubmissionId, act.Action, act.AssginmentDueDate, \\\r\n",
							"            act.ClassCreationDate, act.Grade, act.SourceFileExtension, act.MeetingDuration, pi.PersonId \\\r\n",
							"            from PersonIdentifier pi, RefDefinition rd, Activity act \\\r\n",
							"            where \\\r\n",
							"                pi.RefIdentifierTypeId = rd.Id \\\r\n",
							"                and rd.RefType = 'RefIdentifierType' \\\r\n",
							"                and rd.Code = 'ActiveDirectoryId' \\\r\n",
							"                and pi.Identifier = act.ActorId\")\r\n",
							"#pi.RefIdentifierTypeId = '7DAF8820-6691-4D61-A210-CE94EA7D3667' \\\r\n",
							"\r\n",
							"\r\n",
							"        df = df.dropDuplicates(['SignalId'])\r\n",
							"        df = df.withColumn('year', F.year(F.col('StartTime'))).withColumn('month', F.month(F.col('StartTime')))\r\n",
							"        df = self.oea.fix_column_names(df)\r\n",
							"        df.write.format('parquet').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/TechActivity')\r\n",
							"\r\n",
							"    def reset_activity_processing(self):\r\n",
							"        \"\"\" Resets all TechActivity processing. This is intended for use during initial testing - use with caution. \"\"\"\r\n",
							"        self.oea.rm_if_exists(self.stage2p + '/TechActivity')\r\n",
							"        self.oea.rm_if_exists(self.stage2np + '/TechActivity')\r\n",
							"        logger.info(f\"Deleted TechActivity from stage2\")  \r\n",
							"\r\n",
							"    def _process_roster_entity(self, path):\r\n",
							"        try:\r\n",
							"            base_path, filename = self.oea.pop_from_path(path)\r\n",
							"            entity = filename[:-4]\r\n",
							"            logger.debug(f\"Processing roster entity: path={path}, entity={entity}\")\r\n",
							"            spark_schema = self.oea.to_spark_schema(self.schemas[entity])\r\n",
							"            df = spark.read.csv(path, header='false', schema=spark_schema)\r\n",
							"            df = self.oea.fix_column_names(df)\r\n",
							"            df.write.format('parquet').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/' + entity)\r\n",
							"\r\n",
							"        except (AnalysisException) as error:\r\n",
							"            logger.exception(str(error))\r\n",
							"\r\n",
							"    def process_roster_data_from_stage1(self):\r\n",
							"        \"\"\" Processes all roster data in stage1 and writes out to stage2 and stage2p \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights roster data from: \" + self.stage1np)\r\n",
							"\r\n",
							"        items = mssparkutils.fs.ls(self.stage1np_roster)\r\n",
							"        #print(items)\r\n",
							"        for item in items:\r\n",
							"            if item.isFile:\r\n",
							"                self._process_roster_entity(item.path)\r\n",
							"\r\n",
							"    def reset_roster_processing(self):\r\n",
							"        \"\"\" Resets all stage1 to stage2 processing of roster data. \"\"\"\r\n",
							"        # cleanup stage2np\r\n",
							"        if self.oea.path_exists(self.stage2np):\r\n",
							"            # Delete roster tables (everything other than TechActivity)\r\n",
							"            items = mssparkutils.fs.ls(self.stage2np)\r\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"            for item in items:\r\n",
							"                if item.name != 'TechActivity':\r\n",
							"                    mssparkutils.fs.rm(item.path, True)\r\n",
							"        # cleanup stage2p\r\n",
							"        if self.oea.path_exists(self.stage2p):\r\n",
							"            # Delete roster tables (everything other than TechActivity)\r\n",
							"            items = mssparkutils.fs.ls(self.stage2p)\r\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"            for item in items:\r\n",
							"                if item.name != 'TechActivity':\r\n",
							"                    mssparkutils.fs.rm(item.path, True)    \r\n",
							"  \r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/hybrid_engagement')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Insights Module"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ded12eb6-3516-4fb8-9ac7-8cc1a60aebc1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Hybrid Student Engagement Notebook\r\n",
							"\r\n",
							"This notebook creates 4 tables (student, dayactivity, yearactivity and calendar) into a new Spark database called s3_hybrid (stage 3 hybrid). \r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Provision storage accounts\r\n",
							"\r\n",
							"The storage account variable has to be changed to the name of the storage account associated with your Azure resource group."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.window import Window\r\n",
							"\r\n",
							"# data lake and container information\r\n",
							"storage_account = 'stoeahybrid'\r\n",
							"use_test_env = True\r\n",
							"\r\n",
							"if use_test_env:\r\n",
							"    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\r\n",
							"    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\r\n",
							"    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\r\n",
							"else:\r\n",
							"    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Load Raw Data from Lake\r\n",
							"To ensure that that the right tables are loaded, confirm that the file paths match your data lake storage containers."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# load needed tables from parquet data lake storage\r\n",
							"dfStudAttendanceRaw = spark.read.format('parquet').load(f'{stage3}/contoso_sis/studentattendance')\r\n",
							"\r\n",
							"#still most are m365 \r\n",
							"dfActivityV2Raw = spark.read.format('parquet').load(f'{stage3}/m365/Activity0p2')\r\n",
							"dfPersonRaw = spark.read.format('parquet').load(f'{stage3}/m365/Person')\r\n",
							"dfStudOrgRaw = spark.read.format('parquet').load(f'{stage3}/m365/StudentOrgAffiliation')\r\n",
							"dfOrgRaw = spark.read.format('parquet').load(f'{stage3}/m365/Org')\r\n",
							"dfSectionRaw = spark.read.format('parquet').load(f'{stage3}/m365/Section')\r\n",
							"dfCourseRaw = spark.read.format('parquet').load(f'{stage3}/m365/Course')\r\n",
							"dfRefRaw = spark.read.format('parquet').load(f'{stage3}/m365/RefDefinition')"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 1. Student table\r\n",
							"Contains students' information at a school level\r\n",
							"\r\n",
							"**Databases and tables used:**\r\n",
							"\r\n",
							"1. Spark DB: s3_m365 (stage 3 m365 feed)\r\n",
							"- Table: person (student PersonId and ExternalId relationship)\r\n",
							"- Table: org\r\n",
							"- Table: studentorgaffiliation\r\n",
							"- Table: section\r\n",
							"- Table: course\r\n",
							"- Table: refdefinition\r\n",
							"\r\n",
							"2. Spark DB: stage 3 SIS data\r\n",
							"- Table: studentattendance (student attendance by date, school, and course section)\r\n",
							"\r\n",
							"**Databases and tables created:**\r\n",
							"\r\n",
							"1. Spark DB: s3_hybrid (stage 3 hybrid)\r\n",
							"- Table: student"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Clean and Subset Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# take only active students\r\n",
							"dfPerson = dfPersonRaw.filter(dfPersonRaw.IsActive == 'True')\r\n",
							"dfStudOrg = dfStudOrgRaw.filter(dfStudOrgRaw.IsActive == 'True')\r\n",
							"\r\n",
							"# take needed columns and rename to align with other data sources\r\n",
							"dfPerson = dfPerson.select('Id','ExternalId')\r\n",
							"dfPerson = dfPerson.withColumnRenamed('Id', 'PersonId')\r\n",
							"\r\n",
							"dfStudOrg = dfStudOrg.select('PersonId', 'IsPrimary', 'IsActive', 'OrgId', 'RefGradeLevelId')\r\n",
							"\r\n",
							"dfOrg = dfOrgRaw.select('Identifier', 'name', 'Id')\r\n",
							"dfOrg = dfOrg.withColumnRenamed('Identifier', 'School_ID')\r\n",
							"dfOrg = dfOrg.withColumnRenamed('name', 'School_Name')\r\n",
							"dfOrg = dfOrg.withColumnRenamed('Id', 'OrgId')\r\n",
							"\r\n",
							"dfSection = dfSectionRaw.select('ExternalId', 'Name', 'CourseId', 'Id', 'Code', 'SessionId', 'OrgId')\r\n",
							"dfSection = dfSection.withColumnRenamed('ExternalId', 'section_id')\r\n",
							"dfSection = dfSection.withColumnRenamed('name', 'section_name')\r\n",
							"dfSection = dfSection.drop('School_Name')\r\n",
							"dfSection = dfSection.join(dfOrg, 'OrgId')\r\n",
							"\r\n",
							"dfCourse = dfCourseRaw.withColumnRenamed('Id', 'CourseId')\r\n",
							"dfCourse = dfCourse.withColumnRenamed('Name', 'Course')\r\n",
							"dfCourse = dfCourse.drop('ExternalId')\r\n",
							"\r\n",
							"dfRef = dfRefRaw.select('Id', 'Code', 'Description')\r\n",
							"dfRef = dfRef.withColumnRenamed('Id', 'RefId')\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# combine student information and school details with attendance data\r\n",
							"dfStudAttendance = dfStudAttendanceRaw.select('student_id', 'school_id', 'attendance_date', 'Period', 'section_id', \r\n",
							"                        'PresenceFlag', 'attendance_status')\r\n",
							"                  \r\n",
							"dfStudAttendance = dfStudAttendance.withColumnRenamed('school_id', 'School_ID')\r\n",
							"dfStudAttendance = dfStudAttendance.withColumnRenamed('student_id','ExternalId')\r\n",
							"dfStudAttendance = dfStudAttendance.withColumn(\"Date\", to_date(col(\"attendance_date\"), 'yyyy-MM-dd'))\r\n",
							"dfStudAttendance = dfStudAttendance.drop('attendance_date')\r\n",
							"\r\n",
							"dfStudAttendance = dfStudAttendance.join(dfOrg, 'School_ID')\r\n",
							"dfStudAttendance.show(1,vertical=True)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Find Primary School\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# find school which students have highest attendance count\r\n",
							"df = (dfStudAttendance.groupBy(\"ExternalId\", 'School_ID', 'School_Name')\r\n",
							"    .agg(sum(\"PresenceFlag\").alias(\"Present_Count\")))\r\n",
							"\r\n",
							"\r\n",
							"w = Window.partitionBy('ExternalId')\r\n",
							"dfStudSchoolPrimary = df.withColumn('maxPres', max('Present_Count').over(w))\\\r\n",
							"    .where(col('Present_Count') == col('maxPres'))\\\r\n",
							"    .drop('maxPres').drop('Present_Count')\r\n",
							"\r\n",
							"dfStudSchoolPrimary.show(3, vertical=True)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# rename columns to indicate primary school\r\n",
							"dfStudSchoolPrimary = dfStudSchoolPrimary.withColumnRenamed('School_Name', 'SchoolNamePrimary')\r\n",
							"dfStudSchoolPrimary = dfStudSchoolPrimary.withColumnRenamed('School_ID', 'SchoolIdPrimary')"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Combine tables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# join person table to student and ref tables to create student profile \r\n",
							"dfStudent = dfPerson.join(dfStudOrg, 'PersonId')\r\n",
							"dfStudent = dfStudent.withColumnRenamed('RefGradeLevelId', 'RefId')\r\n",
							"dfStudent = dfStudent.join(dfRef, 'RefId')\r\n",
							"dfStudentFinal = dfStudent.join(dfStudSchoolPrimary, 'ExternalId')\r\n",
							"dfStudentFinal = dfStudentFinal.withColumnRenamed('Code', 'GradeLevel')\r\n",
							"dfStudentFinal = dfStudentFinal.withColumnRenamed('Description', 'GradeName')\r\n",
							"dfStudentFinal = dfStudentFinal.select('ExternalId',  'PersonId', 'IsActive', 'SchoolNamePrimary', 'SchoolIdPrimary', 'OrgId', 'GradeLevel', 'GradeName')\r\n",
							"dfStudentFinal = dfStudentFinal.drop('OrgId', 'GradeLevel')\r\n",
							"dfStudentFinal.show(1, vertical=True)\r\n",
							"print(dfStudentFinal.count())"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Write Data Back to Lake"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# write back to the lake\r\n",
							"dfStudentFinal.write.format('parquet').mode('overwrite').save(stage3 + '/test_s3_hybrid/Student')"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Load to Spark DB"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\r\n",
							"# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\r\n",
							"# This also makes it possible to connect in Power BI via the azure sql data source connector.\r\n",
							"def create_spark_db(db_name, source_path):\r\n",
							"    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
							"    spark.sql(f\"DROP TABLE IF EXISTS {db_name}.Student\")\r\n",
							"    spark.sql(f\"create table if not exists {db_name}.Student using PARQUET location '{source_path}/Student'\")\r\n",
							"    \r\n",
							"create_spark_db('test_s3_hybrid', stage3 + '/test_s3_hybrid')"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 2. Calendar table\r\n",
							"Contains a basic calendar table to support data analysis in a Power BI dashboard.\r\n",
							"\r\n",
							"**Databases and tables used:**\r\n",
							"- None\r\n",
							"\r\n",
							"**Databases and tables created:**\r\n",
							"\r\n",
							"1. Spark DB: s3_hybrid (stage 3 hybrid)\r\n",
							"- Table: calendar"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# date range\r\n",
							"start = \"2020-01-01\"\r\n",
							"stop = \"2021-12-30\"\r\n",
							"\r\n",
							"# create calendar dataframe\r\n",
							"temp_df = spark.createDataFrame([(start, stop)], (\"start\", \"stop\"))\r\n",
							"temp_df = temp_df.select([col(c).cast(\"timestamp\") for c in (\"start\", \"stop\")])\r\n",
							"temp_df = temp_df.withColumn(\"stop\",date_add(\"stop\",1).cast(\"timestamp\"))\r\n",
							"temp_df = temp_df.select([col(c).cast(\"long\") for c in (\"start\", \"stop\")])\r\n",
							"start, stop = temp_df.first()\r\n",
							"interval=60*60*24\r\n",
							"\r\n",
							"df = spark.range(start,stop,interval).select(col(\"id\").cast(\"timestamp\").alias(\"DateTime\"))\r\n",
							"df = df.withColumn(\"Date\", to_date(col(\"DateTime\")))\r\n",
							"\r\n",
							"df = df.drop(\"DateTime\")\r\n",
							"df = df.withColumn('Year', date_format('Date', 'YYYY'))\r\n",
							"df = df.withColumn('Month', date_format('Date', 'MMMM'))\r\n",
							"df = df.withColumn('MonthNum', date_format('Date', 'M'))\r\n",
							"df = df.withColumn('Week', date_format('Date', 'W'))\r\n",
							"df = df.withColumn('Day', date_format('Date', 'D'))\r\n",
							"df.show(2)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Write Data Back to Lake"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# write back to the lake in stage 3 ds3_main directory\r\n",
							"df.write.format('parquet').mode('overwrite').save(stage3 + '/test_s3_hybrid/Calendar')"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Load to Spark DB"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Create spark db to allow for access to the data in the data lake via SQL on-demand.\r\n",
							"#This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\r\n",
							"# This also makes it possible to connect in Power BI via the azure sql data source connector.\r\n",
							"def create_spark_db(db_name, source_path):\r\n",
							"    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
							"    spark.sql(f\"DROP TABLE IF EXISTS {db_name}.Calendar\")\r\n",
							"    spark.sql(f\"create table if not exists {db_name}.Calendar using PARQUET location '{source_path}/Calendar'\")\r\n",
							"\r\n",
							"create_spark_db('test_s3_hybrid', stage3 + '/test_s3_hybrid')"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 3. Dayactivity table\r\n",
							"Contains student daily digital and in-person activity \r\n",
							"\r\n",
							"**Databases and tables used:** \r\n",
							"\r\n",
							"1. Spark DB: s3_m365 (stage 3 m365 feed)\r\n",
							"- Table: Activity0p2 (user m365 app activity v2)\r\n",
							"\r\n",
							"2. Spark DB: stage 3 SIS data\r\n",
							"- Table: studentattendance (student attendance by date, school, and course section)"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Clean and Subset data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# combine student attendance with section and course data\r\n",
							"dfStudAttendanceFinal = dfStudAttendance.join(dfStudentFinal, 'ExternalId')\r\n",
							"dfStudAttendanceFinal = dfStudAttendanceFinal.join(dfSection, ['section_id', 'School_ID'])\r\n",
							"dfStudAttendanceFinal = dfStudAttendanceFinal.join(dfCourse, 'CourseId')\r\n",
							"dfStudAttendanceFinal = dfStudAttendanceFinal.select('PersonId', 'ExternalId', 'Date', 'attendance_status', 'PresenceFlag'\r\n",
							"                                                    ,'Period', 'SchoolNamePrimary', 'SessionId', 'section_id', 'CourseId', 'Course')\r\n",
							"dfStudAttendanceFinal = dfStudAttendanceFinal.withColumnRenamed('section_id', 'SectionId')\r\n",
							"dfStudAttendanceFinal.show(1, vertical=True)\r\n",
							"print(dfStudAttendanceFinal.count())"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# isolate in-person school days\r\n",
							"schoolDays = dfStudAttendanceFinal.select('Date').distinct()\r\n",
							"print(schoolDays.count())"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# take only needed columns and filter for only students\r\n",
							"dfActivityV2 = dfActivityV2Raw.where(col('ActorRole') == 'Student').select('PersonId','AppName', 'SignalType'\r\n",
							"                                    ,'StartTime', 'MeetingDuration')\r\n",
							"\r\n",
							"# active students only, include external id\r\n",
							"dfActivityV2 = dfActivityV2.join(dfStudentFinal, 'PersonId')\r\n",
							"dfActivityV2.show(1, vertical=True)\r\n",
							"print(dfActivityV2.count())"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Aggregate Activity by Date\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# convert MeetingDuration column to time format\r\n",
							"dfActivityV2 = dfActivityV2.withColumn('MeetingDuration', to_timestamp('MeetingDuration'))\r\n",
							"dfActivityV2 = dfActivityV2.withColumn('HourToMinutes', hour(col('MeetingDuration'))*60)\r\n",
							"dfActivityV2 = dfActivityV2.withColumn('Minutes', minute(col('MeetingDuration')))\r\n",
							"dfActivityV2 = dfActivityV2.withColumn('Duration', col('HourToMinutes') + col('Minutes'))\r\n",
							"\r\n",
							"# aggregation of activity\r\n",
							"dfActivityV2agg = (dfActivityV2.groupBy(\"PersonId\", \"ExternalId\",\r\n",
							"                    \"AppName\", \"SignalType\",\r\n",
							"                    to_date(\"StartTime\").alias(\"Date\"))\r\n",
							"    .agg(sum(\"Duration\").alias(\"DurationSum\")))\r\n",
							"\r\n",
							"dfActivityV2agg.show(1,vertical=True)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# count number of presence flags per date\r\n",
							"dfStudAttendanceAgg = (dfStudAttendanceFinal.groupBy('PersonId', 'ExternalId', 'Date', 'SectionId', 'Course')\r\n",
							"                    .agg(mean(\"PresenceFlag\").alias(\"Present_Mean\")))\r\n",
							"\r\n",
							"dfStudAttendanceAgg = dfStudAttendanceAgg.withColumn('Present', when(col('Present_Mean') > 0, 1).otherwise(0))\r\n",
							"\r\n",
							"print(dfStudAttendanceAgg.count())\r\n",
							"dfStudAttendanceAgg.show(1,vertical=True)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Daily activity: Merge into a single wide table\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# focus on teams meetings, assignments, and communications\r\n",
							"\r\n",
							"df1 = dfActivityV2agg.where( ( col(\"AppName\") == \"Teams\" ) &\r\n",
							"                            ( col(\"SignalType\") == \"CallRecordSummarized\" ))\r\n",
							"\r\n",
							"dfDayAct = df1.select(\"PersonId\", \"ExternalId\", \"Date\", \"DurationSum\")\r\n",
							"dfDayAct = dfDayAct.withColumnRenamed(\"DurationSum\", \"TeamsMeetingsDuration\")\r\n",
							"\r\n",
							"df2 = dfActivityV2agg.withColumn('Assignments', when(( col(\"AppName\") == \"Assignments\" ) & \r\n",
							"                            ( col(\"SignalType\") == \"SubmissionEvent\" ) , 1).otherwise(0))\r\n",
							"df2 = df2.select(\"PersonId\", \"ExternalId\", \"Date\", \"Assignments\")\r\n",
							"\r\n",
							"dfDayAct = dfDayAct.join(df2, [\"PersonId\", \"ExternalId\", \"Date\"], 'outer')\r\n",
							"\r\n",
							"df3 = dfActivityV2agg.withColumn('TeamsCommunications', when(( col(\"AppName\") == \"Teams\" ) & \r\n",
							"                        ( col(\"SignalType\").isin(['AddedToSharedWithMe', 'CommentCreated',\r\n",
							"                            'CommentDeleted', 'ExpandChannelMessage', 'PostChannelMessage',\r\n",
							"                            'ReactedWithEmoji', 'ReplyChannelMessage', 'Unlike',\r\n",
							"                            'UserAtMentioned', 'VisitTeamChannel'])), 1).otherwise(0))\r\n",
							"df3 = df3.select(\"PersonId\", \"ExternalId\", \"Date\", \"TeamsCommunications\")\r\n",
							"\r\n",
							"dfDayAct = dfDayAct.join(df3, [\"PersonId\", \"ExternalId\", \"Date\"], 'outer')\r\n",
							"\r\n",
							"\r\n",
							"dfDayAct.show(5, vertical=True)\r\n",
							"\r\n",
							"dfDayAct = dfDayAct.groupBy(\"PersonId\", \"ExternalId\", \"Date\").agg(sum('TeamsMeetingsDuration').alias('TeamsMeetingsDuration'), mean('Assignments').alias('Assignments'), mean('TeamsCommunications').alias('TeamsCommunications'))\r\n",
							" \r\n",
							"dfDayAct.show(5, vertical=True)\r\n",
							"print(dfDayAct.count())"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# add in person attendance\r\n",
							"dfDayAct = dfDayAct.join(dfStudAttendanceAgg, [\"PersonId\", \"ExternalId\", \"Date\"], 'outer')\r\n",
							"\r\n",
							"dfDayAct.show(2,vertical=True)\r\n",
							"dfDayAct.count()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# subset dates to only school dates\r\n",
							"dfDayAct = dfDayAct.join(schoolDays, [\"Date\"], \"inner\")\r\n",
							"\r\n",
							"print(dfDayAct.count())\r\n",
							"\r\n",
							"schoolDaysCheck = dfDayAct.select('Date').distinct()\r\n",
							"\r\n",
							"print(schoolDaysCheck.count())"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"print(dfDayAct.dtypes)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# fill missing with 0\r\n",
							"dfDayAct = dfDayAct.na.fill(0)\r\n",
							"\r\n",
							"print(dfDayAct.count())\r\n",
							"dfDayAct.show(1, vertical=True)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# add activity indicator columns\r\n",
							"dfDayAct= dfDayAct.withColumn('ActiveTeamsMeetings', when(col('TeamsMeetingsDuration') > 0, 1).otherwise(0))\r\n",
							"dfDayAct= dfDayAct.withColumn('ActiveTeamsCommunications', when(col('TeamsCommunications') > 0 , 1).otherwise(0))\r\n",
							"dfDayAct= dfDayAct.withColumn('ActiveAssignments', when(col('Assignments') > 0 , 1).otherwise(0))\r\n",
							"dfDayAct= dfDayAct.withColumn('DigitallyActive', when( \\\r\n",
							"                (col('ActiveTeamsMeetings')+ col('ActiveTeamsCommunications')+ col('ActiveAssignments'))\\\r\n",
							"                 > 0, 1).otherwise(0))\r\n",
							"\r\n",
							"dfDayAct.show(1,vertical=True)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 3. Dayactivity table\r\n",
							"Contains student daily digital and in-person activity \r\n",
							"\r\n",
							"**Databases and tables used:** \r\n",
							"\r\n",
							"1. Spark DB: s3_m365 (stage 3 m365 feed)\r\n",
							"- Table: Activity0p2 (user m365 app activity v2)\r\n",
							"\r\n",
							"2. Spark DB: stage 3 SIS data\r\n",
							"- Table: studentattendance (student attendance by date, school, and course section)"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Yearly Aggregates\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"dfYearAct = dfDayAct.groupBy(\"PersonId\", \"ExternalId\")\\\r\n",
							"    .agg(sum(\"ActiveTeamsMeetings\").alias(\"DaysActiveTeamsMeetings\")\\\r\n",
							"    ,sum(\"ActiveTeamsCommunications\").alias(\"DaysActiveTeamsCommunications\")\\\r\n",
							"    ,sum(\"ActiveAssignments\").alias(\"DaysActiveAssignments\")\\\r\n",
							"    ,sum(\"DigitallyActive\").alias(\"DaysDigitallyActive\")\\\r\n",
							"    ,sum(\"Present\").alias(\"DaysPresent\"))\r\n",
							"\r\n",
							"\r\n",
							"dfYearAct = dfYearAct.withColumn('Present_Perc', \r\n",
							"            col(\"DaysPresent\")/schoolDays.count())\r\n",
							"\r\n",
							"dfYearAct = dfYearAct.withColumn('Atten_Threshold_Met', \r\n",
							"            when(col('Present_Perc') > 0.9, 1).otherwise(0))\r\n",
							"\r\n",
							"print(dfYearAct.count())\r\n",
							"dfYearAct.show(5,vertical=True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Write Back to the Lake"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# write back to the lake\r\n",
							"dfDayAct.write.format('parquet').mode('overwrite').save(stage3 + '/test_s3_hybrid/dayActivity')\r\n",
							"dfYearAct.write.format('parquet').mode('overwrite').save(stage3 + '/test_s3_hybrid/yearActivity')"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load to Spark DB"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\r\n",
							"# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\r\n",
							"# This also makes it possible to connect in Power BI via the azure sql data source connector.\r\n",
							"def create_spark_db(db_name, source_path):\r\n",
							"    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
							"    spark.sql(f\"DROP TABLE IF EXISTS {db_name}.dayActivity\")\r\n",
							"    spark.sql(f\"DROP TABLE IF EXISTS {db_name}.yearActivity\")\r\n",
							"    spark.sql(f\"create table if not exists {db_name}.dayActivity using PARQUET location '{source_path}/dayActivity'\")\r\n",
							"    spark.sql(f\"create table if not exists {db_name}.yearActivity using PARQUET location '{source_path}/yearActivity'\")\r\n",
							"    \r\n",
							"create_spark_db('test_s3_hybrid', stage3 + '/test_s3_hybrid')"
						],
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3p1sm",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8e6ffb27-0ace-49de-84aa-db851a656d21"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
						"name": "spark3p1sm",
						"type": "Spark",
						"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Test Class"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source_folder= 'SDS/M365'\r\n",
							"storage_account = 'bceaaeoeadevlrs' "
						],
						"outputs": [],
						"execution_count": 155
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.window import Window\r\n",
							"\r\n",
							"# data lake and container information\r\n",
							"\r\n",
							"use_test_env = False\r\n",
							"\r\n",
							"if use_test_env:\r\n",
							"    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net/stage1'\r\n",
							"    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/'\r\n",
							"    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/'\r\n",
							"else:\r\n",
							"    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 156
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run OEA_py"
						],
						"outputs": [],
						"execution_count": 157
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class test(BaseOEAModule):\r\n",
							"    \"\"\"    Provides data processing methods for MS Insights data.\r\n",
							"    Data is expected to be received via ADS into stage1np/ms_insights\r\n",
							"    The structure of the folders in stage1np will then be something like:\r\n",
							"        -> stage1np/example/test/*\r\n",
							"\r\n",
							"    In stage2, everything is written to stage2np/test and stage2p/test\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    def __init__(self, source_folder= 'example/test'):\r\n",
							"        BaseOEAModule.__init__(self, source_folder)\r\n",
							"        self.stage1np_activity = self.stage1np + '/'\r\n",
							"        self.schemas['test'] = [['col', 'Integer', 'no-op']\r\n",
							"                    ]\r\n",
							"    \r\n",
							"    def ingest(self):\r\n",
							"        \"\"\"  Processes insights data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
							"        logger.info(\"Processing data from: \" + self.stage1np)\r\n",
							"        \r\n",
							"        items = mssparkutils.fs.ls(self.stage1np)\r\n",
							"        for item in items:\r\n",
							"            if item.name.__contains__(\"csv\"):\r\n",
							"                self.process_stage1_data()\r\n",
							"\r\n",
							"            else:\r\n",
							"                logger.info(\"No defined function for processing this insights data\")\r\n",
							"        \r\n",
							"        logger.info(\"Finished ingesting data from stage 1 to stage 2\")\r\n",
							"\r\n",
							"    def process_stage1_data(self):\r\n",
							"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
							"        logger.info(\"Processing test data from: \" + self.stage1np_activity)\r\n",
							"\r\n",
							"        # Currently not using the OEA ingest_incremental_data function due to pulling out the partition folders\r\n",
							"        activity_spark_schema = oea.to_spark_schema(self.schemas['test'])\r\n",
							"        df = spark.readStream.csv(self.stage1np_activity + '*.csv', header='false', schema=activity_spark_schema)\r\n",
							"        df = df.dropDuplicates(['col'])\r\n",
							"        \r\n",
							"       \r\n",
							"        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['test'])\r\n",
							"\r\n",
							"        if len(df_pseudo.columns) == 0:\r\n",
							"            logger.info('No data to be written to stage2p')\r\n",
							"        else:\r\n",
							"            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_activity + '/_checkpoints_p')\r\n",
							"            query = query.start(self.stage2p + '')\r\n",
							"            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
							"            logger.info(query.lastProgress)\r\n",
							"        \r\n",
							"        if len(df_lookup.columns) == 0:\r\n",
							"            logger.info('No data to be written to stage2np')\r\n",
							"        else:\r\n",
							"            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_activity + '/_checkpoints_np')\r\n",
							"            query2 = query2.start(self.stage2np + '')\r\n",
							"            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
							"            logger.info(query2.lastProgress)   "
						],
						"outputs": [],
						"execution_count": 158
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea= OEA()"
						],
						"outputs": [],
						"execution_count": 159
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /Insights_py"
						],
						"outputs": [],
						"execution_count": 160
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test0= test()\r\n",
							"\r\n",
							"test= Insights()"
						],
						"outputs": [],
						"execution_count": 161
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df= test.returnDataFrameBeforeProcessing('TechActivity')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 162
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df.select('signalType', 'year', 'month')"
						],
						"outputs": [],
						"execution_count": 163
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"test.stage2p"
						],
						"outputs": [],
						"execution_count": 164
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test.stage2np = stage2\r\n",
							"#test.stage2p = stage2\r\n",
							"#oea.stage2np = stage2\r\n",
							"#oea.stage2p = stage2\r\n",
							"\r\n",
							"oea.stage2p "
						],
						"outputs": [],
						"execution_count": 165
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"test.stage2p"
						],
						"outputs": [],
						"execution_count": 166
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source_path=   oea.stage1np +'/'+ source_folder +'/activity'\r\n",
							"\r\n",
							"source_path"
						],
						"outputs": [],
						"execution_count": 150
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark_schema = oea.to_spark_schema(test.schemas['TechActivity'])"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#query = df.writeStream.format(\"delta\").outputMode(\"append\") \\\r\n",
							"#                                .option(\"checkpointLocation\", test.stage2p + '/_checkpoints/TechActivity_pseudo/') \\\r\n",
							"#                                .option(\"startingOffsets\", \"earliest\") \\\r\n",
							"#                                .option(\"truncate\", False).partitionBy('year')  \r\n",
							"#query = query.start(test.stage2p + '/TechActivity')\r\n",
							"#query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
							"#logger.info(query.lastProgress)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source= test.stage1np_activity +'/*/*.csv'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df1 = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"csv\").option(\"header\", \"false\").option( \"schema\",spark_schema).load(source)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"csv\") \\\r\n",
							"  # The schema location directory keeps track of your data schema over time\r\n",
							" # .option(\"cloudFiles.schemaLocation\", \"<path-to-checkpoint>\").load(\"<path-to-source-data>\") \\\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Queries with streaming sources must be executed with writeStream.start();\r\n",
							"#display(df1)\r\n",
							"#df1.show()\r\n",
							"spark.catalog.listDatabases()\r\n",
							"\r\n",
							"#sqlContext.registerDataFrameAsTable(df, 'Activity')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.catalog.setCurrentDatabase('oea')"
						],
						"outputs": [],
						"execution_count": 151
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sqlContext.registerDataFrameAsTable(df, 'Activity')"
						],
						"outputs": [],
						"execution_count": 134
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#oea.ingest_incremental_data( source_system=source_folder, tablename='Activity', schema=spark_schema, partition_by='year', primary_key='SignalId', data_format='csv', has_header=False)\r\n",
							"        "
						],
						"outputs": [],
						"execution_count": 135
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dfquery = spark.sql(\"select * from Activity where year = 2022 and SignalType = 'Call'\")"
						],
						"outputs": [],
						"execution_count": 136
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#dfquery.show()"
						],
						"outputs": [],
						"execution_count": 137
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"test.delete_stage2()"
						],
						"outputs": [],
						"execution_count": 152
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test.ingest()\r\n",
							"\r\n",
							"test.process_insights_activity_stage1_data()"
						],
						"outputs": [],
						"execution_count": 167
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test.delete_stage2()"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test.process_activity()"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"db_name= 'bceoea'\r\n",
							"\r\n",
							"\r\n",
							"pathp  = test.stage2p\r\n",
							"pathnp = test.stage2np\r\n",
							"source_format= 'DELTA'\r\n",
							"\r\n",
							"print(pathp, pathnp)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.create_sql_views(pathp, source_format)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 71
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print (test.stage1np_activity)\r\n",
							"\r\n",
							"oea.get_folders(test.stage1np_activity)\r\n",
							"\r\n",
							"mssparkutils.fs.ls(test.stage1np)\r\n",
							"#pathnp = classSchool.stage2n"
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark3p1sm')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 15,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "opencensus-ext-azure >= 1.0.8\r\nFaker >= 8.0",
					"filename": "/usr/csuser/clouddrive/OpenEduAnalytics/framework/requirements.txt",
					"time": "2022-07-14T06:54:48.8041552Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "australiaeast"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark8pmd')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"libraryRequirements": {
					"content": "opencensus-ext-azure >= 1.0.8\r\nFaker >= 8.0",
					"filename": "_usr_csuser_clouddrive_OpenEduAnalytics_framework_requirements.txt",
					"time": "2022-07-28T08:51:10.1620213Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "australiaeast"
		}
	]
}