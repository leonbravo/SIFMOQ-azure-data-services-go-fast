{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "TaskObject = \"{\\\"TaskInstanceId\\\":53,\\\"TaskMasterId\\\":1,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"Execute Synapse Notebook\\\",\\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"dc872650-b992-4cae-9ae2-c714c95563ee\\\",\\\"NumberOfRetries\\\":2,\\\"DegreeOfCopyParallelism\\\":1,\\\"KeyVaultBaseUrl\\\":\\\"https://ads-dev-kv-ads-tad5.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\",\\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-4,\\\"SystemServer\\\":\\\"https://adsdevdlsadstad5adsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakeraw\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"/samples/sif/\\\",\\\"TargetRelativePath\\\":\\\"/samples/sif/StudentPersonal/\\\"},\\\"DataFileName\\\":\\\"StudentPersonal.json\\\",\\\"RelativePath\\\":\\\"/samples/sif/\\\",\\\"SchemaFileName\\\":\\\"\\\",\\\"Type\\\":\\\"Notebook-Optional\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\"},\\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-4,\\\"SystemServer\\\":\\\"https://adsdevdlsadstad5adsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakeraw\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"/samples/sif/\\\",\\\"TargetRelativePath\\\":\\\"/samples/sif/StudentPersonal/\\\"},\\\"DataFileName\\\":\\\"StudentPersonal.parquet\\\",\\\"RelativePath\\\":\\\"/samples/sif/StudentPersonal/\\\",\\\"SchemaFileName\\\":\\\"\\\",\\\"Type\\\":\\\"Notebook-Optional\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\"},\\\"TMOptionals\\\":{\\\"CustomDefinitions\\\":\\\"\\\",\\\"ExecuteNotebook\\\":\\\"SIFParameterizedJson\\\",\\\"Purview\\\":\\\"Disabled\\\",\\\"QualifiedIDAssociation\\\":\\\"TaskMasterId\\\",\\\"UseNotebookActivity\\\":\\\"Enabled\\\"}}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "TaskObject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import random\r\n",
        "import json\r\n",
        "from pyspark.sql import Row\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "session_id = random.randint(0,1000000)\r\n",
        "#invalid source\r\n",
        "##TaskObject = \"{\\\"TaskInstanceId\\\":1,\\\"TaskMasterId\\\":2,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"TestTask Type Name\\\",\\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"8448eabb-9ba4-4779-865b-29e973431273\\\",\\\"NumberOfRetries\\\":0,\\\"DegreeOfCopyParallelism\\\":1,\\\"KeyVaultBaseUrl\\\":\\\"https://ark-stg-kv-ads-irud.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\",\\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"ExecutionEngine\\\":{\\\"EngineId\\\":-1,\\\"EngineName\\\":\\\"ark-stg-adf-ads-irud\\\",\\\"SystemType\\\":\\\"Datafactory\\\",\\\"ResourceGroup\\\":\\\"dlzdev04\\\",\\\"SubscriptionId\\\":\\\"ed1206e0-17c7-4bc2-ad4b-f8d4dab9284f\\\",\\\"ADFPipeline\\\":\\\"GPL_AzureSqlTable_NA_AzureBlobFS_Parquet_Azure\\\",\\\"EngineJson\\\":\\\"{}\\\",\\\"TaskDatafactoryIR\\\":\\\"Azure\\\",\\\"JsonProperties\\\":{}},\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://arkstgdlsadsirudadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakelanding\\\"},\\\"Instance\\\":{\\\"TargetRelativePath\\\":\\\"\\\"},\\\"DataFileName\\\":\\\"TestFile.parquet\\\",\\\"RelativePath\\\":\\\"\\\",\\\"SchemaFileName\\\":\\\"TestFile.json\\\"},\\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://arkstgdlsadsirudadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakelanding\\\"},\\\"Instance\\\":{\\\"TargetRelativePath\\\":\\\"\\\"},\\\"DataFileName\\\":\\\"TestFile.parquet\\\",\\\"RelativePath\\\":\\\"\\\",\\\"SchemaFileName\\\":\\\"TestFile.json\\\",\\\"Type\\\":\\\"Parquet\\\"}}\"\r\n",
        "#valid source\r\n",
        "#TaskObject = \"{\\\"TaskInstanceId\\\":1,\\\"TaskMasterId\\\":2,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"TestTask Type Name\\\", \\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"8448eabb-9ba4-4779-865b-29e973431273\\\",\\\"NumberOfRetries\\\":0,\\\"DegreeOfCopyParallelism\\\":1, \\\"KeyVaultBaseUrl\\\":\\\"https://ads-dev-kv-ads-ic038069.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\", \\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"ExecutionEngine\\\":{\\\"EngineId\\\":-1,\\\"EngineName\\\":\\\"ads-dev-kv-ads-ic038069\\\", \\\"SystemType\\\":\\\"Microsoft.Synapse/workspaces\\\",\\\"ResourceGroup\\\":\\\"sifgofast\\\",\\\"SubscriptionId\\\":\\\"cd486ba9-eef3-466d-b16c-7f1b2941ae9d\\\", \\\"ADFPipeline\\\":\\\"GPL_AzureSqlTable_NA_AzureBlobFS_Parquet_Azure\\\",\\\"EngineJson\\\":\\\"{}\\\",\\\"TaskDatafactoryIR\\\":\\\"Azure\\\", \\\"JsonProperties\\\":{}},\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://adsdevdlsadsic03adsl.blob.core.windows.net\\\", \\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"adsdevdlsadsic03\\\"},\\\"Instance\\\":\\\"\\\",{\\\"TargetRelativePath\\\":\\\"synapse/sif\\\"}, \\\"DataFileName\\\":\\\"StudentPersonal.parquet\\\",\\\"SourceRelativePath\\\":\\\"synapse/sif\\\",\\\"SchemaFileName\\\":\\\"StudentPersonal.json\\\",\\\"Type\\\":\\\"Parquet\\\"}, \\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://adsdevdlsadsic03adsl.blob.core.windows.net\\\", \\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ABS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"adsdevdlsadsic03\\\"}, \\\"Instance\\\":{\\\"TargetRelativePath\\\":\\\"\\\"},\\\"DataFileName\\\":\\\"StudentPersonal.parquet\\\",\\\"SourceRelativePath\\\":\\\"synapse\\/sif\\\", \\\"SchemaFileName\\\":\\\"StudentPersonal.json\\\",\\\"Type\\\":\\\"Parquet\\\"}}\"\r\n",
        "TaskDict = {}\r\n",
        "OutputDict = {}\r\n",
        "TaskObjectJson = json.loads(TaskObject)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "##we want to delete EngineJson as it causes issues when converting back to a json and it is not needed as its properties are within JsonProperties as children\r\n",
        "try:\r\n",
        "    del TaskObjectJson['ExecutionEngine']['EngineJson']\r\n",
        "except:\r\n",
        "    print(\"No EngineJson Found\")\r\n",
        "\r\n",
        "Source = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\").replace(\"blob\",\"dfs\") + \"/\"\r\n",
        "Source = Source.replace('///', '/')\r\n",
        "Source = Source.replace('//', '/')\r\n",
        "print(Source)\r\n",
        "Source = Source + TaskObjectJson['Source'][\"Instance\"]['SourceRelativePath']\r\n",
        "print(Source)\r\n",
        "Source = Source  +\"/\" + TaskObjectJson['Source']['DataFileName'] \r\n",
        "print(Source)\r\n",
        "Source = Source.replace('///', '/')\r\n",
        "Source = Source.replace('//', '/')\r\n",
        "print(Source)\r\n",
        "Source = \"abfss://\" + Source \r\n",
        "print(Source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Target = TaskObjectJson['Target']['System']['Container'] + \"@\" + TaskObjectJson['Target']['System']['SystemServer'].replace(\"https://\",\"\").replace(\"blob\",\"dfs\") + \"/\"\r\n",
        "Target = Target.replace('///', '/')\r\n",
        "Target = Target.replace('//', '/')\r\n",
        "print(Target)\r\n",
        "Target = Target + TaskObjectJson['Target'][\"Instance\"]['TargetRelativePath']\r\n",
        "Target = Target  +\"/\" + TaskObjectJson['Target']['DataFileName'] \r\n",
        "print(Target)\r\n",
        "Target = Target.replace('///', '/')\r\n",
        "Target = Target.replace('//', '/')\r\n",
        "print(Target)\r\n",
        "Target = \"abfss://\" + Target \r\n",
        "print(Target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import col, explode_outer, from_json, lit, concat\r\n",
        "from pyspark.sql.types import StructType, ArrayType\r\n",
        "print(Source)\r\n",
        "input_df = spark.read.option(\"multiline\",\"true\").json(Source)\r\n",
        "#display(input_df)\r\n",
        "\r\n",
        "\r\n",
        "#output_df = execute_autoflatten_with_PK(input_df,None)\r\n",
        "#display(output_df)\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from delta.tables import *\r\n",
        "\r\n",
        "df = input_df\r\n",
        "\r\n",
        "#SIF model uses RefId as Unique identifier in all the structures\r\n",
        "mergeCondition = \"oldData.RefId = newData.RefId\"\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print(\"SourceDT = \" + TaskObjectJson['Source']['Type']  + \", TargetDT = \" + TaskObjectJson['Target']['Type'] )\r\n",
        "#df = spark.read.load(Source, format=SourceDT)\r\n",
        "\r\n",
        "var_check = DeltaTable.isDeltaTable(spark, Target )\r\n",
        "try:\r\n",
        "    if (var_check):\r\n",
        "        print(\"Performing Merge... on Existing table\")\r\n",
        "\r\n",
        "        olddt = DeltaTable.forPath(spark, Target) \r\n",
        "\r\n",
        "        olddt.alias(\"oldData\").merge(\r\n",
        "            df.alias(\"newData\"),\r\n",
        "            mergeCondition) \\\r\n",
        "        .whenMatchedUpdateAll() \\\r\n",
        "        .whenNotMatchedInsertAll() \\\r\n",
        "        .execute()\r\n",
        "    else:\r\n",
        "        print(\"Creating new Delta Table.\")    \r\n",
        "        df.write.format(\"Delta\").save(Target)\r\n",
        "except:\r\n",
        "    print(\"Table does not exist. Creating new Delta Table.\")    \r\n",
        "    df.write.format(\"Delta\").save(Target)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(\"Creating Spark Table\")\r\n",
        "df = spark.read.load(Target, format='delta')\r\n",
        "targetDB = \"sif\"\r\n",
        "targetTable = \"raw_\" + TaskObjectJson['Target']['DataFileName'].split(\".\")[0] \r\n",
        "#if the target datatype is parquet then we do not need to create a copy of the data - we can use the recently saved sink target\r\n",
        "if (TaskObjectJson['Target']['Type'] == 'Parquet'):\r\n",
        "    SnapshotTarget = Target\r\n",
        "else:\r\n",
        "    SnapshotTarget = Target + '/Snapshot/' + targetTable\r\n",
        "    #we need to update the parquet file - this is not very efficient but there isnt a current better way as delta tables are not supported for persistent tables\r\n",
        "    df.write.format(\"parquet\").mode(\"overwrite\").save(SnapshotTarget)\r\n",
        "\r\n",
        "\r\n",
        "#we need to make the DB and table lowercase as synapse persistent tables dont identify them as different identities\r\n",
        "targetDB = targetDB.lower()\r\n",
        "targetTable = targetTable.lower()\r\n",
        "\r\n",
        "#check if the specified DB / table exists - if so only do required actions.\r\n",
        "dbList = spark.catalog.listDatabases()\r\n",
        "dbExists = False\r\n",
        "for db in dbList:\r\n",
        "    if (db.name == targetDB):\r\n",
        "        dbExists = True\r\n",
        "        break\r\n",
        "if (dbExists):\r\n",
        "    print(\"DB Exists\")\r\n",
        "    tableExists = False\r\n",
        "    spark.catalog.setCurrentDatabase(targetDB)\r\n",
        "    tableList = spark.catalog.listTables()\r\n",
        "    for table in tableList:\r\n",
        "        if (table.name == targetTable):\r\n",
        "            tableExists = True\r\n",
        "            break\r\n",
        "    if (tableExists):\r\n",
        "        print(\"Table exists - nothing needed to be done\")\r\n",
        "        spark.catalog.refreshTable(targetTable)\r\n",
        "    else:\r\n",
        "        print(\"Table doesnt exist - creating\")\r\n",
        "        spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')\r\n",
        "else:\r\n",
        "    print(\"DB Doesnt exist - creating DB and table\")\r\n",
        "    createDBString = \"CREATE DATABASE \" + targetDB \r\n",
        "    spark.sql(createDBString)\r\n",
        "    spark.catalog.setCurrentDatabase(targetDB)\r\n",
        "    spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}