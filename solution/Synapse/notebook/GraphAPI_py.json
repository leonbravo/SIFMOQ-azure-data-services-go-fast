{
	"name": "GraphAPI_py",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5ffad234-18e0-40d0-a239-ddc8275c89c1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import LongType\r\n",
					"import datetime\r\n",
					"import logging\r\n",
					"\r\n",
					"class GraphAPI(BaseOEAModule):\r\n",
					"    def __init__(self, source_folder='graph_api'):\r\n",
					"        BaseOEAModule.__init__(self, source_folder)\r\n",
					"\r\n",
					"        self.stage1np_graphapi_users = self.stage1np + '/users'\r\n",
					"        self.stage1np_graphapi_m365 = self.stage1np + '/m365_app_user_detail'\r\n",
					"        self.stage1np_graphapi_teams = self.stage1np + '/teams_activity_user_detail'\r\n",
					"        self.stage1np_graphapi_signInLogs = self.stage1np + '/signin_logs'\r\n",
					"\r\n",
					"        self.schemas['users'] = [['surname', 'string', 'mask'],\r\n",
					"                                ['givenName', 'string', 'mask'],\r\n",
					"                                ['userPrincipalName', 'string', 'hash'],\r\n",
					"                                ['id', 'string', 'mask'],\r\n",
					"                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
					"\r\n",
					"        self.schemas['m365'] = [['reportRefreshDate', 'date', 'no-op'],\r\n",
					"                                ['userPrincipalName', 'string', 'hash'],\r\n",
					"                                ['lastActivationDate', 'date', 'no-op'],\r\n",
					"                                ['lastActivityDate', 'date', 'no-op'],\r\n",
					"                                ['reportPeriod', 'long', 'no-op'],\r\n",
					"                                ['mobile', 'boolean', 'no-op'],\r\n",
					"                                ['web', 'boolean', 'no-op'],\r\n",
					"                                ['mac', 'boolean', 'no-op'],\r\n",
					"                                ['windows', 'boolean', 'no-op'],\r\n",
					"                                ['excel', 'boolean', 'no-op'],\r\n",
					"                                ['excelMac', 'boolean', 'no-op'],\r\n",
					"                                ['excelMobile', 'boolean', 'no-op'],\r\n",
					"                                ['excelWeb', 'boolean', 'no-op'],\r\n",
					"                                ['excelWindows', 'boolean', 'no-op'],\r\n",
					"                                ['oneNote', 'boolean', 'no-op'],\r\n",
					"                                ['oneNoteMac', 'boolean', 'no-op'],\r\n",
					"                                ['oneNoteMobile', 'boolean', 'no-op'],\r\n",
					"                                ['oneNoteWeb', 'boolean', 'no-op'],\r\n",
					"                                ['oneNoteWindows', 'boolean', 'no-op'],\r\n",
					"                                ['outlook', 'boolean', 'no-op'],\r\n",
					"                                ['outlookMac', 'boolean', 'no-op'],\r\n",
					"                                ['outlookMobile', 'boolean', 'no-op'],\r\n",
					"                                ['outlookWeb', 'boolean', 'no-op'],\r\n",
					"                                ['outlookWindows', 'boolean', 'no-op'],\r\n",
					"                                ['powerPoint', 'boolean', 'no-op'],\r\n",
					"                                ['powerPointMac', 'boolean', 'no-op'],\r\n",
					"                                ['powerPointMobile', 'boolean', 'no-op'],\r\n",
					"                                ['powerPointWeb', 'boolean', 'no-op'],\r\n",
					"                                ['powerPointWindows', 'boolean', 'no-op'],\r\n",
					"                                ['teams', 'boolean', 'no-op'],\r\n",
					"                                ['teamsMac', 'boolean', 'no-op'],\r\n",
					"                                ['teamsMobile', 'boolean', 'no-op'],\r\n",
					"                                ['teamsWeb', 'boolean', 'no-op'],\r\n",
					"                                ['teamsWindows', 'boolean', 'no-op'],\r\n",
					"                                ['word', 'boolean', 'no-op'],\r\n",
					"                                ['wordMac', 'boolean', 'no-op'],\r\n",
					"                                ['wordMobile', 'boolean', 'no-op'],\r\n",
					"                                ['wordWeb', 'boolean', 'no-op'],\r\n",
					"                                ['wordWindows', 'boolean', 'no-op'],\r\n",
					"                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
					"\r\n",
					"        self.schemas['teams'] = [['reportRefreshDate', 'date', 'no-op'],\r\n",
					"                                ['lastActivityDate', 'date', 'no-op'],\r\n",
					"                                ['deletedDate', 'string', 'no-op'],\r\n",
					"                                ['isDeleted', 'boolean', 'no-op'],\r\n",
					"                                ['isLicensed', 'boolean', 'no-op'], \r\n",
					"                                ['reportPeriod', 'long', 'no-op'],\r\n",
					"                                ['userPrincipalName', 'string', 'hash'],\r\n",
					"                                ['privateChatMessageCount', 'integer', 'no-op'],\r\n",
					"                                ['teamChatMessageCount', 'integer', 'no-op'],\r\n",
					"                                ['meetingsAttendedCount', 'integer', 'no-op'],\r\n",
					"                                ['meetingCount', 'integer', 'no-op'],\r\n",
					"                                ['meetingsOrganizedCount', 'integer', 'no-op'],                        \r\n",
					"                                ['callCount', 'integer', 'no-op'],\r\n",
					"                                ['audioDuration', 'integer', 'no-op'],\r\n",
					"                                ['videoDuration', 'integer', 'no-op'],\r\n",
					"                                ['screenShareDuration', 'integer', 'no-op'],                        \r\n",
					"                                ['scheduledOneTimeMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
					"                                ['scheduledOneTimeMeetingsOrganizedCount', 'integer', 'no-op'],\r\n",
					"                                ['scheduledRecurringMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
					"                                ['scheduledRecurringMeetingsOrganizedCount', 'integer', 'no-op'],\r\n",
					"                                ['adHocMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
					"                                ['adHocMeetingsOrganizedCount', 'integer', 'no-op'],\r\n",
					"                                ['assignedProducts', 'string', 'no-op'],\r\n",
					"                                ['hasOtherAction', 'boolean', 'no-op'],\r\n",
					"                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
					"\r\n",
					"        self.schemas['sign_in_logs'] = [['id', 'string', 'mask'],\r\n",
					"                                ['createdDateTime', 'timestamp', 'no-op'],\r\n",
					"                                ['userPrincipalName', 'string', 'hash'],\r\n",
					"                                ['ipAddress', 'string', 'mask'], \r\n",
					"                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
					"    \r\n",
					"    def ingest(self):\r\n",
					"        \"\"\" Processes graphapi data from stage1 into stage2 using structured streaming within the defined functions below. \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph data from: \" + self.stage1np)\r\n",
					"\r\n",
					"        items = mssparkutils.fs.ls(self.stage1np)\r\n",
					"        for item in items:\r\n",
					"            if item.name == \"users\":\r\n",
					"                self._process_graphapi_users_stage1_data()\r\n",
					"            elif item.name == \"m365_app_user_detail\":\r\n",
					"                self._process_graphapi_m365_stage1_data()\r\n",
					"            elif item.name == \"teams_activity_user_detail\":\r\n",
					"                self._process_graphapi_teams_stage1_data()\r\n",
					"            elif item.name == \"signin_logs\":\r\n",
					"                self._process_graphapi_signInLogs_stage1_data()\r\n",
					"            else:\r\n",
					"                logger.info(\"No defined function for processing this queried data\")\r\n",
					"        \r\n",
					"        logger.info(\"Finished processing graphapi data from stage 1 to stage 2\")\r\n",
					"\r\n",
					"    def _process_graphapi_users_stage1_data(self):\r\n",
					"        \"\"\" Processes users data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph users data from: \" + self.stage1np_graphapi_users)\r\n",
					"\r\n",
					"        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
					"        # read in the raw data, and explode the \"value\" array\r\n",
					"        df = spark.readStream.format('json').load(self.stage1np_graphapi_users + '/*/*.json', header='true')\r\n",
					"        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
					"        currentDate = datetime.datetime.now()\r\n",
					"        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
					"            # create a new column for partitioning the folder structure\r\n",
					"        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
					"        # use the users_spark_schema for pseudonymization\r\n",
					"        users_spark_schema = oea.to_spark_schema(self.schemas['users'])\r\n",
					"        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['users'])\r\n",
					"\r\n",
					"        if len(df_pseudo.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2p')\r\n",
					"        else:\r\n",
					"            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_users + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
					"            query = query.start(self.stage2p + '/users_pseudo')\r\n",
					"            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"            logger.info(query.lastProgress)\r\n",
					"        \r\n",
					"        if len(df_lookup.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2np')\r\n",
					"        else:\r\n",
					"            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_users + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
					"            query2 = query2.start(self.stage2np + '/users_lookup')\r\n",
					"            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"            logger.info(query2.lastProgress)      \r\n",
					"\r\n",
					"    def _process_graphapi_m365_stage1_data(self):\r\n",
					"        \"\"\" Processes m365 data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph m365 data from: \" + self.stage1np_graphapi_m365)\r\n",
					"        \r\n",
					"        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
					"        # read in the raw data, and explode the \"value\" and \"details\" arrays\r\n",
					"        df = spark.readStream.format('json').load(self.stage1np_graphapi_m365 + '/*/*.json', header='true')\r\n",
					"        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        df = df.withColumn('reportPeriod', F.explode(F.col('details').reportPeriod)) \\\r\n",
					"                        .withColumn('mobile', F.explode(F.col('details').mobile)) \\\r\n",
					"                        .withColumn('web', F.explode(F.col('details').web)) \\\r\n",
					"                        .withColumn('mac', F.explode(F.col('details').mac)) \\\r\n",
					"                        .withColumn('windows', F.explode(F.col('details').windows)) \\\r\n",
					"                        .withColumn('excel', F.explode(F.col('details').excel)) \\\r\n",
					"                        .withColumn('excelMobile', F.explode(F.col('details').excelMobile)) \\\r\n",
					"                        .withColumn('excelWeb', F.explode(F.col('details').excelWeb)) \\\r\n",
					"                        .withColumn('excelMac', F.explode(F.col('details').excelMac)) \\\r\n",
					"                        .withColumn('excelWindows', F.explode(F.col('details').excelWindows)) \\\r\n",
					"                        .withColumn('oneNote', F.explode(F.col('details').oneNote)) \\\r\n",
					"                        .withColumn('oneNoteMobile', F.explode(F.col('details').oneNoteMobile)) \\\r\n",
					"                        .withColumn('oneNoteWeb', F.explode(F.col('details').oneNoteWeb)) \\\r\n",
					"                        .withColumn('oneNoteMac', F.explode(F.col('details').oneNoteMac)) \\\r\n",
					"                        .withColumn('oneNoteWindows', F.explode(F.col('details').oneNoteWindows)) \\\r\n",
					"                        .withColumn('outlook', F.explode(F.col('details').outlook)) \\\r\n",
					"                        .withColumn('outlookMobile', F.explode(F.col('details').outlookMobile)) \\\r\n",
					"                        .withColumn('outlookWeb', F.explode(F.col('details').outlookWeb)) \\\r\n",
					"                        .withColumn('outlookMac', F.explode(F.col('details').outlookMac)) \\\r\n",
					"                        .withColumn('outlookWindows', F.explode(F.col('details').outlookWindows)) \\\r\n",
					"                        .withColumn('powerPoint', F.explode(F.col('details').powerPoint)) \\\r\n",
					"                        .withColumn('powerPointMobile', F.explode(F.col('details').powerPointMobile)) \\\r\n",
					"                        .withColumn('powerPointWeb', F.explode(F.col('details').powerPointWeb)) \\\r\n",
					"                        .withColumn('powerPointMac', F.explode(F.col('details').powerPointMac)) \\\r\n",
					"                        .withColumn('powerPointWindows', F.explode(F.col('details').powerPointWindows)) \\\r\n",
					"                        .withColumn('teams', F.explode(F.col('details').teams)) \\\r\n",
					"                        .withColumn('teamsMobile', F.explode(F.col('details').teamsMobile)) \\\r\n",
					"                        .withColumn('teamsWeb', F.explode(F.col('details').teamsWeb)) \\\r\n",
					"                        .withColumn('teamsMac', F.explode(F.col('details').teamsMac)) \\\r\n",
					"                        .withColumn('teamsWindows', F.explode(F.col('details').teamsWindows)) \\\r\n",
					"                        .withColumn('word', F.explode(F.col('details').word)) \\\r\n",
					"                        .withColumn('wordMobile', F.explode(F.col('details').wordMobile)) \\\r\n",
					"                        .withColumn('wordWeb', F.explode(F.col('details').wordWeb)) \\\r\n",
					"                        .withColumn('wordMac', F.explode(F.col('details').wordMac)) \\\r\n",
					"                        .withColumn('wordWindows', F.explode(F.col('details').wordWindows)) \\\r\n",
					"                        .drop('details')\r\n",
					"        # change columns with dates to be of date types\r\n",
					"        df.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
					"        df.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
					"        df.select(F.col('lastActivationDate'), F.to_date(F.col('lastActivationDate'), 'yyyy-MM-dd'))\r\n",
					"        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
					"        currentDate = datetime.datetime.now()\r\n",
					"        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
					"            # create a new column for partitioning the folder structure\r\n",
					"        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
					"        # use the m365_spark_schema for pseudonymization\r\n",
					"        m365_spark_schema = oea.to_spark_schema(self.schemas['m365'])\r\n",
					"        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['m365'])\r\n",
					"\r\n",
					"        if len(df_pseudo.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2p')\r\n",
					"        else:\r\n",
					"            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_m365 + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
					"            query = query.start(self.stage2p + '/m365_app_user_detail_pseudo')\r\n",
					"            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"            logger.info(query.lastProgress)\r\n",
					"        \r\n",
					"        if len(df_lookup.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2np')\r\n",
					"        else:\r\n",
					"            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_m365 + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
					"            query2 = query2.start(self.stage2np + '/m365_app_user_detail_lookup')\r\n",
					"            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"            logger.info(query2.lastProgress)      \r\n",
					"\r\n",
					"    def _process_graphapi_teams_stage1_data(self):\r\n",
					"        \"\"\" Processes teams data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph teams data from: \" + self.stage1np_graphapi_teams)\r\n",
					"\r\n",
					"        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
					"        # read in the raw data, and explode the \"value\" and \"assignedProducts\" arrays \r\n",
					"        df = spark.readStream.format('json').load(self.stage1np_graphapi_teams + '/*/*.json', header='true')\r\n",
					"        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        df = df.withColumn('assignedProducts', F.explode(F.col('assignedProducts')))\r\n",
					"            # convert duration to seconds only \r\n",
					"            # NOTE: The duration expression has changed and this will need to be modified to accommodate the new format\r\n",
					"        df = df.withColumn(\r\n",
					"            'screenShareDuration', \r\n",
					"            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
					"            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
					"            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
					"            ).withColumn(\r\n",
					"            'videoDuration', \r\n",
					"            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
					"            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
					"            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
					"            ).withColumn(\r\n",
					"            'audioDuration', \r\n",
					"            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
					"            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
					"            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
					"            )\r\n",
					"        # change columns with dates to be of date types\r\n",
					"        df.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
					"        df.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
					"        # uncomment this code when using actual data, since this will be null in the test data\r\n",
					"        #df.select(F.col('deletedDate'), F.to_date(F.col('deletedDate'), 'yyyy-MM-dd'))\r\n",
					"        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
					"        currentDate = datetime.datetime.now()\r\n",
					"        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
					"            # create a new column for partitioning the folder structure\r\n",
					"        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
					"        # use the teams_spark_schema for pseudonymization\r\n",
					"        teams_spark_schema = oea.to_spark_schema(self.schemas['teams'])\r\n",
					"        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['teams'])\r\n",
					"\r\n",
					"        if len(df_pseudo.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2p')\r\n",
					"        else:\r\n",
					"            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_teams + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
					"            query = query.start(self.stage2p + '/teams_activity_user_detail_pseudo')\r\n",
					"            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"            logger.info(query.lastProgress)\r\n",
					"        \r\n",
					"        if len(df_lookup.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2np')\r\n",
					"        else:\r\n",
					"            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_teams + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
					"            query2 = query2.start(self.stage2np + '/teams_activity_user_detail_lookup')\r\n",
					"            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"            logger.info(query2.lastProgress)   \r\n",
					"\r\n",
					"    def _process_graphapi_signInLogs_stage1_data(self):\r\n",
					"        \"\"\" Processes teams data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph teams data from: \" + self.stage1np_graphapi_signInLogs)\r\n",
					"\r\n",
					"        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
					"        # read in the raw data, and isolate only the needed columns\r\n",
					"        df = spark.readStream.format('json').load(self.stage1np_graphapi_signInLogs + '/*/*.json', header='true')\r\n",
					"        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        df = df.select(F.col('id'), F.col('createdDateTime'), F.col('userPrincipalName'), F.col('ipAddress'))\r\n",
					"        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
					"        currentDate = datetime.datetime.now()\r\n",
					"        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
					"            # create a new column for partitioning the folder structure\r\n",
					"        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
					"        # use the signIn_spark_schema for pseudonymization\r\n",
					"        signIn_spark_schema = oea.to_spark_schema(self.schemas['sign_in_logs'])\r\n",
					"        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['sign_in_logs'])\r\n",
					"\r\n",
					"        if len(df_pseudo.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2p')\r\n",
					"        else:\r\n",
					"            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_signInLogs + '/_checkpoints_p')\r\n",
					"            query = query.start(self.stage2p + '/sign_in_audit_logs_pseudo')\r\n",
					"            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"            logger.info(query.lastProgress)\r\n",
					"        \r\n",
					"        if len(df_lookup.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2np')\r\n",
					"        else:\r\n",
					"            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_signInLogs + '/_checkpoints_np')\r\n",
					"            query2 = query2.start(self.stage2np + '/sign_in_audit_logs_lookup')\r\n",
					"            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"            logger.info(query2.lastProgress)   \r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}