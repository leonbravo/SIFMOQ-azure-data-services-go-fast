{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def execute_autoflatten_with_PK(df, dfcols):\r\n",
        "    '''\r\n",
        "    Description:\r\n",
        "    This function executes the core autoflattening operation\r\n",
        "    :param df: [type: pyspark.sql.dataframe.DataFrame] dataframe to be used for flattening\r\n",
        "    :param dfcols: columns of Pk for hash if known otherwise all fields    \r\n",
        "    :return df: DataFrame containing flattened records and \r\n",
        "    :a unique PK column (surrogate key) \"rowhash\" that is the sha2 hash of specific columns in the DataFrame.\r\n",
        "    '''\r\n",
        "    from pyspark.sql.functions import sha2, concat_ws\r\n",
        "\r\n",
        "    # gets all fields of StructType or ArrayType in the nested_fields dictionary\r\n",
        "    nested_fields = dict([\r\n",
        "        (field.name, field.dataType)\r\n",
        "        for field in df.schema.fields\r\n",
        "        if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)\r\n",
        "    ])\r\n",
        " \r\n",
        "    # repeat until all nested_fields i.e. belonging to StructType or ArrayType are covered\r\n",
        "    while nested_fields:\r\n",
        "        # if there are any elements in the nested_fields dictionary\r\n",
        "        if nested_fields:\r\n",
        "            # get a column\r\n",
        "            column_name = list(nested_fields.keys())[0]\r\n",
        "            # if field belongs to a StructType, all child fields inside it are accessed\r\n",
        "            # and are aliased with complete path to every child field\r\n",
        "            if isinstance(nested_fields[column_name], StructType):\r\n",
        "                unnested = [col(column_name + '.' + child).alias(column_name + '_' + child) for child in [ n.name for n in  nested_fields[column_name]]]\r\n",
        "                df = df.select(\"*\", *unnested).drop(column_name)\r\n",
        "            # else, if the field belongs to an ArrayType, an explode_outer is done\r\n",
        "            elif isinstance(nested_fields[column_name], ArrayType):\r\n",
        "                df = df.withColumn(column_name, explode_outer(column_name))\r\n",
        " \r\n",
        "        # Now that df is updated, gets all fields of StructType and ArrayType in a fresh nested_fields dictionary\r\n",
        "        nested_fields = dict([\r\n",
        "            (field.name, field.dataType)\r\n",
        "            for field in df.schema.fields\r\n",
        "            if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)\r\n",
        "        ])\r\n",
        "    #generate row_sha2 column\r\n",
        "    if dfcols==None: df = df.withColumn(\"PK_rowid_sha2\", sha2(concat_ws(\"||\", *df.columns), 256))\r\n",
        "    else:     df.withColumn(\"PK_rowid_sha2\", sha2(concat_ws(\"||\", *dfcols.columns), 256))\r\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def execute_autoflatten_v2(df):\r\n",
        "    '''\r\n",
        "    Description:\r\n",
        "    This function executes the core autoflattening operation\r\n",
        "    :param df: [type: pyspark.sql.dataframe.DataFrame] dataframe to be used for flattening\r\n",
        "    :return df: DataFrame containing flattened records\r\n",
        "    '''\r\n",
        "    # gets all fields of StructType or ArrayType in the nested_fields dictionary\r\n",
        "    nested_fields = dict([\r\n",
        "        (field.name, field.dataType)\r\n",
        "        for field in df.schema.fields\r\n",
        "        if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)\r\n",
        "    ])\r\n",
        " \r\n",
        "    # repeat until all nested_fields i.e. belonging to StructType or ArrayType are covered\r\n",
        "    while nested_fields:\r\n",
        "        # if there are any elements in the nested_fields dictionary\r\n",
        "        if nested_fields:\r\n",
        "            # get a column\r\n",
        "            column_name = list(nested_fields.keys())[0]\r\n",
        "            # if field belongs to a StructType, all child fields inside it are accessed\r\n",
        "            # and are aliased with complete path to every child field\r\n",
        "            if isinstance(nested_fields[column_name], StructType):\r\n",
        "                unnested = [col(column_name + '.' + child).alias(column_name + '_' + child) for child in [ n.name for n in  nested_fields[column_name]]]\r\n",
        "                df = df.select(\"*\", *unnested).drop(column_name)\r\n",
        "            # else, if the field belongs to an ArrayType, an explode_outer is done\r\n",
        "            elif isinstance(nested_fields[column_name], ArrayType):\r\n",
        "                df = df.withColumn(column_name, explode_outer(column_name))\r\n",
        " \r\n",
        "        # Now that df is updated, gets all fields of StructType and ArrayType in a fresh nested_fields dictionary\r\n",
        "        nested_fields = dict([\r\n",
        "            (field.name, field.dataType)\r\n",
        "            for field in df.schema.fields\r\n",
        "            if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)\r\n",
        "        ])\r\n",
        " \r\n",
        "    return df\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    }
  }
}