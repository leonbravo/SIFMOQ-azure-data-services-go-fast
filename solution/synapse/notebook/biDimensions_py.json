{
	"name": "biDimensions_py",
	"properties": {
		"description": "WIP meant to contain the schemas for bi dimensions",
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark3p1sm",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "883538e0-795d-4742-914d-bf09945d48b4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
				"name": "spark3p1sm",
				"type": "Spark",
				"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Microsoft Insights Class"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class biDimensions(BaseOEAModule):\r\n",
					"    \"\"\"\r\n",
					"    Provides data processing methods for biDimensions data.\r\n",
					"    Data is expected to be received via ADS into stage1np/BCE/BI\r\n",
					"    The structure of the folders in stage1np will then be something like:\r\n",
					"    In stage2, everything is written to stage2np/BCE/BI and stage2p/ms_insights\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    def __init__(self, source_folder= 'BCE/BI/Dimensions'):\r\n",
					"        BaseOEAModule.__init__(self, source_folder)\r\n",
					"      \r\n",
					"        self.schemas['Location'] = [\r\n",
					"                        ['ADMIN_LOCATION','String','no-op'],\r\n",
					"                        ['SUB_LOCATION','String','no-op'],\r\n",
					"                        ['ADMIN_DESC','String','no-op'],\r\n",
					"                        ['Directorate','String','no-op'],\r\n",
					"                        ['ServiceCentre','String','no-op'],\r\n",
					"                        ['Department','String','no-op'],\r\n",
					"                        ['CampusID','Integer','no-op'],\r\n",
					"                        ['SchoolType','String','no-op'],\r\n",
					"                        ['IntegrumOU','String','no-op'],\r\n",
					"                        ['OU4','String','no-op'],\r\n",
					"                        ['Directorate_old','String','no-op'],\r\n",
					"                        ['LocationType','String','no-op'],\r\n",
					"                        ['ADMIN_LOCATION_PBI','String','no-op'],\r\n",
					"                        ['Clusters','String','no-op'],\r\n",
					"                        ['Suburb','String','no-op'],\r\n",
					"                        ['Directorate_Sort','String','no-op'],\r\n",
					"                        ['School Name','String','no-op'],\r\n",
					"                        ['School_Code','Integer','no-op'],\r\n",
					"                        ['Campus_Code','Integer','no-op'],\r\n",
					"                        ['Campus Name','String','no-op'],\r\n",
					"                        ['latitude','float','no-op'],\r\n",
					"                        ['longitude','float','no-op'],\r\n",
					"                        ['LatLong','String','no-op'],\r\n",
					"                        ['OfficeName','String','no-op'],\r\n",
					"                        ['Suburb - School','String','no-op'],\r\n",
					"                        ['GeoType','String','no-op'],\r\n",
					"                        ['LeuvenCode','String','no-op']\r\n",
					"                        ]\r\n",
					"\r\n",
					"    \r\n",
					"    def ingest(self):\r\n",
					"        \"\"\"  Processes insights data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
					"        logger.info(\"Processing microsoft_insights data from: \" + self.stage1np)\r\n",
					"        \r\n",
					"        items = mssparkutils.fs.ls(self.stage1np)\r\n",
					"        for item in items:\r\n",
					"            if item.name == \"Locations\":\r\n",
					"                ingest_snapshot_data( source_system=self.source_folder, tablename='Locations', schema=self.schemas['Location'], partition_by='SUB_LOCATION', primary_key='CampusID', data_format='csv', has_header=True)\r\n",
					"            else:\r\n",
					"                logger.info(\"No defined function for processing this insights data\")\r\n",
					"        \r\n",
					"        logger.info(\"Finished ingesting insights data from stage 1 to stage 2\")\r\n",
					"\r\n",
					"\r\n",
					"class SchoolPerformance(BaseOEAModule):\r\n",
					"    \"\"\"\r\n",
					"    Provides data processing methods for biDimensions data.\r\n",
					"    Data is expected to be received via ADS into stage1np/BCE/BI\r\n",
					"    The structure of the folders in stage1np will then be something like:\r\n",
					"    In stage2, everything is written to stage2np/BCE/BI and stage2p/ms_insights\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    def __init__(self, source_folder= 'BCE/BI/SchoolClassification'):\r\n",
					"        BaseOEAModule.__init__(self, source_folder)\r\n",
					"      \r\n",
					"        self.schemas['SchoolClassification'] = [\r\n",
					"                    ['Year' ,   'int64', 'no-op'],\r\n",
					"                    ['School' ,   'object', 'no-op'],\r\n",
					"                    ['Attendance Level %' ,   'float64', 'no-op'],\r\n",
					"                    ['5Y Enrolment %' ,   'float64', 'no-op'],\r\n",
					"                    ['Similar Student Difference' ,   'float64', 'no-op'],\r\n",
					"                    ['SRS English A-C % Semester 1' ,   'float64', 'no-op'],\r\n",
					"                    ['SRS English A-C % Semester 2' ,   'float64', 'no-op'],\r\n",
					"                    ['SRS Maths A-C % Semester 1' ,   'float64', 'no-op'],\r\n",
					"                    ['SRS Maths A-C % Semester 2' ,   'float64', 'no-op'],\r\n",
					"                    ['Overall' ,   'int64  ', 'no-op']                      ,\r\n",
					"                    ['Change from PY' ,   'float64', 'no-op']\r\n",
					"        ]\r\n",
					"\r\n",
					"    def ingest(self, inpdataframe, primary_key):\r\n",
					"        df = inpdataframe.dropDuplicates([primary_key]) \r\n",
					"        # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\r\n",
					"        \r\n",
					"        df_pseudo, df_lookup = self.pseudonymize(df, schema)\r\n",
					"\r\n",
					"        if len(df_pseudo.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2p')\r\n",
					"        else:\r\n",
					"            df_pseudo.write.save(p_destination_path, format='delta', mode='overwrite') \r\n",
					"\r\n",
					"        if len(df_lookup.columns) == 0:\r\n",
					"            logger.info('No data to be written to stage2np')\r\n",
					"        else:\r\n",
					"            df_lookup.write.save(np_destination_path, format='delta', mode='overwrite', partitionBy=partition_by) \r\n",
					""
				],
				"execution_count": 1
			}
		]
	}
}