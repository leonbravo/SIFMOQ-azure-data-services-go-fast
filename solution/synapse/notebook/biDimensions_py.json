{
	"name": "biDimensions_py",
	"properties": {
		"description": "WIP meant to contain the schemas for bi dimensions",
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark3p1sm",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "19c8ff64-7ac3-446c-b3c8-878b0b3fbf08"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/21fec1ab-7af8-4f99-b66f-a69e7ba77a22/resourceGroups/BCE-AAE-OEA-DEV-RG/providers/Microsoft.Synapse/workspaces/bce-aae-oea-dev-syn/bigDataPools/spark3p1sm",
				"name": "spark3p1sm",
				"type": "Spark",
				"endpoint": "https://bce-aae-oea-dev-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# BCE BI Dimensions Class"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class biDimensions(BaseOEAModule):\r\n",
					"    \"\"\"\r\n",
					"    Provides data processing methods for biDimensions data.\r\n",
					"    Data is expected to be received via ADS into stage1np/BCE/BI\r\n",
					"    The structure of the folders in stage1np will then be something like:\r\n",
					"    In stage2, everything is written to stage2np/BCE/BI and stage2p/ms_insights\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    def __init__(self, source_folder= 'BCE/BI/Dimensions'):\r\n",
					"        BaseOEAModule.__init__(self, source_folder)\r\n",
					"      \r\n",
					"        self.schemas['Locations'] = [\r\n",
					"                        ['ADMIN_LOCATION','String','no-op'],\r\n",
					"                        ['SUB_LOCATION','String','no-op'],\r\n",
					"                        ['ADMIN_DESC','String','no-op'],\r\n",
					"                        ['Directorate','String','no-op'],\r\n",
					"                        ['ServiceCentre','String','no-op'],\r\n",
					"                        ['Department','String','no-op'],\r\n",
					"                        ['CampusID','Integer','no-op'],\r\n",
					"                        ['SchoolType','String','no-op'],\r\n",
					"                        ['IntegrumOU','String','no-op'],\r\n",
					"                        ['OU4','String','no-op'],\r\n",
					"                        ['Directorate_old','String','no-op'],\r\n",
					"                        ['LocationType','String','no-op'],\r\n",
					"                        ['ADMIN_LOCATION_PBI','String','no-op'],\r\n",
					"                        ['Clusters','String','no-op'],\r\n",
					"                        ['Suburb','String','no-op'],\r\n",
					"                        ['Directorate_Sort','String','no-op'],\r\n",
					"                        ['School Name','String','no-op'],\r\n",
					"                        ['School_Code','Integer','no-op'],\r\n",
					"                        ['Campus_Code','Integer','no-op'],\r\n",
					"                        ['Campus Name','String','no-op'],\r\n",
					"                        ['latitude','float64','no-op'],\r\n",
					"                        ['longitude','float64','no-op'],\r\n",
					"                        ['LatLong','String','no-op'],\r\n",
					"                        ['OfficeName','String','no-op'],\r\n",
					"                        ['Suburb - School','String','no-op'],\r\n",
					"                        ['GeoType','String','no-op'],\r\n",
					"                        ['LeuvenCode','String','no-op']\r\n",
					"                        ]\r\n",
					"        self.schemas['SRS_ScaleScoreConversion'] = [['Score', 'integer', 'no-op'],\r\n",
					"                                    ['Mark', 'string', 'no-op']]\r\n",
					" \r\n",
					"        self.schemas['SRS_SubjectNames'] = [['SubjectName', 'string', 'no-op'],\r\n",
					"                                            ['SubjectGroupName', 'string', 'no-op']]\r\n",
					"\r\n",
					"        self.schemas['Dates_WithSchoolTerm']=[  ['YearCalendar','integer','no-op'],\r\n",
					"                ['MonthNumberOfYear','integer','no-op'],\r\n",
					"                ['Month Dates','string','no-op'],\r\n",
					"                ['WeekNumberOfYear','integer','no-op'],\r\n",
					"                ['Week','string','no-op'],\r\n",
					"                ['DateKey','integer','no-op'],\r\n",
					"                ['FullDate','timestamp','no-op'],\r\n",
					"                ['ShortDate','timestamp','no-op'],\r\n",
					"                ['DayNumberOfWeek','integer','no-op'],\r\n",
					"                ['DayNameOfWeek','string','no-op'],\r\n",
					"                ['WeekDayType','string','no-op'],\r\n",
					"                ['DayNumberOfMonth','integer','no-op'],\r\n",
					"                ['DayNumberOfYear','integer','no-op'],\r\n",
					"                ['MonthNameOfYear','string','no-op'],\r\n",
					"                ['QuarterNumberCalendar','integer','no-op'],\r\n",
					"                ['QuarterNameCalendar','string','no-op'],\r\n",
					"                ['SemesterNumberCalendar','integer','no-op'],\r\n",
					"                ['SemesterNameCalendar','string','no-op'],\r\n",
					"                ['Term','integer','no-op'],\r\n",
					"                ['TermName','string','no-op'],\r\n",
					"                ['FutureDate','string','no-op'],\r\n",
					"                ['IsToday','string','no-op'],\r\n",
					"                ['Today Date','timestamp','no-op'],\r\n",
					"                ['Year - Week','string','no-op'],\r\n",
					"                ['Year - Month','string','no-op'],\r\n",
					"                ['School Week','integer','no-op'],\r\n",
					"                ['School Day','integer','no-op'],\r\n",
					"                ['School Week Name','string','no-op'],\r\n",
					"                ['Day of Term','integer','no-op'],\r\n",
					"                ['Day of Term Reverse','integer','no-op'],\r\n",
					"                ['Work Days','integer','no-op']]\r\n",
					"\r\n",
					"                                 \r\n",
					"    \r\n",
					"    def ingest(self):\r\n",
					"        \"\"\"  Processes insights data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
					"        logger.info(\"Processing microsoft_insights data from: \" + self.stage1np)\r\n",
					"        \r\n",
					"        items = mssparkutils.fs.ls(self.stage1np)\r\n",
					"        for item in items:\r\n",
					"            df= self.returnDataFrameBeforeProcessing(item.name)\r\n",
					"            if item.name == \"Locations.csv\":\r\n",
					"                oea.ingest_snapshot_data_frame( inpdataframe=df,source_path=f'{source_folder}{table_name}', schema=self.schemas['Locations']       , primary_key='CampusID')\r\n",
					"            if item.name == \"SRS_SubjectNames.csv\":\r\n",
					"               # df= self.returnDataFrameBeforeProcessing(\"SRS_SubjectNames\")\r\n",
					"                oea.ingest_snapshot_data_frame( inpdataframe=df,source_path=f'{source_folder}{table_name}', schema=self.schemas['SRS_SubjectNames'], primary_key='SubjectName')\r\n",
					"            if item.name == \"SRS_ScaleScoreConversion.csv\":\r\n",
					"                #df= self.returnDataFrameBeforeProcessing(\"SRS_ScaleScoreConversion\")\r\n",
					"                oea.ingest_snapshot_data_frame( inpdataframe=df,source_path=f'{source_folder}{table_name}', schema=self.schemas['SRS_ScaleScoreConversion'], primary_key='Score')\r\n",
					"            if item.name == \"Dates_WithSchoolTerm.csv\":\r\n",
					"                #df= self.returnDataFrameBeforeProcessing(\"Dates_WithSchoolTerm\")\r\n",
					"                oea.ingest_snapshot_data_frame( inpdataframe=df,source_path=f'{source_folder}{table_name}', schema=self.schemas['Dates_WithSchoolTerm'], primary_key='DateKey')\r\n",
					"\r\n",
					"            else:\r\n",
					"                logger.info(\"No defined function for processing this insights data\"+item.name)\r\n",
					"        \r\n",
					"        logger.info(\"Finished ingesting insights data from stage 1 to stage 2\")\r\n",
					"    \r\n",
					"    def returnDataFrameBeforeProcessing(self, filename='Locations.csv', sourcepath=''):\r\n",
					"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
					"        logger.info(\"Processing ms_insights activity data from: \" + sourcepath)\r\n",
					"        if sourcepath == '': \r\n",
					"            sourcepath=self.stage1np + '/'+ filename\r\n",
					"        tableName = filename.replace('.csv','')\r\n",
					"        # Currently not using the OEA ingest_incremental_data function due to pulling out the partition folders\r\n",
					"        spark_schema = oea.to_spark_schema(self.schemas[f'{tableName}'])\r\n",
					"        df = spark.read.csv(sourcepath , header='True', schema=spark_schema)\r\n",
					"\r\n",
					"        return df\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"class SchoolPerformance(BaseOEAModule):\r\n",
					"    \"\"\"\r\n",
					"    Provides data processing methods for biDimensions data.\r\n",
					"    Data is expected to be received via ADS into stage1np/BCE/BI\r\n",
					"    The structure of the folders in stage1np will then be something like:\r\n",
					"    In stage2, everything is written to stage2np/BCE/BI and stage2p/ms_insights\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    def __init__(self, source_folder= 'BCE/BI/SchoolClassification'):\r\n",
					"        BaseOEAModule.__init__(self, source_folder)\r\n",
					"      \r\n",
					"        self.schemas['SchoolClassification'] = [\r\n",
					"                    ['Year' ,   'Integer', 'no-op'],\r\n",
					"                    ['School' ,   'string', 'no-op'],\r\n",
					"                    ['Attendance_Level_%' ,   'float64', 'no-op'],\r\n",
					"                    ['5Y_Enrolment_%' ,   'float64', 'no-op'],\r\n",
					"                    ['Similar_Student_Difference' ,   'float64', 'no-op'],\r\n",
					"                    ['SRS_English_A-C_%_Semester_1' ,   'float64', 'no-op'],\r\n",
					"                    ['SRS_English_A-C_%_Semester_2' ,   'float64', 'no-op'],\r\n",
					"                    ['SRS_Maths_A-C_%_Semester_1' ,   'float64', 'no-op'],\r\n",
					"                    ['SRS_Maths_A-C_%_Semester_2' ,   'float64', 'no-op'],\r\n",
					"                    ['Overall' ,   'Integer  ', 'no-op']                      ,\r\n",
					"                    ['Change_from_PY' ,   'Integer', 'no-op']\r\n",
					"        ]\r\n",
					"\r\n",
					"    def ingest(self, inpdataframe, primary_key):\r\n",
					"\r\n",
					"        \r\n",
					"        \"\"\"  Processes insights data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
					"        logger.info(\"Processing microsoft_insights data from: \" + self.stage1np)\r\n",
					"        \r\n",
					"        items = mssparkutils.fs.ls(self.stage1np)\r\n",
					"        for item in items:\r\n",
					"            if item.name == \"SchoolClassification\":\r\n",
					"                oea.ingest_snapshot_data_frame(inpdataframe, schema=self.schemas['SchoolClassification'], primary_key=primary_key, source_folder= self.source_folder)\r\n",
					"            else:\r\n",
					"                logger.info(\"No defined function for processing this insights data\")\r\n",
					"        \r\n",
					"        logger.info(\"Finished ingesting insights data from stage 1 to stage 2\")\r\n",
					"\r\n",
					""
				],
				"execution_count": 2
			}
		]
	}
}